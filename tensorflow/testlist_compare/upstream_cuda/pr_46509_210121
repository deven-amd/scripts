WARNING: The following configs were expanded more than once: [v2]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
WARNING: Option 'bes_best_effort' is deprecated: BES best effort upload has been removed. The flag has no more functionality attached to it and will be removed in a future release.
INFO: Invocation ID: 3ad8b6b5-2c53-4d0f-9d3d-ec58c7f7e719
INFO: Streaming build results to: https://source.cloud.google.com/results/invocations/3ad8b6b5-2c53-4d0f-9d3d-ec58c7f7e719
WARNING: BAZEL_INTERNAL_INVOCATION_ID is set. This will soon be deprecated in favor of --invocation_id. Please switch to using the flag.
INFO: Reading 'startup' options from /tmpfs/src/piper/google3/learning/brain/testing/kokoro/.bazelrc: --host_jvm_args=-Dbazel.DigestFunction=SHA256
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=0 --terminal_columns=80
INFO: Reading rc options for 'test' from /tmpfs/src/github/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'test' from /tmpfs/src/github/tensorflow/.bazelrc:
  Inherited 'build' options: --apple_platform_type=macos --define framework_shared_object=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2
INFO: Reading rc options for 'test' from /tmpfs/src/piper/google3/learning/brain/testing/kokoro/.bazelrc:
  Inherited 'build' options: --announce_rc --experimental_strict_action_env=true --keep_going --verbose_failures=true --output_filter=^$
INFO: Reading rc options for 'test' from /tmpfs/src/github/tensorflow/.bazelrc:
  'test' options: --config=v2
INFO: Reading rc options for 'test' from /tmpfs/src/piper/google3/learning/brain/testing/kokoro/.bazelrc:
  'test' options: --build_tests_only --test_output=errors --test_size_filters=small,medium --test_timeout=300,450,1200,3600
INFO: Found applicable config definition build:short_logs in file /tmpfs/src/github/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /tmpfs/src/github/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:v2 in file /tmpfs/src/github/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:rbe in file /tmpfs/src/github/tensorflow/.bazelrc: --action_env=BAZEL_DO_NOT_DETECT_CPP_TOOLCHAIN=1 --google_default_credentials --bes_backend=buildeventservice.googleapis.com --bes_results_url=https://source.cloud.google.com/results/invocations --bes_timeout=600s --define=EXECUTOR=remote --distinct_host_configuration=false --flaky_test_attempts=3 --jobs=200 --remote_executor=grpcs://remotebuildexecution.googleapis.com --remote_timeout=3600 --spawn_strategy=remote,worker,standalone,local --remote_download_toplevel
INFO: Found applicable config definition test:rbe in file /tmpfs/src/github/tensorflow/.bazelrc: --test_env=USER=anon
INFO: Found applicable config definition build:linux in file /tmpfs/src/github/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels
INFO: Found applicable config definition build:dynamic_kernels in file /tmpfs/src/github/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
Loading: 
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
Loading: 0 packages loaded
    currently loading: tensorflow/compiler/xrt/cc
Loading: 32 packages loaded
    currently loading: tensorflow/compiler/xrt/cc ... (144 packages)
Loading: 90 packages loaded
    currently loading: tensorflow/compiler/xrt/cc ... (402 packages)
Loading: 136 packages loaded
    currently loading: tensorflow/compiler/xrt/cc ... (431 packages)
Loading: 252 packages loaded
    currently loading: tensorflow/c ... (315 packages)
Analyzing: 877 targets (567 packages loaded)
Analyzing: 877 targets (567 packages loaded, 0 targets configured)
DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = "1556410077 -0400"
DEBUG: Repository io_bazel_rules_docker instantiated at:
  /tmpfs/src/github/tensorflow/WORKSPACE:23:10: in <toplevel>
  /tmpfs/src/github/tensorflow/tensorflow/workspace0.bzl:65:34: in workspace
  /tmpfs/bazel_output/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/external/bazel_toolchains/repositories/repositories.bzl:37:23: in repositories
Repository rule git_repository defined at:
  /tmpfs/bazel_output/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/external/bazel_tools/tools/build_defs/repo/git.bzl:199:33: in <toplevel>
Analyzing: 877 targets (581 packages loaded, 14 targets configured)
Analyzing: 877 targets (582 packages loaded, 14 targets configured)
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/www.sqlite.org/2020/sqlite-amalgamation-3340000.zip failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
Analyzing: 877 targets (635 packages loaded, 2120 targets configured)
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/nvidia/nccl/archive/v2.8.3-1.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
Analyzing: 877 targets (662 packages loaded, 7585 targets configured)
WARNING: Download from http://mirror.tensorflow.org/files.pythonhosted.org/packages/12/59/eaa15ab9710a20e22225efd042cd2d6a0b559a0656d5baba9641a2a4a921/gast-0.4.0.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
Analyzing: 877 targets (694 packages loaded, 15238 targets configured)
Analyzing: 877 targets (696 packages loaded, 18616 targets configured)
WARNING: Download from https://mirror.bazel.build/github.com/aws/aws-sdk-cpp/archive/1.7.336.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
Analyzing: 877 targets (700 packages loaded, 25801 targets configured)
Analyzing: 877 targets (703 packages loaded, 29654 targets configured)
INFO: Analyzed 877 targets (703 packages loaded, 29654 targets configured).
INFO: Found 877 test targets...
[0 / 12] [Prepa] BazelWorkspaceStatusAction stable-status.txt
[2,973 / 13,228] Creating runfiles tree bazel-out/k8-opt/bin/tensorflow/cc/ops/control_flow_ops_gen_cc.runfiles; 3s local ... (199 actions, 2 running)
[6,010 / 13,385] [Prepa] action 'SolibSymlink _solib_local/libtensorflow_Scompiler_Sxla_Sservice_Scpu_Slibruntime_Umatmul.so' ... (200 actions, 0 running)
[12,027 / 22,458] [Prepa] Symlinking virtual headers for @local_config_cuda//cuda:cuda_headers_virtual; 15s ... (118 actions, 1 running)
[13,404 / 22,497] [Prepa] action 'SolibSymlink _solib_local/libexternal_Sllvm-project_Sllvm_SlibObject.so'; 5s ... (158 actions, 0 running)
[15,256 / 22,497] [Prepa] action 'SolibSymlink _solib_local/libexternal_Sllvm-project_Sllvm_SlibAnalysis.so'; 15s ... (200 actions, 0 running)
[17,349 / 22,497] [Sched] Compiling tensorflow/core/kernels/fixed_length_record_reader_op.cc; 26s ... (193 actions, 0 running)
[19,373 / 22,497] [Prepa] action 'SolibSymlink _solib_local/libtensorflow_Scompiler_Smlir_Stensorflow_Slibtensorflow_Uops_Ua_Um.so'; 50s ... (179 actions, 0 running)
[21,001 / 22,497] [Prepa] action 'SolibSymlink _solib_local/libtensorflow_Scompiler_Stf2tensorrt_Slibtrt_Uengine_Uutils.so'; 37s ... (190 actions, 5 running)
[24,903 / 26,932] Compiling tensorflow/compiler/xla/service/gpu/gpu_executable.cc; 28s remote ... (173 actions, 20 running)
[29,393 / 30,298] Compiling tensorflow/compiler/xla/service/gpu/ir_emitter_unnested.cc; 61s remote ... (174 actions, 79 running)
[31,057 / 31,379] 23 / 877 tests; Linking tensorflow/core/common_runtime/permuter_test_gpu; 21s remote ... (200 actions, 197 running)
[32,098 / 32,354] 87 / 877 tests; Linking tensorflow/core/common_runtime/permuter_test_gpu; 69s remote ... (200 actions, 192 running)
[33,522 / 33,694] 237 / 877 tests; Linking tensorflow/c/eager/c_api_distributed_test_gpu; 122s remote ... (167 actions, 164 running)
[34,034 / 35,591] 416 / 877 tests; Linking tensorflow/c/eager/c_api_unified_experimental_test_gpu; 184s remote ... (200 actions, 188 running)
[34,698 / 35,595] 644 / 877 tests; Linking tensorflow/c/eager/gradient_checker_test_gpu; 179s remote ... (200 actions, 187 running)
FAIL: //tensorflow/python/eager:function_test_gpu (shard 9 of 15) (see /tmpfs/bazel_output/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/eager/function_test_gpu/shard_9_of_15/test_attempts/attempt_1.log)
FAIL: //tensorflow/compiler/tests:image_ops_test_gpu (shard 5 of 10) (see /tmpfs/bazel_output/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/compiler/tests/image_ops_test_gpu/shard_5_of_10/test_attempts/attempt_1.log)
FAIL: //tensorflow/python/eager:function_test_gpu (shard 9 of 15) (see /tmpfs/bazel_output/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/eager/function_test_gpu/shard_9_of_15/test_attempts/attempt_2.log)
FAIL: //tensorflow/python/eager:function_test_gpu (shard 9 of 15) (see /tmpfs/bazel_output/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/eager/function_test_gpu/shard_9_of_15/test.log)

FAILED: //tensorflow/python/eager:function_test_gpu (Summary)
      /tmpfs/bazel_output/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/eager/function_test_gpu/shard_9_of_15/test.log
      /tmpfs/bazel_output/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/eager/function_test_gpu/shard_9_of_15/test_attempts/attempt_1.log
      /tmpfs/bazel_output/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/eager/function_test_gpu/shard_9_of_15/test_attempts/attempt_2.log
INFO: From Testing //tensorflow/python/eager:function_test_gpu (shard 9 of 15):
==================== Test output for //tensorflow/python/eager:function_test_gpu (shard 9 of 15):
2021-01-19 18:13:34.301311: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
Running tests under Python 3.6.10: /usr/bin/python3
[ RUN      ] FunctionTest.testCacheKeyVariables
2021-01-19 18:13:37.460480: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-01-19 18:13:37.473523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:37.474275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:13:37.474314: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
2021-01-19 18:13:37.476608: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10
2021-01-19 18:13:37.476683: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10
2021-01-19 18:13:37.478609: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-01-19 18:13:37.479081: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-01-19 18:13:37.481216: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-01-19 18:13:37.482373: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10
2021-01-19 18:13:37.486802: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7
2021-01-19 18:13:37.486965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:37.487674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:37.488333: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:13:37.506513: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-01-19 18:13:37.506807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:37.507531: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:13:37.507652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:37.508401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:37.509039: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:13:37.509092: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
2021-01-19 18:13:38.369316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-19 18:13:38.369363: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2021-01-19 18:13:38.369372: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2021-01-19 18:13:38.369628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:38.370502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:38.371255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:38.371936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15404 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
2021-01-19 18:13:38.739171: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:128] None of the MLIR optimization passes are enabled (registered 2)
2021-01-19 18:13:38.739964: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2299995000 Hz
INFO:tensorflow:time(__main__.FunctionTest.testCacheKeyVariables): 1.34s
I0119 18:13:38.799670 140707827947264 test_util.py:2071] time(__main__.FunctionTest.testCacheKeyVariables): 1.34s
[       OK ] FunctionTest.testCacheKeyVariables
[ RUN      ] FunctionTest.testCapturesVariables
INFO:tensorflow:time(__main__.FunctionTest.testCapturesVariables): 0.1s
I0119 18:13:38.903273 140707827947264 test_util.py:2071] time(__main__.FunctionTest.testCapturesVariables): 0.1s
[       OK ] FunctionTest.testCapturesVariables
[ RUN      ] FunctionTest.testConcreteFunctionFlatSignatureErrorMissingKeywordArg
WARNING:tensorflow:From /usr/lib/python3.6/contextlib.py:60: TensorFlowTestCase.test_session (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.
Instructions for updating:
Use `self.session()` or `self.cached_session()` instead.
W0119 18:13:38.904335 140707827947264 deprecation.py:336] From /usr/lib/python3.6/contextlib.py:60: TensorFlowTestCase.test_session (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.
Instructions for updating:
Use `self.session()` or `self.cached_session()` instead.
2021-01-19 18:13:38.905223: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:38.905749: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:13:38.906020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:38.906561: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:38.906980: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:13:38.907022: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-19 18:13:38.907030: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2021-01-19 18:13:38.907057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2021-01-19 18:13:38.907236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:38.907806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:38.908247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15404 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
INFO:tensorflow:time(__main__.FunctionTest.testConcreteFunctionFlatSignatureErrorMissingKeywordArg): 0.05s
I0119 18:13:38.952150 140707827947264 test_util.py:2071] time(__main__.FunctionTest.testConcreteFunctionFlatSignatureErrorMissingKeywordArg): 0.05s
INFO:tensorflow:time(__main__.FunctionTest.testConcreteFunctionFlatSignatureErrorMissingKeywordArg): 0.0s
I0119 18:13:38.956553 140707827947264 test_util.py:2071] time(__main__.FunctionTest.testConcreteFunctionFlatSignatureErrorMissingKeywordArg): 0.0s
[       OK ] FunctionTest.testConcreteFunctionFlatSignatureErrorMissingKeywordArg
[ RUN      ] FunctionTest.testConcreteFunctionStructuredSignatureErrorMissingArg
2021-01-19 18:13:38.957659: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:38.958080: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:13:38.958216: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:38.958738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:38.959161: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:13:38.959207: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-19 18:13:38.959217: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2021-01-19 18:13:38.959224: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2021-01-19 18:13:38.959381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:38.959858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:38.960237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15404 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
INFO:tensorflow:time(__main__.FunctionTest.testConcreteFunctionStructuredSignatureErrorMissingArg): 0.04s
I0119 18:13:38.999924 140707827947264 test_util.py:2071] time(__main__.FunctionTest.testConcreteFunctionStructuredSignatureErrorMissingArg): 0.04s
INFO:tensorflow:time(__main__.FunctionTest.testConcreteFunctionStructuredSignatureErrorMissingArg): 0.0s
I0119 18:13:39.003569 140707827947264 test_util.py:2071] time(__main__.FunctionTest.testConcreteFunctionStructuredSignatureErrorMissingArg): 0.0s
[       OK ] FunctionTest.testConcreteFunctionStructuredSignatureErrorMissingArg
[ RUN      ] FunctionTest.testDefunAssignAddVariable
INFO:tensorflow:time(__main__.FunctionTest.testDefunAssignAddVariable): 0.04s
I0119 18:13:39.048901 140707827947264 test_util.py:2071] time(__main__.FunctionTest.testDefunAssignAddVariable): 0.04s
[       OK ] FunctionTest.testDefunAssignAddVariable
[ RUN      ] FunctionTest.testEagerCapturesDefFunction
INFO:tensorflow:time(__main__.FunctionTest.testEagerCapturesDefFunction): 0.04s
I0119 18:13:39.086303 140707827947264 test_util.py:2071] time(__main__.FunctionTest.testEagerCapturesDefFunction): 0.04s
[       OK ] FunctionTest.testEagerCapturesDefFunction
[ RUN      ] FunctionTest.testFollowTypeHintsTraceWithArgsKwOnlyArgsKwargsAndTypedKwOnlyArg
INFO:tensorflow:time(__main__.FunctionTest.testFollowTypeHintsTraceWithArgsKwOnlyArgsKwargsAndTypedKwOnlyArg): 0.06s
I0119 18:13:39.146420 140707827947264 test_util.py:2071] time(__main__.FunctionTest.testFollowTypeHintsTraceWithArgsKwOnlyArgsKwargsAndTypedKwOnlyArg): 0.06s
[       OK ] FunctionTest.testFollowTypeHintsTraceWithArgsKwOnlyArgsKwargsAndTypedKwOnlyArg
[ RUN      ] FunctionTest.testFunctionHandlesInputsOnDifferentDevices
WARNING:tensorflow:From /b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py:1451: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.config.list_physical_devices('GPU')` instead.
W0119 18:13:39.147485 140707827947264 deprecation.py:336] From /b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py:1451: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.config.list_physical_devices('GPU')` instead.
2021-01-19 18:13:39.147987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:39.148456: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:13:39.148713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:39.149280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:39.149759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:13:39.149824: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-19 18:13:39.149839: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2021-01-19 18:13:39.149864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2021-01-19 18:13:39.150138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:39.150665: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:39.151101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/device:GPU:0 with 15404 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
WARNING:tensorflow:From /b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/eager/function_test.py:1621: _EagerTensorBase.gpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.identity instead.
W0119 18:13:39.151534 140707827947264 deprecation.py:336] From /b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/eager/function_test.py:1621: _EagerTensorBase.gpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.identity instead.
WARNING:tensorflow:From /b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/eager/function_test.py:1623: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.identity instead.
W0119 18:13:39.159234 140707827947264 deprecation.py:336] From /b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/eager/function_test.py:1623: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.identity instead.
INFO:tensorflow:time(__main__.FunctionTest.testFunctionHandlesInputsOnDifferentDevices): 0.01s
I0119 18:13:39.160394 140707827947264 test_util.py:2071] time(__main__.FunctionTest.testFunctionHandlesInputsOnDifferentDevices): 0.01s
[       OK ] FunctionTest.testFunctionHandlesInputsOnDifferentDevices
[ RUN      ] FunctionTest.testGetConcreteFunctionThreadSafety
INFO:tensorflow:time(__main__.FunctionTest.testGetConcreteFunctionThreadSafety): 0.1s
I0119 18:13:39.257583 140707827947264 test_util.py:2071] time(__main__.FunctionTest.testGetConcreteFunctionThreadSafety): 0.1s
[       OK ] FunctionTest.testGetConcreteFunctionThreadSafety
[ RUN      ] FunctionTest.testInitScopeTensorInitializationInFunction
WARNING:tensorflow:From /b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py:1236: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0119 18:13:39.377433 140707827947264 deprecation.py:336] From /b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py:1236: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
INFO:tensorflow:time(__main__.FunctionTest.testInitScopeTensorInitializationInFunction): 0.13s
I0119 18:13:39.384649 140707827947264 test_util.py:2071] time(__main__.FunctionTest.testInitScopeTensorInitializationInFunction): 0.13s
[       OK ] FunctionTest.testInitScopeTensorInitializationInFunction
[ RUN      ] FunctionTest.testInputSpecGraphFunction
2021-01-19 18:13:39.429648: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10
INFO:tensorflow:time(__main__.FunctionTest.testInputSpecGraphFunction): 0.25s
I0119 18:13:39.631373 140707827947264 test_util.py:2071] time(__main__.FunctionTest.testInputSpecGraphFunction): 0.25s
[       OK ] FunctionTest.testInputSpecGraphFunction
[ RUN      ] FunctionTest.testNestedInputsGraphMode
INFO:tensorflow:time(__main__.FunctionTest.testNestedInputsGraphMode): 0.05s
I0119 18:13:39.685814 140707827947264 test_util.py:2071] time(__main__.FunctionTest.testNestedInputsGraphMode): 0.05s
[       OK ] FunctionTest.testNestedInputsGraphMode
[ RUN      ] FunctionTest.testPythonFunctionWithDefaultArgs
INFO:tensorflow:time(__main__.FunctionTest.testPythonFunctionWithDefaultArgs): 0.04s
I0119 18:13:39.727068 140707827947264 test_util.py:2071] time(__main__.FunctionTest.testPythonFunctionWithDefaultArgs): 0.04s
[       OK ] FunctionTest.testPythonFunctionWithDefaultArgs
[ RUN      ] FunctionTest.testReturnCompositeTensorWithDefunSparseTensor
INFO:tensorflow:time(__main__.FunctionTest.testReturnCompositeTensorWithDefunSparseTensor): 0.04s
I0119 18:13:39.763753 140707827947264 test_util.py:2071] time(__main__.FunctionTest.testReturnCompositeTensorWithDefunSparseTensor): 0.04s
[       OK ] FunctionTest.testReturnCompositeTensorWithDefunSparseTensor
[ RUN      ] FunctionTest.testTensorKeywordArguments
INFO:tensorflow:time(__main__.FunctionTest.testTensorKeywordArguments): 0.04s
I0119 18:13:39.804558 140707827947264 test_util.py:2071] time(__main__.FunctionTest.testTensorKeywordArguments): 0.04s
[       OK ] FunctionTest.testTensorKeywordArguments
[ RUN      ] FunctionTest.testWithModuleNameScopeRedundantArgs
INFO:tensorflow:time(__main__.FunctionTest.testWithModuleNameScopeRedundantArgs): 0.0s
I0119 18:13:39.805445 140707827947264 test_util.py:2071] time(__main__.FunctionTest.testWithModuleNameScopeRedundantArgs): 0.0s
[  SKIPPED ] FunctionTest.testWithModuleNameScopeRedundantArgs
[ RUN      ] MultiDeviceTest.testMultiDeviceResources
2021-01-19 18:13:39.806366: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:39.806817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:13:39.807022: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:39.807523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:39.807934: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:13:39.808000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-19 18:13:39.808013: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2021-01-19 18:13:39.808019: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2021-01-19 18:13:39.808225: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:39.808732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:39.809149: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/device:GPU:0 with 15404 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
INFO:tensorflow:time(__main__.MultiDeviceTest.testMultiDeviceResources): 0.08s
I0119 18:13:39.887343 140707827947264 test_util.py:2071] time(__main__.MultiDeviceTest.testMultiDeviceResources): 0.08s
[       OK ] MultiDeviceTest.testMultiDeviceResources
----------------------------------------------------------------------
Ran 17 tests in 2.429s

OK (skipped=1)
terminate called without an active exception
Fatal Python error: Aborted

Thread 0x00007ff4237a6700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff423fa7700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff4247a8700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff424fa9700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff425fab700 (most recent call first):
  File "/usr/lib/python3.6/threading.py", line 295 in wait
  File "/usr/lib/python3.6/queue.py", line 164 in get
  File "/usr/lib/python3.6/multiprocessing/pool.py", line 108 in worker
  File "/usr/lib/python3.6/threading.py", line 864 in run
  File "/usr/lib/python3.6/threading.py", line 916 in _bootstrap_inner
  File "/usr/lib/python3.6/threading.py", line 884 in _bootstrap

Thread 0x00007ff4267ac700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff426fad700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff4277ae700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff427faf700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff4287b0700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff428fb1700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff4297b2700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff429fb3700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff42a7b4700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff42afb5700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff42b7b6700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff42bfb7700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff42c7b8700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff42cfb9700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff42d7ba700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff42dfbb700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff42e7bc700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff42efbd700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff42f7be700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff42ffbf700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff4307c0700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff430fc1700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff4317c2700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff431fc3700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff4327c4700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff432fc5700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff4337c6700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff433fc7700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff4347c8700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff434fc9700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff4357ca700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff435fcb700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff4367cc700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff436fcd700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff4377ce700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff437fcf700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff4387d0700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff438fd1700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff4397d2700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff439fd3700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff43a7d4700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff43afd5700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff43b7d6700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff43bfd7700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff43c7d8700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff43cfd9700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff43d7da700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff43dfdb700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff43e7dc700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff43efdd700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff43f7de700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff43ffdf700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff4407e0700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff440fe1700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff4417e2700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff441fe3700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff4427e4700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff442fe5700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff4437e6700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff443fe7700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff4447e8700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff444fe9700 (most recent call first):
  File "/usr/lib/python3.6/threading.py", line 295 in wait
  File "/usr/lib/python3.6/queue.py", line 164 in get
  File "/usr/lib/python3.6/multiprocessing/pool.py", line 108 in worker
  File "/usr/lib/python3.6/threading.py", line 864 in run
  File "/usr/lib/python3.6/threading.py", line 916 in _bootstrap_inner
  File "/usr/lib/python3.6/threading.py", line 884 in _bootstrap

Thread 0x00007ff4457ea700 (most recent call first):
  File "/usr/lib/python3.6/threading.py", line 295 in wait
  File "/usr/lib/python3.6/queue.py", line 164 in get
  File "/usr/lib/python3.6/multiprocessing/pool.py", line 108 in worker
  File "/usr/lib/python3.6/threading.py", line 864 in run
  File "/usr/lib/python3.6/threading.py", line 916 in _bootstrap_inner
  File "/usr/lib/python3.6/threading.py", line 884 in _bootstrap

Thread 0x00007ff445feb700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff4467ec700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff447fef700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff4487f0700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff448ff1700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff4497f2700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff449ff3700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff44a7f4700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Current thread 0x00007ff44aff5700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff44b7f6700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff44bff7700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff44c7f8700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff44cff9700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff44ffff700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff464e98700 (most recent call first):
  File "/usr/lib/python3.6/threading.py", line 295 in wait
  File "/usr/lib/python3.6/queue.py", line 164 in get
  File "/usr/lib/python3.6/multiprocessing/pool.py", line 108 in worker
  File "/usr/lib/python3.6/threading.py", line 864 in run
  File "/usr/lib/python3.6/threading.py", line 916 in _bootstrap_inner
  File "/usr/lib/python3.6/threading.py", line 884 in _bootstrap

Thread 0x00007ff465699700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007ff854ff9700 (most recent call first):
  File "/usr/lib/python3.6/threading.py", line 295 in wait
  File "/usr/lib/python3.6/queue.py", line 164 in get
  File "/usr/lib/python3.6/multiprocessing/pool.py", line 108 in worker
  File "/usr/lib/python3.6/threading.py", line 864 in run
  File "/usr/lib/python3.6/threading.py", line 916 in _bootstrap_inner
  File "/usr/lib/python3.6/threading.py", line 884 in _bootstrap

Thread 0x00007ff855ffb700 (most recent call first):
  File "/usr/lib/python3.6/threading.py", line 295 in wait
  File "/usr/lib/python3.6/queue.py", line 164 in get
  File "/usr/lib/python3.6/multiprocessing/pool.py", line 108 in worker
  File "/usr/lib/python3.6/threading.py", line 864 in run
  File "/usr/lib/python3.6/threading.py", line 916 in _bootstrap_inner
  File "/usr/lib/python3.6/threading.py", line 884 in _bootstrap

Thread 0x00007ff9181a1700 (most recent call first):
*** Received signal 6 ***
*** BEGIN MANGLED STACK TRACE ***
terminate called recursively
terminate called recursively
================================================================================
==================== Test output for //tensorflow/python/eager:function_test_gpu (shard 9 of 15):
2021-01-19 18:13:43.755218: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
Running tests under Python 3.6.10: /usr/bin/python3
[ RUN      ] FunctionTest.testCacheKeyVariables
2021-01-19 18:13:47.634598: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-01-19 18:13:47.663388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:47.664543: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:13:47.664587: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
2021-01-19 18:13:47.667291: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10
2021-01-19 18:13:47.667376: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10
2021-01-19 18:13:47.669691: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-01-19 18:13:47.670236: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-01-19 18:13:47.672991: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-01-19 18:13:47.674542: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10
2021-01-19 18:13:47.680759: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7
2021-01-19 18:13:47.680989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:47.682197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:47.683238: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:13:47.706795: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-01-19 18:13:47.707201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:47.708149: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:13:47.708290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:47.709457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:47.710592: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:13:47.710669: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
2021-01-19 18:13:48.702722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-19 18:13:48.702769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2021-01-19 18:13:48.702779: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2021-01-19 18:13:48.703123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:48.704271: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:48.705319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:48.706315: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15405 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
2021-01-19 18:13:49.152702: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:128] None of the MLIR optimization passes are enabled (registered 2)
2021-01-19 18:13:49.153775: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2300000000 Hz
INFO:tensorflow:time(__main__.FunctionTest.testCacheKeyVariables): 1.6s
I0119 18:13:49.234132 139764566910720 test_util.py:2071] time(__main__.FunctionTest.testCacheKeyVariables): 1.6s
[       OK ] FunctionTest.testCacheKeyVariables
[ RUN      ] FunctionTest.testCapturesVariables
INFO:tensorflow:time(__main__.FunctionTest.testCapturesVariables): 0.15s
I0119 18:13:49.381413 139764566910720 test_util.py:2071] time(__main__.FunctionTest.testCapturesVariables): 0.15s
[       OK ] FunctionTest.testCapturesVariables
[ RUN      ] FunctionTest.testConcreteFunctionFlatSignatureErrorMissingKeywordArg
WARNING:tensorflow:From /usr/lib/python3.6/contextlib.py:60: TensorFlowTestCase.test_session (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.
Instructions for updating:
Use `self.session()` or `self.cached_session()` instead.
W0119 18:13:49.382976 139764566910720 deprecation.py:336] From /usr/lib/python3.6/contextlib.py:60: TensorFlowTestCase.test_session (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.
Instructions for updating:
Use `self.session()` or `self.cached_session()` instead.
2021-01-19 18:13:49.383907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:49.384426: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:13:49.384574: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:49.385132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:49.385592: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:13:49.385657: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-19 18:13:49.385667: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2021-01-19 18:13:49.385674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2021-01-19 18:13:49.385857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:49.386422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:49.386899: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15405 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
INFO:tensorflow:time(__main__.FunctionTest.testConcreteFunctionFlatSignatureErrorMissingKeywordArg): 0.06s
I0119 18:13:49.445806 139764566910720 test_util.py:2071] time(__main__.FunctionTest.testConcreteFunctionFlatSignatureErrorMissingKeywordArg): 0.06s
INFO:tensorflow:time(__main__.FunctionTest.testConcreteFunctionFlatSignatureErrorMissingKeywordArg): 0.01s
I0119 18:13:49.452052 139764566910720 test_util.py:2071] time(__main__.FunctionTest.testConcreteFunctionFlatSignatureErrorMissingKeywordArg): 0.01s
[       OK ] FunctionTest.testConcreteFunctionFlatSignatureErrorMissingKeywordArg
[ RUN      ] FunctionTest.testConcreteFunctionStructuredSignatureErrorMissingArg
2021-01-19 18:13:49.453648: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:49.454193: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:13:49.454341: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:49.454900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:49.455364: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:13:49.455424: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-19 18:13:49.455445: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2021-01-19 18:13:49.455454: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2021-01-19 18:13:49.455632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:49.456179: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:49.456690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15405 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
INFO:tensorflow:time(__main__.FunctionTest.testConcreteFunctionStructuredSignatureErrorMissingArg): 0.06s
I0119 18:13:49.514907 139764566910720 test_util.py:2071] time(__main__.FunctionTest.testConcreteFunctionStructuredSignatureErrorMissingArg): 0.06s
INFO:tensorflow:time(__main__.FunctionTest.testConcreteFunctionStructuredSignatureErrorMissingArg): 0.0s
I0119 18:13:49.520267 139764566910720 test_util.py:2071] time(__main__.FunctionTest.testConcreteFunctionStructuredSignatureErrorMissingArg): 0.0s
[       OK ] FunctionTest.testConcreteFunctionStructuredSignatureErrorMissingArg
[ RUN      ] FunctionTest.testDefunAssignAddVariable
INFO:tensorflow:time(__main__.FunctionTest.testDefunAssignAddVariable): 0.06s
I0119 18:13:49.585137 139764566910720 test_util.py:2071] time(__main__.FunctionTest.testDefunAssignAddVariable): 0.06s
[       OK ] FunctionTest.testDefunAssignAddVariable
[ RUN      ] FunctionTest.testEagerCapturesDefFunction
INFO:tensorflow:time(__main__.FunctionTest.testEagerCapturesDefFunction): 0.05s
I0119 18:13:49.640058 139764566910720 test_util.py:2071] time(__main__.FunctionTest.testEagerCapturesDefFunction): 0.05s
[       OK ] FunctionTest.testEagerCapturesDefFunction
[ RUN      ] FunctionTest.testFollowTypeHintsTraceWithArgsKwOnlyArgsKwargsAndTypedKwOnlyArg
INFO:tensorflow:time(__main__.FunctionTest.testFollowTypeHintsTraceWithArgsKwOnlyArgsKwargsAndTypedKwOnlyArg): 0.08s
I0119 18:13:49.721313 139764566910720 test_util.py:2071] time(__main__.FunctionTest.testFollowTypeHintsTraceWithArgsKwOnlyArgsKwargsAndTypedKwOnlyArg): 0.08s
[       OK ] FunctionTest.testFollowTypeHintsTraceWithArgsKwOnlyArgsKwargsAndTypedKwOnlyArg
[ RUN      ] FunctionTest.testFunctionHandlesInputsOnDifferentDevices
WARNING:tensorflow:From /b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py:1451: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.config.list_physical_devices('GPU')` instead.
W0119 18:13:49.722929 139764566910720 deprecation.py:336] From /b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py:1451: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.config.list_physical_devices('GPU')` instead.
2021-01-19 18:13:49.723720: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:49.724335: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:13:49.724726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:49.725395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:49.725967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:13:49.726018: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-19 18:13:49.726165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2021-01-19 18:13:49.726277: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2021-01-19 18:13:49.726651: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:49.727418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:49.728092: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/device:GPU:0 with 15405 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
WARNING:tensorflow:From /b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/eager/function_test.py:1621: _EagerTensorBase.gpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.identity instead.
W0119 18:13:49.728929 139764566910720 deprecation.py:336] From /b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/eager/function_test.py:1621: _EagerTensorBase.gpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.identity instead.
WARNING:tensorflow:From /b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/eager/function_test.py:1623: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.identity instead.
W0119 18:13:49.739373 139764566910720 deprecation.py:336] From /b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/eager/function_test.py:1623: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.identity instead.
INFO:tensorflow:time(__main__.FunctionTest.testFunctionHandlesInputsOnDifferentDevices): 0.02s
I0119 18:13:49.740902 139764566910720 test_util.py:2071] time(__main__.FunctionTest.testFunctionHandlesInputsOnDifferentDevices): 0.02s
[       OK ] FunctionTest.testFunctionHandlesInputsOnDifferentDevices
[ RUN      ] FunctionTest.testGetConcreteFunctionThreadSafety
INFO:tensorflow:time(__main__.FunctionTest.testGetConcreteFunctionThreadSafety): 0.14s
I0119 18:13:49.879237 139764566910720 test_util.py:2071] time(__main__.FunctionTest.testGetConcreteFunctionThreadSafety): 0.14s
[       OK ] FunctionTest.testGetConcreteFunctionThreadSafety
[ RUN      ] FunctionTest.testInitScopeTensorInitializationInFunction
WARNING:tensorflow:From /b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py:1236: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0119 18:13:50.024661 139764566910720 deprecation.py:336] From /b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py:1236: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
INFO:tensorflow:time(__main__.FunctionTest.testInitScopeTensorInitializationInFunction): 0.15s
I0119 18:13:50.033929 139764566910720 test_util.py:2071] time(__main__.FunctionTest.testInitScopeTensorInitializationInFunction): 0.15s
[       OK ] FunctionTest.testInitScopeTensorInitializationInFunction
[ RUN      ] FunctionTest.testInputSpecGraphFunction
2021-01-19 18:13:50.096499: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10
INFO:tensorflow:time(__main__.FunctionTest.testInputSpecGraphFunction): 0.3s
I0119 18:13:50.338117 139764566910720 test_util.py:2071] time(__main__.FunctionTest.testInputSpecGraphFunction): 0.3s
[       OK ] FunctionTest.testInputSpecGraphFunction
[ RUN      ] FunctionTest.testNestedInputsGraphMode
INFO:tensorflow:time(__main__.FunctionTest.testNestedInputsGraphMode): 0.08s
I0119 18:13:50.418534 139764566910720 test_util.py:2071] time(__main__.FunctionTest.testNestedInputsGraphMode): 0.08s
[       OK ] FunctionTest.testNestedInputsGraphMode
[ RUN      ] FunctionTest.testPythonFunctionWithDefaultArgs
INFO:tensorflow:time(__main__.FunctionTest.testPythonFunctionWithDefaultArgs): 0.05s
I0119 18:13:50.475173 139764566910720 test_util.py:2071] time(__main__.FunctionTest.testPythonFunctionWithDefaultArgs): 0.05s
[       OK ] FunctionTest.testPythonFunctionWithDefaultArgs
[ RUN      ] FunctionTest.testReturnCompositeTensorWithDefunSparseTensor
INFO:tensorflow:time(__main__.FunctionTest.testReturnCompositeTensorWithDefunSparseTensor): 0.05s
I0119 18:13:50.530288 139764566910720 test_util.py:2071] time(__main__.FunctionTest.testReturnCompositeTensorWithDefunSparseTensor): 0.05s
[       OK ] FunctionTest.testReturnCompositeTensorWithDefunSparseTensor
[ RUN      ] FunctionTest.testTensorKeywordArguments
INFO:tensorflow:time(__main__.FunctionTest.testTensorKeywordArguments): 0.06s
I0119 18:13:50.586894 139764566910720 test_util.py:2071] time(__main__.FunctionTest.testTensorKeywordArguments): 0.06s
[       OK ] FunctionTest.testTensorKeywordArguments
[ RUN      ] FunctionTest.testWithModuleNameScopeRedundantArgs
INFO:tensorflow:time(__main__.FunctionTest.testWithModuleNameScopeRedundantArgs): 0.0s
I0119 18:13:50.588210 139764566910720 test_util.py:2071] time(__main__.FunctionTest.testWithModuleNameScopeRedundantArgs): 0.0s
[  SKIPPED ] FunctionTest.testWithModuleNameScopeRedundantArgs
[ RUN      ] MultiDeviceTest.testMultiDeviceResources
2021-01-19 18:13:50.589591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:50.590250: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:13:50.590495: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:50.591185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:50.591724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:13:50.591797: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-19 18:13:50.591809: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2021-01-19 18:13:50.591817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2021-01-19 18:13:50.592134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:50.592773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:50.593356: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/device:GPU:0 with 15405 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
INFO:tensorflow:time(__main__.MultiDeviceTest.testMultiDeviceResources): 0.11s
I0119 18:13:50.703020 139764566910720 test_util.py:2071] time(__main__.MultiDeviceTest.testMultiDeviceResources): 0.11s
[       OK ] MultiDeviceTest.testMultiDeviceResources
----------------------------------------------------------------------
Ran 17 tests in 3.071s

OK (skipped=1)
terminate called without an active exception
Fatal Python error: Aborted

Thread 0x00007f18827a4700 (most recent call first):
  File "/usr/lib/python3.6/threading.py", line 295 in wait
  File "/usr/lib/python3.6/queue.py", line 164 in get
  File "/usr/lib/python3.6/multiprocessing/pool.py", line 108 in worker
  File "/usr/lib/python3.6/threading.py", line 864 in run
  File "/usr/lib/python3.6/threading.py", line 916 in _bootstrap_inner
  File "/usr/lib/python3.6/threading.py", line 884 in _bootstrap

Thread 0x00007f18847a8700 (most recent call first):
  File "/usr/lib/python3.6/threading.py", line 295 in wait
  File "/usr/lib/python3.6/queue.py", line 164 in get
  File "/usr/lib/python3.6/multiprocessing/pool.py", line 108 in worker
  File "/usr/lib/python3.6/threading.py", line 864 in run
  File "/usr/lib/python3.6/threading.py", line 916 in _bootstrap_inner
  File "/usr/lib/python3.6/threading.py", line 884 in _bootstrap

Thread 0x00007f18877ae700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007f18887b0700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007f188bfb7700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007f188cfb9700 (most recent call first):
  File "/usr/lib/python3.6/threading.py", line 295 in wait
  File "/usr/lib/python3.6/queue.py", line 164 in get
  File "/usr/lib/python3.6/multiprocessing/pool.py", line 108 in worker
  File "/usr/lib/python3.6/threading.py", line 864 in run
  File "/usr/lib/python3.6/threading.py", line 916 in _bootstrap_inner
  File "/usr/lib/python3.6/threading.py", line 884 in _bootstrap

Thread 0x00007f188d7ba700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Current thread 0x00007f18a6fed700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007f18a87f0700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007f18ad7fa700 (most recent call first):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/c_api_util.py", line 58 in __del__

Thread 0x00007f1d795ce700 (most recent call first):
*** Received signal 6 ***
*** BEGIN MANGLED STACK TRACE ***
/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/../libtensorflow_framework.so.2(+0x11bf75d)[0x7f1d55bc975d]
/lib/x86_64-linux-gnu/libpthread.so.0(+0x11390)[0x7f1d791ad390]
/lib/x86_64-linux-gnu/libpthread.so.0(raise+0x29)[0x7f1d791ad269]
/lib/x86_64-linux-gnu/libpthread.so.0(+0x11390)[0x7f1d791ad390]
/lib/x86_64-linux-gnu/libc.so.6(gsignal+0x38)[0x7f1d782b4428]
/lib/x86_64-linux-gnu/libc.so.6(abort+0x16a)[0x7f1d782b602a]
/usr/lib/x86_64-linux-gnu/libstdc++.so.6(_ZN9__gnu_cxx27__verbose_terminate_handlerEv+0x16d)[0x7f1d6e68c84d]
/usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0x8d6b6)[0x7f1d6e68a6b6]
/usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0x8d701)[0x7f1d6e68a701]
/usr/lib/x86_64-linux-gnu/libstdc++.so.6(__gxx_personality_v0+0x5c0)[0x7f1d6e68a2e0]
/lib/x86_64-linux-gnu/libgcc_s.so.1(+0x10059)[0x7f1d6e3f7059]
/lib/x86_64-linux-gnu/libgcc_s.so.1(_Unwind_ForcedUnwind+0x64)[0x7f1d6e3f73b4]
/lib/x86_64-linux-gnu/libpthread.so.0(__pthread_unwind+0x40)[0x7f1d791ac070]
/lib/x86_64-linux-gnu/libpthread.so.0(+0x8845)[0x7f1d791a4845]
/usr/bin/python3[0x5ef305]
/usr/bin/python3[0x4794cd]
/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/_pywrap_tf_session.so(+0x2e03c)[0x7f1d53e8a03c]
/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/_pywrap_tf_session.so(+0x49773)[0x7f1d53ea5773]
/b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/_pywrap_tf_session.so(+0x3efa1)[0x7f1d53e9afa1]
/usr/bin/python3(_PyCFunction_FastCallKeywords+0x1eb)[0x52367b]
/usr/bin/python3[0x57dbf9]
/usr/bin/python3(_PyEval_EvalFrameDefault+0x45f)[0x57642f]
/usr/bin/python3(_PyFunction_FastCallDict+0x133)[0x57f563]
/usr/bin/python3(_PyObject_Call_Prepend+0x24c)[0x4e839c]
/usr/bin/python3(_PyObject_FastCallDict+0xeb)[0x4e726b]
/usr/bin/python3[0x539f11]
/usr/bin/python3(PyObject_CallFinalizerFromDealloc+0x42)[0x525222]
/usr/bin/python3[0x536d87]
/usr/bin/python3[0x5175cb]
/usr/bin/python3[0x536b2d]
/usr/bin/python3[0x517497]
/usr/bin/python3[0x5172e0]
/usr/bin/python3(_PyDict_DelItem_KnownHash+0x9d)[0x60213d]
/usr/bin/python3[0x48f928]
/usr/bin/python3(_PyCFunction_FastCallDict+0x165)[0x5239f5]
/usr/bin/python3(PyObject_CallFunctionObjArgs+0x2c0)[0x4e8710]
/usr/bin/python3(PyObject_ClearWeakRefs+0x130)[0x55fbc0]
/usr/bin/python3[0x48f7f1]
/usr/bin/python3[0x517497]
/usr/bin/python3(PyThreadState_Clear+0x112)[0x5ea6f2]
/usr/bin/python3[0x6213ae]
/usr/bin/python3[0x5ef2d4]
/lib/x86_64-linux-gnu/libpthread.so.0(+0x76ba)[0x7f1d791a36ba]
/lib/x86_64-linux-gnu/libc.so.6(clone+0x6d)[0x7f1d7838641d]
*** END MANGLED STACK TRACE ***

*** Begin stack trace ***
	tensorflow::CurrentStackTrace()
	
	
	raise
	
	gsignal
	abort
	__gnu_cxx::__verbose_terminate_handler()
	
	
	__gxx_personality_v0
	
	_Unwind_ForcedUnwind
	__pthread_unwind
	
	
	
	
	
	
	_PyCFunction_FastCallKeywords
	
	_PyEval_EvalFrameDefault
	_PyFunction_FastCallDict
	_PyObject_Call_Prepend
	_PyObject_FastCallDict
	
	PyObject_CallFinalizerFromDealloc
	
	
	
	
	
	_PyDict_DelItem_KnownHash
	
	_PyCFunction_FastCallDict
	PyObject_CallFunctionObjArgs
	PyObject_ClearWeakRefs
	
	
	PyThreadState_Clear
	
	
	
	clone
*** End stack trace ***
================================================================================
==================== Test output for //tensorflow/python/eager:function_test_gpu (shard 9 of 15):
2021-01-19 18:13:53.926076: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
Running tests under Python 3.6.10: /usr/bin/python3
[ RUN      ] FunctionTest.testCacheKeyVariables
2021-01-19 18:13:58.011242: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-01-19 18:13:58.030151: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:58.069236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:13:58.069318: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
2021-01-19 18:13:58.148917: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10
2021-01-19 18:13:58.148989: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10
2021-01-19 18:13:58.181436: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-01-19 18:13:58.181918: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-01-19 18:13:58.214668: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-01-19 18:13:58.222167: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10
2021-01-19 18:13:58.249722: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7
2021-01-19 18:13:58.250047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:58.250889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:58.251647: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:13:58.270364: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-01-19 18:13:58.270734: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:58.271563: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:13:58.271808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:58.272584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:58.273271: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:13:58.273334: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
2021-01-19 18:13:59.149329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-19 18:13:59.149369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2021-01-19 18:13:59.149380: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2021-01-19 18:13:59.149652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:59.150486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:59.151266: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:59.151954: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15404 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
2021-01-19 18:13:59.531599: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:128] None of the MLIR optimization passes are enabled (registered 2)
2021-01-19 18:13:59.532359: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2299995000 Hz
INFO:tensorflow:time(__main__.FunctionTest.testCacheKeyVariables): 1.58s
I0119 18:13:59.592451 139849670387456 test_util.py:2071] time(__main__.FunctionTest.testCacheKeyVariables): 1.58s
[       OK ] FunctionTest.testCacheKeyVariables
[ RUN      ] FunctionTest.testCapturesVariables
INFO:tensorflow:time(__main__.FunctionTest.testCapturesVariables): 0.1s
I0119 18:13:59.694967 139849670387456 test_util.py:2071] time(__main__.FunctionTest.testCapturesVariables): 0.1s
[       OK ] FunctionTest.testCapturesVariables
[ RUN      ] FunctionTest.testConcreteFunctionFlatSignatureErrorMissingKeywordArg
WARNING:tensorflow:From /usr/lib/python3.6/contextlib.py:60: TensorFlowTestCase.test_session (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.
Instructions for updating:
Use `self.session()` or `self.cached_session()` instead.
W0119 18:13:59.695729 139849670387456 deprecation.py:336] From /usr/lib/python3.6/contextlib.py:60: TensorFlowTestCase.test_session (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.
Instructions for updating:
Use `self.session()` or `self.cached_session()` instead.
2021-01-19 18:13:59.696488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:59.696900: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:13:59.697054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:59.697498: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:59.697859: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:13:59.697901: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-19 18:13:59.697910: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2021-01-19 18:13:59.697920: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2021-01-19 18:13:59.698061: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:59.698499: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:59.698870: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15404 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
INFO:tensorflow:time(__main__.FunctionTest.testConcreteFunctionFlatSignatureErrorMissingKeywordArg): 0.04s
I0119 18:13:59.738771 139849670387456 test_util.py:2071] time(__main__.FunctionTest.testConcreteFunctionFlatSignatureErrorMissingKeywordArg): 0.04s
INFO:tensorflow:time(__main__.FunctionTest.testConcreteFunctionFlatSignatureErrorMissingKeywordArg): 0.0s
I0119 18:13:59.743260 139849670387456 test_util.py:2071] time(__main__.FunctionTest.testConcreteFunctionFlatSignatureErrorMissingKeywordArg): 0.0s
[       OK ] FunctionTest.testConcreteFunctionFlatSignatureErrorMissingKeywordArg
[ RUN      ] FunctionTest.testConcreteFunctionStructuredSignatureErrorMissingArg
2021-01-19 18:13:59.744183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:59.744574: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:13:59.744692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:59.745117: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:59.745507: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:13:59.745546: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-19 18:13:59.745554: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2021-01-19 18:13:59.745561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2021-01-19 18:13:59.745698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:59.746161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:59.746553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15404 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
INFO:tensorflow:time(__main__.FunctionTest.testConcreteFunctionStructuredSignatureErrorMissingArg): 0.04s
I0119 18:13:59.786637 139849670387456 test_util.py:2071] time(__main__.FunctionTest.testConcreteFunctionStructuredSignatureErrorMissingArg): 0.04s
INFO:tensorflow:time(__main__.FunctionTest.testConcreteFunctionStructuredSignatureErrorMissingArg): 0.0s
I0119 18:13:59.790238 139849670387456 test_util.py:2071] time(__main__.FunctionTest.testConcreteFunctionStructuredSignatureErrorMissingArg): 0.0s
[       OK ] FunctionTest.testConcreteFunctionStructuredSignatureErrorMissingArg
[ RUN      ] FunctionTest.testDefunAssignAddVariable
INFO:tensorflow:time(__main__.FunctionTest.testDefunAssignAddVariable): 0.04s
I0119 18:13:59.835119 139849670387456 test_util.py:2071] time(__main__.FunctionTest.testDefunAssignAddVariable): 0.04s
[       OK ] FunctionTest.testDefunAssignAddVariable
[ RUN      ] FunctionTest.testEagerCapturesDefFunction
INFO:tensorflow:time(__main__.FunctionTest.testEagerCapturesDefFunction): 0.04s
I0119 18:13:59.873409 139849670387456 test_util.py:2071] time(__main__.FunctionTest.testEagerCapturesDefFunction): 0.04s
[       OK ] FunctionTest.testEagerCapturesDefFunction
[ RUN      ] FunctionTest.testFollowTypeHintsTraceWithArgsKwOnlyArgsKwargsAndTypedKwOnlyArg
INFO:tensorflow:time(__main__.FunctionTest.testFollowTypeHintsTraceWithArgsKwOnlyArgsKwargsAndTypedKwOnlyArg): 0.06s
I0119 18:13:59.935235 139849670387456 test_util.py:2071] time(__main__.FunctionTest.testFollowTypeHintsTraceWithArgsKwOnlyArgsKwargsAndTypedKwOnlyArg): 0.06s
[       OK ] FunctionTest.testFollowTypeHintsTraceWithArgsKwOnlyArgsKwargsAndTypedKwOnlyArg
[ RUN      ] FunctionTest.testFunctionHandlesInputsOnDifferentDevices
WARNING:tensorflow:From /b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py:1451: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.config.list_physical_devices('GPU')` instead.
W0119 18:13:59.936203 139849670387456 deprecation.py:336] From /b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py:1451: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.config.list_physical_devices('GPU')` instead.
2021-01-19 18:13:59.936699: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:59.937191: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:13:59.937412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:59.937946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:59.938379: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:13:59.938417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-19 18:13:59.938425: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2021-01-19 18:13:59.938431: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2021-01-19 18:13:59.938649: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:59.939210: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:59.939691: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/device:GPU:0 with 15404 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
WARNING:tensorflow:From /b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/eager/function_test.py:1621: _EagerTensorBase.gpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.identity instead.
W0119 18:13:59.940098 139849670387456 deprecation.py:336] From /b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/eager/function_test.py:1621: _EagerTensorBase.gpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.identity instead.
WARNING:tensorflow:From /b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/eager/function_test.py:1623: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.identity instead.
W0119 18:13:59.947457 139849670387456 deprecation.py:336] From /b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/eager/function_test.py:1623: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.identity instead.
INFO:tensorflow:time(__main__.FunctionTest.testFunctionHandlesInputsOnDifferentDevices): 0.01s
I0119 18:13:59.948384 139849670387456 test_util.py:2071] time(__main__.FunctionTest.testFunctionHandlesInputsOnDifferentDevices): 0.01s
[       OK ] FunctionTest.testFunctionHandlesInputsOnDifferentDevices
[ RUN      ] FunctionTest.testGetConcreteFunctionThreadSafety
INFO:tensorflow:time(__main__.FunctionTest.testGetConcreteFunctionThreadSafety): 0.09s
I0119 18:14:00.041183 139849670387456 test_util.py:2071] time(__main__.FunctionTest.testGetConcreteFunctionThreadSafety): 0.09s
[       OK ] FunctionTest.testGetConcreteFunctionThreadSafety
[ RUN      ] FunctionTest.testInitScopeTensorInitializationInFunction
WARNING:tensorflow:From /b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py:1236: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
W0119 18:14:00.158464 139849670387456 deprecation.py:336] From /b/f/w/bazel-out/k8-opt/bin/tensorflow/python/eager/function_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py:1236: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.
Instructions for updating:
tf.py_func is deprecated in TF V2. Instead, there are two
    options available in V2.
    - tf.py_function takes a python function which manipulates tf eager
    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to
    an ndarray (just call tensor.numpy()) but having access to eager tensors
    means `tf.py_function`s can use accelerators such as GPUs as well as
    being differentiable using a gradient tape.
    - tf.numpy_function maintains the semantics of the deprecated tf.py_func
    (it is not differentiable, and manipulates numpy arrays). It drops the
    stateful argument making all functions stateful.
    
INFO:tensorflow:time(__main__.FunctionTest.testInitScopeTensorInitializationInFunction): 0.12s
I0119 18:14:00.165366 139849670387456 test_util.py:2071] time(__main__.FunctionTest.testInitScopeTensorInitializationInFunction): 0.12s
[       OK ] FunctionTest.testInitScopeTensorInitializationInFunction
[ RUN      ] FunctionTest.testInputSpecGraphFunction
2021-01-19 18:14:00.209014: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10
INFO:tensorflow:time(__main__.FunctionTest.testInputSpecGraphFunction): 1.76s
I0119 18:14:01.927727 139849670387456 test_util.py:2071] time(__main__.FunctionTest.testInputSpecGraphFunction): 1.76s
[       OK ] FunctionTest.testInputSpecGraphFunction
[ RUN      ] FunctionTest.testNestedInputsGraphMode
INFO:tensorflow:time(__main__.FunctionTest.testNestedInputsGraphMode): 0.05s
I0119 18:14:01.981351 139849670387456 test_util.py:2071] time(__main__.FunctionTest.testNestedInputsGraphMode): 0.05s
[       OK ] FunctionTest.testNestedInputsGraphMode
[ RUN      ] FunctionTest.testPythonFunctionWithDefaultArgs
INFO:tensorflow:time(__main__.FunctionTest.testPythonFunctionWithDefaultArgs): 0.04s
I0119 18:14:02.020322 139849670387456 test_util.py:2071] time(__main__.FunctionTest.testPythonFunctionWithDefaultArgs): 0.04s
[       OK ] FunctionTest.testPythonFunctionWithDefaultArgs
[ RUN      ] FunctionTest.testReturnCompositeTensorWithDefunSparseTensor
INFO:tensorflow:time(__main__.FunctionTest.testReturnCompositeTensorWithDefunSparseTensor): 0.04s
I0119 18:14:02.056556 139849670387456 test_util.py:2071] time(__main__.FunctionTest.testReturnCompositeTensorWithDefunSparseTensor): 0.04s
[       OK ] FunctionTest.testReturnCompositeTensorWithDefunSparseTensor
[ RUN      ] FunctionTest.testTensorKeywordArguments
INFO:tensorflow:time(__main__.FunctionTest.testTensorKeywordArguments): 0.04s
I0119 18:14:02.096585 139849670387456 test_util.py:2071] time(__main__.FunctionTest.testTensorKeywordArguments): 0.04s
[       OK ] FunctionTest.testTensorKeywordArguments
[ RUN      ] FunctionTest.testWithModuleNameScopeRedundantArgs
INFO:tensorflow:time(__main__.FunctionTest.testWithModuleNameScopeRedundantArgs): 0.0s
I0119 18:14:02.097367 139849670387456 test_util.py:2071] time(__main__.FunctionTest.testWithModuleNameScopeRedundantArgs): 0.0s
[  SKIPPED ] FunctionTest.testWithModuleNameScopeRedundantArgs
[ RUN      ] MultiDeviceTest.testMultiDeviceResources
2021-01-19 18:14:02.098032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:02.098447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:14:02.098609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:02.099070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:02.099445: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:14:02.099487: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-19 18:14:02.099495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2021-01-19 18:14:02.099501: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2021-01-19 18:14:02.099641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:02.100082: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:02.100465: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/device:GPU:0 with 15404 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
INFO:tensorflow:time(__main__.MultiDeviceTest.testMultiDeviceResources): 0.08s
I0119 18:14:02.178006 139849670387456 test_util.py:2071] time(__main__.MultiDeviceTest.testMultiDeviceResources): 0.08s
[       OK ] MultiDeviceTest.testMultiDeviceResources
----------------------------------------------------------------------
Ran 17 tests in 4.169s

OK (skipped=1)
terminate called without an active exception
Fatal Python error: terminate called recursively
terminate called recursively
*** Received signal 6 ***
*** BEGIN MANGLED STACK TRACE ***
================================================================================
FAIL: //tensorflow/compiler/tests:image_ops_test_gpu (shard 5 of 10) (see /tmpfs/bazel_output/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/compiler/tests/image_ops_test_gpu/shard_5_of_10/test_attempts/attempt_2.log)
FAIL: //tensorflow/compiler/tests:image_ops_test_gpu (shard 5 of 10) (see /tmpfs/bazel_output/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/compiler/tests/image_ops_test_gpu/shard_5_of_10/test.log)

FAILED: //tensorflow/compiler/tests:image_ops_test_gpu (Summary)
      /tmpfs/bazel_output/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/compiler/tests/image_ops_test_gpu/shard_5_of_10/test.log
      /tmpfs/bazel_output/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/compiler/tests/image_ops_test_gpu/shard_5_of_10/test_attempts/attempt_1.log
      /tmpfs/bazel_output/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/compiler/tests/image_ops_test_gpu/shard_5_of_10/test_attempts/attempt_2.log
INFO: From Testing //tensorflow/compiler/tests:image_ops_test_gpu (shard 5 of 10):
==================== Test output for //tensorflow/compiler/tests:image_ops_test_gpu (shard 5 of 10):
2021-01-19 18:13:32.240398: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
Running tests under Python 3.6.10: /usr/bin/python3
WARNING:tensorflow:From /b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/compiler/tests/xla_test.py:87: Context.enable_xla_devices (from tensorflow.python.eager.context) is deprecated and will be removed in a future version.
Instructions for updating:
XLA:CPU and XLA:GPU devices are deprecated
W0119 18:13:34.708070 140566812481280 deprecation.py:336] From /b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/compiler/tests/xla_test.py:87: Context.enable_xla_devices (from tensorflow.python.eager.context) is deprecated and will be removed in a future version.
Instructions for updating:
XLA:CPU and XLA:GPU devices are deprecated
[ RUN      ] AdjustContrastTest.test_session
[  SKIPPED ] AdjustContrastTest.test_session
[ RUN      ] AdjustSaturationTest.testTwiceSaturation
INFO:tensorflow:Start test case: AdjustSaturationTest.testTwiceSaturation
I0119 18:13:34.716844 140566812481280 xla_test.py:199] Start test case: AdjustSaturationTest.testTwiceSaturation
2021-01-19 18:13:34.742308: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-01-19 18:13:34.771559: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:34.772665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:13:34.772724: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
2021-01-19 18:13:34.775742: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10
2021-01-19 18:13:34.775814: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10
2021-01-19 18:13:34.778482: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-01-19 18:13:34.779088: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-01-19 18:13:34.781813: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-01-19 18:13:34.783499: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10
2021-01-19 18:13:34.789243: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7
2021-01-19 18:13:34.789433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:34.790599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:34.791728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:13:34.792501: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-01-19 18:13:34.809949: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2300000000 Hz
2021-01-19 18:13:34.810341: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x39022b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-01-19 18:13:34.810420: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): Host, Default Version
2021-01-19 18:13:35.175880: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:35.177181: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x3904d40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2021-01-19 18:13:35.177255: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0
2021-01-19 18:13:35.177748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:35.178868: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:13:35.179033: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:35.180241: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:35.181308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:13:35.181406: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
2021-01-19 18:13:35.847699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-19 18:13:35.848446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2021-01-19 18:13:35.848491: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2021-01-19 18:13:35.849194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:35.850602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:35.851724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15405 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
2021-01-19 18:13:35.862062: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:260] None of the MLIR optimization passes are enabled (registered 0 passes)
2021-01-19 18:13:36.227818: I tensorflow/compiler/jit/xla_device.cc:399] XLA_GPU and XLA_CPU devices are deprecated and will be removed in subsequent releases. Instead, use either @tf.function(jit_compile=True) for must-compile semantics, or run with TF_XLA_FLAGS=--tf_xla_auto_jit=2 for auto-clustering best-effort compilation.
2021-01-19 18:13:36.453494: I tensorflow/compiler/jit/xla_compilation_cache.cc:321] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
INFO:tensorflow:time(__main__.AdjustSaturationTest.testTwiceSaturation): 1.77s
I0119 18:13:36.486742 140566812481280 test_util.py:2071] time(__main__.AdjustSaturationTest.testTwiceSaturation): 1.77s
INFO:tensorflow:End test case: testTwiceSaturation
I0119 18:13:36.487815 140566812481280 xla_test.py:206] End test case: testTwiceSaturation
[       OK ] AdjustSaturationTest.testTwiceSaturation
[ RUN      ] BatchedNonMaxSuppressionCorrectnessTest.testBatchedNMSSingleFrom6Max3
INFO:tensorflow:Start test case: BatchedNonMaxSuppressionCorrectnessTest.testBatchedNMSSingleFrom6Max3
I0119 18:13:36.489756 140566812481280 xla_test.py:199] Start test case: BatchedNonMaxSuppressionCorrectnessTest.testBatchedNMSSingleFrom6Max3
2021-01-19 18:13:36.491301: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:36.491934: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:13:36.492171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:36.492843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:36.493403: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:13:36.493462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-19 18:13:36.493474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2021-01-19 18:13:36.493481: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2021-01-19 18:13:36.493756: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:36.494538: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:36.495126: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15405 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
INFO:tensorflow:time(__main__.BatchedNonMaxSuppressionCorrectnessTest.testBatchedNMSSingleFrom6Max3): 4.39s
I0119 18:13:40.878840 140566812481280 test_util.py:2071] time(__main__.BatchedNonMaxSuppressionCorrectnessTest.testBatchedNMSSingleFrom6Max3): 4.39s
INFO:tensorflow:End test case: testBatchedNMSSingleFrom6Max3
I0119 18:13:40.879781 140566812481280 xla_test.py:206] End test case: testBatchedNMSSingleFrom6Max3
[       OK ] BatchedNonMaxSuppressionCorrectnessTest.testBatchedNMSSingleFrom6Max3
[ RUN      ] NonMaxSuppressionTest.test_scope
INFO:tensorflow:Start test case: NonMaxSuppressionTest.test_scope
I0119 18:13:40.881670 140566812481280 xla_test.py:199] Start test case: NonMaxSuppressionTest.test_scope
INFO:tensorflow:time(__main__.NonMaxSuppressionTest.test_scope): 0.0s
I0119 18:13:40.882431 140566812481280 test_util.py:2071] time(__main__.NonMaxSuppressionTest.test_scope): 0.0s
INFO:tensorflow:End test case: test_scope
I0119 18:13:40.882941 140566812481280 xla_test.py:206] End test case: test_scope
[       OK ] NonMaxSuppressionTest.test_scope
[ RUN      ] ResizeBilinearGradTest.test120x120To545x545
INFO:tensorflow:Start test case: ResizeBilinearGradTest.test120x120To545x545
I0119 18:13:40.884294 140566812481280 xla_test.py:199] Start test case: ResizeBilinearGradTest.test120x120To545x545
2021-01-19 18:13:40.932008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:40.932709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:13:40.933019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:40.933696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:40.934352: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:13:40.934493: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-19 18:13:40.934611: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2021-01-19 18:13:40.934660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2021-01-19 18:13:40.935008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:40.935777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:40.936327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15405 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
2021-01-19 18:13:40.950866: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10
2021-01-19 18:13:41.176504: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7
2021-01-19 18:13:42.006001: I tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Loaded cuDNN version 7605
INFO:tensorflow:time(__main__.ResizeBilinearGradTest.test120x120To545x545): 1.52s
I0119 18:13:42.404454 140566812481280 test_util.py:2071] time(__main__.ResizeBilinearGradTest.test120x120To545x545): 1.52s
INFO:tensorflow:End test case: test120x120To545x545
I0119 18:13:42.405581 140566812481280 xla_test.py:206] End test case: test120x120To545x545
[       OK ] ResizeBilinearGradTest.test120x120To545x545
[ RUN      ] ResizeBilinearGradTest.test3x3To9x9
INFO:tensorflow:Start test case: ResizeBilinearGradTest.test3x3To9x9
I0119 18:13:42.407713 140566812481280 xla_test.py:199] Start test case: ResizeBilinearGradTest.test3x3To9x9
2021-01-19 18:13:42.409829: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:42.410548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:13:42.410952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:42.411777: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:42.412427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:13:42.412576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-19 18:13:42.412733: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2021-01-19 18:13:42.412753: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2021-01-19 18:13:42.413175: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:42.413893: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:42.414514: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15405 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
INFO:tensorflow:time(__main__.ResizeBilinearGradTest.test3x3To9x9): 0.17s
I0119 18:13:42.574819 140566812481280 test_util.py:2071] time(__main__.ResizeBilinearGradTest.test3x3To9x9): 0.17s
INFO:tensorflow:End test case: test3x3To9x9
I0119 18:13:42.575961 140566812481280 xla_test.py:206] End test case: test3x3To9x9
[       OK ] ResizeBilinearGradTest.test3x3To9x9
[ RUN      ] ResizeBilinearGradTest.test8x8To16x16
INFO:tensorflow:Start test case: ResizeBilinearGradTest.test8x8To16x16
I0119 18:13:42.577876 140566812481280 xla_test.py:199] Start test case: ResizeBilinearGradTest.test8x8To16x16
2021-01-19 18:13:42.579884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:42.580591: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:13:42.580736: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:42.581376: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:42.581918: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:13:42.581984: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-19 18:13:42.581997: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2021-01-19 18:13:42.582005: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2021-01-19 18:13:42.582263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:42.583009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:42.583472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15405 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
INFO:tensorflow:time(__main__.ResizeBilinearGradTest.test8x8To16x16): 0.17s
I0119 18:13:42.749102 140566812481280 test_util.py:2071] time(__main__.ResizeBilinearGradTest.test8x8To16x16): 0.17s
INFO:tensorflow:End test case: test8x8To16x16
I0119 18:13:42.749682 140566812481280 xla_test.py:206] End test case: test8x8To16x16
[       OK ] ResizeBilinearGradTest.test8x8To16x16
[ RUN      ] ResizeBilinearTest.test103x103To545x545
INFO:tensorflow:Start test case: ResizeBilinearTest.test103x103To545x545
I0119 18:13:42.751456 140566812481280 xla_test.py:199] Start test case: ResizeBilinearTest.test103x103To545x545
2021-01-19 18:13:42.829047: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:42.829670: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:13:42.829831: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:42.830457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:42.830914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:13:42.830979: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-19 18:13:42.830991: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2021-01-19 18:13:42.831005: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2021-01-19 18:13:42.831240: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:42.831881: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:42.832348: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15405 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
INFO:tensorflow:time(__main__.ResizeBilinearTest.test103x103To545x545): 0.25s
I0119 18:13:42.998289 140566812481280 test_util.py:2071] time(__main__.ResizeBilinearTest.test103x103To545x545): 0.25s
INFO:tensorflow:End test case: test103x103To545x545
I0119 18:13:42.998827 140566812481280 xla_test.py:206] End test case: test103x103To545x545
[       OK ] ResizeBilinearTest.test103x103To545x545
[ RUN      ] ResizeBilinearTest.test256x48To2048x384
INFO:tensorflow:Start test case: ResizeBilinearTest.test256x48To2048x384
I0119 18:13:42.999973 140566812481280 xla_test.py:199] Start test case: ResizeBilinearTest.test256x48To2048x384
2021-01-19 18:13:43.214953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:43.215793: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:13:43.216138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:43.216840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:43.217413: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:13:43.217654: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-19 18:13:43.217679: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2021-01-19 18:13:43.217690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2021-01-19 18:13:43.218116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:43.218752: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:43.219254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15405 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
2021-01-19 18:13:43.627054: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 302.88M (317587456 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-01-19 18:13:43.662089: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 547.00M (573571072 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
INFO:tensorflow:time(__main__.ResizeBilinearTest.test256x48To2048x384): 0.83s
I0119 18:13:43.828396 140566812481280 test_util.py:2071] time(__main__.ResizeBilinearTest.test256x48To2048x384): 0.83s
INFO:tensorflow:End test case: test256x48To2048x384
I0119 18:13:43.829105 140566812481280 xla_test.py:206] End test case: test256x48To2048x384
[       OK ] ResizeBilinearTest.test256x48To2048x384
[ RUN      ] ResizeBilinearTest.test512x512To299x299
INFO:tensorflow:Start test case: ResizeBilinearTest.test512x512To299x299
I0119 18:13:43.831063 140566812481280 xla_test.py:199] Start test case: ResizeBilinearTest.test512x512To299x299
2021-01-19 18:13:43.927853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:43.928681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:13:43.928871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:43.929545: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:43.930167: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:13:43.930235: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-19 18:13:43.930251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2021-01-19 18:13:43.930261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2021-01-19 18:13:43.930433: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:43.930991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:43.931534: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15405 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
2021-01-19 18:13:43.944424: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 314.58M (329861120 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-01-19 18:13:43.944480: W tensorflow/compiler/xla/service/gpu/gpu_conv_algorithm_picker.cc:773] Failed to determine best cudnn convolution algorithm: Resource exhausted: Failed to allocate request for 314.58MiB (329861120B) on device ordinal 0

Convolution performance may be suboptimal.
2021-01-19 18:13:43.985607: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 218.92M (229551596 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-01-19 18:13:44.127985: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 299.16M (313696384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-01-19 18:13:44.128138: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at xla_compile_on_demand_op.cc:179 : Resource exhausted: Failed to allocate request for 299.16MiB (313696384B) on device ordinal 0
INFO:tensorflow:time(__main__.ResizeBilinearTest.test512x512To299x299): 0.41s
I0119 18:13:44.241236 140566812481280 test_util.py:2071] time(__main__.ResizeBilinearTest.test512x512To299x299): 0.41s
INFO:tensorflow:End test case: test512x512To299x299
I0119 18:13:44.242130 140566812481280 xla_test.py:206] End test case: test512x512To299x299
[  FAILED  ] ResizeBilinearTest.test512x512To299x299
[ RUN      ] ResizeBilinearTest.test8x8To16x16
INFO:tensorflow:Start test case: ResizeBilinearTest.test8x8To16x16
I0119 18:13:44.248525 140566812481280 xla_test.py:199] Start test case: ResizeBilinearTest.test8x8To16x16
2021-01-19 18:13:44.250519: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:44.251176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:13:44.251531: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:44.252176: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:44.252817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:13:44.253034: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-19 18:13:44.253077: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2021-01-19 18:13:44.253160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2021-01-19 18:13:44.253508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:44.254207: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:44.254862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15405 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
INFO:tensorflow:time(__main__.ResizeBilinearTest.test8x8To16x16): 0.18s
I0119 18:13:44.424480 140566812481280 test_util.py:2071] time(__main__.ResizeBilinearTest.test8x8To16x16): 0.18s
INFO:tensorflow:End test case: test8x8To16x16
I0119 18:13:44.425702 140566812481280 xla_test.py:206] End test case: test8x8To16x16
[       OK ] ResizeBilinearTest.test8x8To16x16
[ RUN      ] ResizeNearestNeighborTest.testAlignCorners3x3To12x12_uint8
INFO:tensorflow:Start test case: ResizeNearestNeighborTest.testAlignCorners3x3To12x12_uint8
I0119 18:13:44.427649 140566812481280 xla_test.py:199] Start test case: ResizeNearestNeighborTest.testAlignCorners3x3To12x12_uint8
2021-01-19 18:13:44.429622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:44.430326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:13:44.430676: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:44.431336: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:44.432268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:13:44.432547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-19 18:13:44.432581: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2021-01-19 18:13:44.432653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2021-01-19 18:13:44.433015: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:44.433769: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:44.434446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15405 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
INFO:tensorflow:time(__main__.ResizeNearestNeighborTest.testAlignCorners3x3To12x12_uint8): 0.17s
I0119 18:13:44.593635 140566812481280 test_util.py:2071] time(__main__.ResizeNearestNeighborTest.testAlignCorners3x3To12x12_uint8): 0.17s
INFO:tensorflow:End test case: testAlignCorners3x3To12x12_uint8
I0119 18:13:44.594714 140566812481280 xla_test.py:206] End test case: testAlignCorners3x3To12x12_uint8
[       OK ] ResizeNearestNeighborTest.testAlignCorners3x3To12x12_uint8
======================================================================
ERROR: test512x512To299x299 (__main__.ResizeBilinearTest)
test512x512To299x299 (__main__.ResizeBilinearTest)
test512x512To299x299(512, 512, 299, 299)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/client/session.py", line 1375, in _do_call
    return fn(*args)
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/client/session.py", line 1360, in _run_fn
    target_list, run_metadata)
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/client/session.py", line 1453, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.
  (0) Resource exhausted: Failed to allocate request for 299.16MiB (313696384B) on device ordinal 0
	 [[{{node ResizeBilinear}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

	 [[ResizeBilinear/_5]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

  (1) Resource exhausted: Failed to allocate request for 299.16MiB (313696384B) on device ordinal 0
	 [[{{node ResizeBilinear}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

0 successful operations.
0 derived errors ignored.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/absl_py/absl/testing/parameterized.py", line 265, in bound_param_test
    test_method(self, *testcase_params)
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/compiler/tests/image_ops_test.py", line 626, in test
    large_tolerance=True)
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/compiler/tests/image_ops_test.py", line 564, in _assertForwardOpMatchesExpected
    out = sess.run(resized, {image: image_np[np.newaxis, :, :, np.newaxis]})
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/client/session.py", line 968, in run
    run_metadata_ptr)
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/client/session.py", line 1191, in _run
    feed_dict_tensor, options, run_metadata)
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/client/session.py", line 1369, in _do_run
    run_metadata)
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/client/session.py", line 1394, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.
  (0) Resource exhausted: Failed to allocate request for 299.16MiB (313696384B) on device ordinal 0
	 [[node ResizeBilinear (defined at b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/compiler/tests/image_ops_test.py:563) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

	 [[ResizeBilinear/_5]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

  (1) Resource exhausted: Failed to allocate request for 299.16MiB (313696384B) on device ordinal 0
	 [[node ResizeBilinear (defined at b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/compiler/tests/image_ops_test.py:563) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

0 successful operations.
0 derived errors ignored.

Errors may have originated from an input operation.
Input Source operations connected to node ResizeBilinear:
 Placeholder (defined at b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/compiler/tests/image_ops_test.py:561)

Input Source operations connected to node ResizeBilinear:
 Placeholder (defined at b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/compiler/tests/image_ops_test.py:561)

Original stack trace for 'ResizeBilinear':
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/compiler/tests/image_ops_test.py", line 1294, in <module>
    test.main()
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/platform/test.py", line 58, in main
    return _googletest.main(argv)
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py", line 66, in main
    benchmark.benchmarks_main(true_main=main_wrapper)
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/platform/benchmark.py", line 518, in benchmarks_main
    true_main()
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py", line 65, in main_wrapper
    return app.run(main=g_main, argv=args)
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/platform/app.py", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/absl_py/absl/app.py", line 299, in run
    _run_main(main, args)
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/absl_py/absl/app.py", line 250, in _run_main
    sys.exit(main(argv))
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py", line 56, in g_main
    absltest_main(argv=argv)
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/absl_py/absl/testing/absltest.py", line 1948, in main
    _run_in_app(run_tests, args, kwargs)
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/absl_py/absl/testing/absltest.py", line 2055, in _run_in_app
    function(argv, args, kwargs)
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/absl_py/absl/testing/absltest.py", line 2334, in run_tests
    argv, args, kwargs, xml_reporter.TextAndXMLTestRunner)
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/absl_py/absl/testing/absltest.py", line 2304, in _run_and_get_tests_result
    test_program = unittest.TestProgram(*args, **kwargs)
  File "usr/lib/python3.6/unittest/main.py", line 95, in __init__
    self.runTests()
  File "usr/lib/python3.6/unittest/main.py", line 256, in runTests
    self.result = testRunner.run(self.test)
  File "usr/lib/python3.6/unittest/runner.py", line 176, in run
    test(result)
  File "usr/lib/python3.6/unittest/suite.py", line 84, in __call__
    return self.run(*args, **kwds)
  File "usr/lib/python3.6/unittest/suite.py", line 122, in run
    test(result)
  File "usr/lib/python3.6/unittest/suite.py", line 84, in __call__
    return self.run(*args, **kwds)
  File "usr/lib/python3.6/unittest/suite.py", line 122, in run
    test(result)
  File "usr/lib/python3.6/unittest/case.py", line 653, in __call__
    return self.run(*args, **kwds)
  File "usr/lib/python3.6/unittest/case.py", line 605, in run
    testMethod()
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/absl_py/absl/testing/parameterized.py", line 265, in bound_param_test
    test_method(self, *testcase_params)
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/compiler/tests/image_ops_test.py", line 626, in test
    large_tolerance=True)
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/compiler/tests/image_ops_test.py", line 563, in _assertForwardOpMatchesExpected
    image, target_shape, align_corners=align_corners)
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/ops/gen_image_ops.py", line 3686, in resize_bilinear
    half_pixel_centers=half_pixel_centers, name=name)
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/op_def_library.py", line 750, in _apply_op_helper
    attrs=attr_protos, op_def=op_def)
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/ops.py", line 3543, in _create_op_internal
    op_def=op_def)
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/ops.py", line 2023, in __init__
    self._traceback = tf_stack.extract_stack_for_node(self._c_op)


----------------------------------------------------------------------
Ran 12 tests in 9.880s

FAILED (errors=1, skipped=1)
================================================================================
==================== Test output for //tensorflow/compiler/tests:image_ops_test_gpu (shard 5 of 10):
2021-01-19 18:13:48.397637: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
Running tests under Python 3.6.10: /usr/bin/python3
WARNING:tensorflow:From /b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/compiler/tests/xla_test.py:87: Context.enable_xla_devices (from tensorflow.python.eager.context) is deprecated and will be removed in a future version.
Instructions for updating:
XLA:CPU and XLA:GPU devices are deprecated
W0119 18:13:50.270172 139960831297280 deprecation.py:336] From /b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/compiler/tests/xla_test.py:87: Context.enable_xla_devices (from tensorflow.python.eager.context) is deprecated and will be removed in a future version.
Instructions for updating:
XLA:CPU and XLA:GPU devices are deprecated
[ RUN      ] AdjustContrastTest.test_session
[  SKIPPED ] AdjustContrastTest.test_session
[ RUN      ] AdjustSaturationTest.testTwiceSaturation
INFO:tensorflow:Start test case: AdjustSaturationTest.testTwiceSaturation
I0119 18:13:50.276568 139960831297280 xla_test.py:199] Start test case: AdjustSaturationTest.testTwiceSaturation
2021-01-19 18:13:50.295416: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-01-19 18:13:50.320066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:50.320793: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:13:50.320833: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
2021-01-19 18:13:50.323005: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10
2021-01-19 18:13:50.323070: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10
2021-01-19 18:13:50.325010: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-01-19 18:13:50.325459: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-01-19 18:13:50.327851: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-01-19 18:13:50.329116: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10
2021-01-19 18:13:50.333862: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7
2021-01-19 18:13:50.334006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:50.334698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:50.335329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:13:50.335848: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-01-19 18:13:50.348264: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2299995000 Hz
2021-01-19 18:13:50.348596: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x4ce3350 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-01-19 18:13:50.348745: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): Host, Default Version
2021-01-19 18:13:50.670050: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:50.671065: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x4ce5de0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2021-01-19 18:13:50.671092: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0
2021-01-19 18:13:50.671558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:50.672402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:13:50.672675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:50.673534: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:50.674286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:13:50.674367: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
2021-01-19 18:13:51.224096: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-19 18:13:51.224138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2021-01-19 18:13:51.224146: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2021-01-19 18:13:51.224502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:51.225293: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:51.225978: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15404 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
2021-01-19 18:13:51.233345: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:260] None of the MLIR optimization passes are enabled (registered 0 passes)
2021-01-19 18:13:51.561283: I tensorflow/compiler/jit/xla_device.cc:399] XLA_GPU and XLA_CPU devices are deprecated and will be removed in subsequent releases. Instead, use either @tf.function(jit_compile=True) for must-compile semantics, or run with TF_XLA_FLAGS=--tf_xla_auto_jit=2 for auto-clustering best-effort compilation.
2021-01-19 18:13:51.731787: I tensorflow/compiler/jit/xla_compilation_cache.cc:321] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
INFO:tensorflow:time(__main__.AdjustSaturationTest.testTwiceSaturation): 1.46s
I0119 18:13:51.736432 139960831297280 test_util.py:2071] time(__main__.AdjustSaturationTest.testTwiceSaturation): 1.46s
INFO:tensorflow:End test case: testTwiceSaturation
I0119 18:13:51.737203 139960831297280 xla_test.py:206] End test case: testTwiceSaturation
[       OK ] AdjustSaturationTest.testTwiceSaturation
[ RUN      ] BatchedNonMaxSuppressionCorrectnessTest.testBatchedNMSSingleFrom6Max3
INFO:tensorflow:Start test case: BatchedNonMaxSuppressionCorrectnessTest.testBatchedNMSSingleFrom6Max3
I0119 18:13:51.738461 139960831297280 xla_test.py:199] Start test case: BatchedNonMaxSuppressionCorrectnessTest.testBatchedNMSSingleFrom6Max3
2021-01-19 18:13:51.739688: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:51.740136: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:13:51.740351: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:51.740818: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:51.741209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:13:51.741263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-19 18:13:51.741275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2021-01-19 18:13:51.741282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2021-01-19 18:13:51.741475: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:51.742100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:51.742551: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15404 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
INFO:tensorflow:time(__main__.BatchedNonMaxSuppressionCorrectnessTest.testBatchedNMSSingleFrom6Max3): 3.16s
I0119 18:13:54.896216 139960831297280 test_util.py:2071] time(__main__.BatchedNonMaxSuppressionCorrectnessTest.testBatchedNMSSingleFrom6Max3): 3.16s
INFO:tensorflow:End test case: testBatchedNMSSingleFrom6Max3
I0119 18:13:54.897053 139960831297280 xla_test.py:206] End test case: testBatchedNMSSingleFrom6Max3
[       OK ] BatchedNonMaxSuppressionCorrectnessTest.testBatchedNMSSingleFrom6Max3
[ RUN      ] NonMaxSuppressionTest.test_scope
INFO:tensorflow:Start test case: NonMaxSuppressionTest.test_scope
I0119 18:13:54.898397 139960831297280 xla_test.py:199] Start test case: NonMaxSuppressionTest.test_scope
INFO:tensorflow:time(__main__.NonMaxSuppressionTest.test_scope): 0.0s
I0119 18:13:54.898656 139960831297280 test_util.py:2071] time(__main__.NonMaxSuppressionTest.test_scope): 0.0s
INFO:tensorflow:End test case: test_scope
I0119 18:13:54.898807 139960831297280 xla_test.py:206] End test case: test_scope
[       OK ] NonMaxSuppressionTest.test_scope
[ RUN      ] ResizeBilinearGradTest.test120x120To545x545
INFO:tensorflow:Start test case: ResizeBilinearGradTest.test120x120To545x545
I0119 18:13:54.899515 139960831297280 xla_test.py:199] Start test case: ResizeBilinearGradTest.test120x120To545x545
2021-01-19 18:13:54.943517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:54.943987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:13:54.944131: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:54.944691: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:54.945109: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:13:54.945160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-19 18:13:54.945168: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2021-01-19 18:13:54.945174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2021-01-19 18:13:54.945381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:54.945808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:13:54.946177: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15404 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
2021-01-19 18:13:54.958022: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10
2021-01-19 18:13:55.154250: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7
2021-01-19 18:14:00.945374: I tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Loaded cuDNN version 7605
INFO:tensorflow:time(__main__.ResizeBilinearGradTest.test120x120To545x545): 6.36s
I0119 18:14:01.260859 139960831297280 test_util.py:2071] time(__main__.ResizeBilinearGradTest.test120x120To545x545): 6.36s
INFO:tensorflow:End test case: test120x120To545x545
I0119 18:14:01.261702 139960831297280 xla_test.py:206] End test case: test120x120To545x545
[       OK ] ResizeBilinearGradTest.test120x120To545x545
[ RUN      ] ResizeBilinearGradTest.test3x3To9x9
INFO:tensorflow:Start test case: ResizeBilinearGradTest.test3x3To9x9
I0119 18:14:01.262717 139960831297280 xla_test.py:199] Start test case: ResizeBilinearGradTest.test3x3To9x9
2021-01-19 18:14:01.264006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:01.264522: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:14:01.264672: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:01.265201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:01.265592: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:14:01.265645: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-19 18:14:01.265653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2021-01-19 18:14:01.265660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2021-01-19 18:14:01.265851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:01.266327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:01.266728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15404 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
INFO:tensorflow:time(__main__.ResizeBilinearGradTest.test3x3To9x9): 0.14s
I0119 18:14:01.399818 139960831297280 test_util.py:2071] time(__main__.ResizeBilinearGradTest.test3x3To9x9): 0.14s
INFO:tensorflow:End test case: test3x3To9x9
I0119 18:14:01.400325 139960831297280 xla_test.py:206] End test case: test3x3To9x9
[       OK ] ResizeBilinearGradTest.test3x3To9x9
[ RUN      ] ResizeBilinearGradTest.test8x8To16x16
INFO:tensorflow:Start test case: ResizeBilinearGradTest.test8x8To16x16
I0119 18:14:01.401279 139960831297280 xla_test.py:199] Start test case: ResizeBilinearGradTest.test8x8To16x16
2021-01-19 18:14:01.402619: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:01.403055: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:14:01.403197: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:01.403650: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:01.404019: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:14:01.404071: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-19 18:14:01.404080: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2021-01-19 18:14:01.404090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2021-01-19 18:14:01.404248: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:01.404701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:01.405087: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15404 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
INFO:tensorflow:time(__main__.ResizeBilinearGradTest.test8x8To16x16): 0.13s
I0119 18:14:01.534488 139960831297280 test_util.py:2071] time(__main__.ResizeBilinearGradTest.test8x8To16x16): 0.13s
INFO:tensorflow:End test case: test8x8To16x16
I0119 18:14:01.534941 139960831297280 xla_test.py:206] End test case: test8x8To16x16
[       OK ] ResizeBilinearGradTest.test8x8To16x16
[ RUN      ] ResizeBilinearTest.test103x103To545x545
INFO:tensorflow:Start test case: ResizeBilinearTest.test103x103To545x545
I0119 18:14:01.535913 139960831297280 xla_test.py:199] Start test case: ResizeBilinearTest.test103x103To545x545
2021-01-19 18:14:01.604058: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:01.604539: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:14:01.604690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:01.605139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:01.605496: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:14:01.605548: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-19 18:14:01.605556: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2021-01-19 18:14:01.605563: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2021-01-19 18:14:01.605696: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:01.606195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:01.606592: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15404 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
INFO:tensorflow:time(__main__.ResizeBilinearTest.test103x103To545x545): 0.21s
I0119 18:14:01.741054 139960831297280 test_util.py:2071] time(__main__.ResizeBilinearTest.test103x103To545x545): 0.21s
INFO:tensorflow:End test case: test103x103To545x545
I0119 18:14:01.741876 139960831297280 xla_test.py:206] End test case: test103x103To545x545
[       OK ] ResizeBilinearTest.test103x103To545x545
[ RUN      ] ResizeBilinearTest.test256x48To2048x384
INFO:tensorflow:Start test case: ResizeBilinearTest.test256x48To2048x384
I0119 18:14:01.743346 139960831297280 xla_test.py:199] Start test case: ResizeBilinearTest.test256x48To2048x384
2021-01-19 18:14:01.927409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:01.927953: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:14:01.928163: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:01.928698: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:01.929119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:14:01.929175: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-19 18:14:01.929184: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2021-01-19 18:14:01.929190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2021-01-19 18:14:01.929329: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:01.929739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:01.930116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15404 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
2021-01-19 18:14:02.289847: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 302.88M (317587456 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-01-19 18:14:02.329133: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 547.00M (573571072 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
INFO:tensorflow:time(__main__.ResizeBilinearTest.test256x48To2048x384): 0.72s
I0119 18:14:02.462907 139960831297280 test_util.py:2071] time(__main__.ResizeBilinearTest.test256x48To2048x384): 0.72s
INFO:tensorflow:End test case: test256x48To2048x384
I0119 18:14:02.463408 139960831297280 xla_test.py:206] End test case: test256x48To2048x384
[       OK ] ResizeBilinearTest.test256x48To2048x384
[ RUN      ] ResizeBilinearTest.test512x512To299x299
INFO:tensorflow:Start test case: ResizeBilinearTest.test512x512To299x299
I0119 18:14:02.464354 139960831297280 xla_test.py:199] Start test case: ResizeBilinearTest.test512x512To299x299
2021-01-19 18:14:02.545317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:02.545763: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:14:02.545922: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:02.546359: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:02.546711: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:14:02.546769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-19 18:14:02.546780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2021-01-19 18:14:02.546787: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2021-01-19 18:14:02.546925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:02.547354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:02.547720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15404 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
2021-01-19 18:14:02.557142: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 314.58M (329861120 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-01-19 18:14:02.557187: W tensorflow/compiler/xla/service/gpu/gpu_conv_algorithm_picker.cc:773] Failed to determine best cudnn convolution algorithm: Resource exhausted: Failed to allocate request for 314.58MiB (329861120B) on device ordinal 0

Convolution performance may be suboptimal.
2021-01-19 18:14:02.615813: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 218.92M (229551596 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-01-19 18:14:02.730104: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 299.16M (313696384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-01-19 18:14:02.730243: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at xla_compile_on_demand_op.cc:179 : Resource exhausted: Failed to allocate request for 299.16MiB (313696384B) on device ordinal 0
INFO:tensorflow:time(__main__.ResizeBilinearTest.test512x512To299x299): 0.36s
I0119 18:14:02.828087 139960831297280 test_util.py:2071] time(__main__.ResizeBilinearTest.test512x512To299x299): 0.36s
INFO:tensorflow:End test case: test512x512To299x299
I0119 18:14:02.828824 139960831297280 xla_test.py:206] End test case: test512x512To299x299
[  FAILED  ] ResizeBilinearTest.test512x512To299x299
[ RUN      ] ResizeBilinearTest.test8x8To16x16
INFO:tensorflow:Start test case: ResizeBilinearTest.test8x8To16x16
I0119 18:14:02.832797 139960831297280 xla_test.py:199] Start test case: ResizeBilinearTest.test8x8To16x16
2021-01-19 18:14:02.834211: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:02.834709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:14:02.834942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:02.835457: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:02.835896: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:14:02.835976: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-19 18:14:02.835988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2021-01-19 18:14:02.835995: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2021-01-19 18:14:02.836245: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:02.836781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:02.837251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15404 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
INFO:tensorflow:time(__main__.ResizeBilinearTest.test8x8To16x16): 0.14s
I0119 18:14:02.969898 139960831297280 test_util.py:2071] time(__main__.ResizeBilinearTest.test8x8To16x16): 0.14s
INFO:tensorflow:End test case: test8x8To16x16
I0119 18:14:02.970515 139960831297280 xla_test.py:206] End test case: test8x8To16x16
[       OK ] ResizeBilinearTest.test8x8To16x16
[ RUN      ] ResizeNearestNeighborTest.testAlignCorners3x3To12x12_uint8
INFO:tensorflow:Start test case: ResizeNearestNeighborTest.testAlignCorners3x3To12x12_uint8
I0119 18:14:02.971876 139960831297280 xla_test.py:199] Start test case: ResizeNearestNeighborTest.testAlignCorners3x3To12x12_uint8
2021-01-19 18:14:02.973181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:02.973713: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:14:02.973907: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:02.974423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:02.974848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:14:02.974926: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-19 18:14:02.974937: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2021-01-19 18:14:02.974944: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2021-01-19 18:14:02.975171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:02.975692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:02.976151: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15404 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
INFO:tensorflow:time(__main__.ResizeNearestNeighborTest.testAlignCorners3x3To12x12_uint8): 0.13s
I0119 18:14:03.105875 139960831297280 test_util.py:2071] time(__main__.ResizeNearestNeighborTest.testAlignCorners3x3To12x12_uint8): 0.13s
INFO:tensorflow:End test case: testAlignCorners3x3To12x12_uint8
I0119 18:14:03.106484 139960831297280 xla_test.py:206] End test case: testAlignCorners3x3To12x12_uint8
[       OK ] ResizeNearestNeighborTest.testAlignCorners3x3To12x12_uint8
======================================================================
ERROR: test512x512To299x299 (__main__.ResizeBilinearTest)
test512x512To299x299 (__main__.ResizeBilinearTest)
test512x512To299x299(512, 512, 299, 299)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/client/session.py", line 1375, in _do_call
    return fn(*args)
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/client/session.py", line 1360, in _run_fn
    target_list, run_metadata)
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/client/session.py", line 1453, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.
  (0) Resource exhausted: Failed to allocate request for 299.16MiB (313696384B) on device ordinal 0
	 [[{{node ResizeBilinear}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

  (1) Resource exhausted: Failed to allocate request for 299.16MiB (313696384B) on device ordinal 0
	 [[{{node ResizeBilinear}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

	 [[ResizeBilinear/_5]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

0 successful operations.
0 derived errors ignored.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/absl_py/absl/testing/parameterized.py", line 265, in bound_param_test
    test_method(self, *testcase_params)
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/compiler/tests/image_ops_test.py", line 626, in test
    large_tolerance=True)
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/compiler/tests/image_ops_test.py", line 564, in _assertForwardOpMatchesExpected
    out = sess.run(resized, {image: image_np[np.newaxis, :, :, np.newaxis]})
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/client/session.py", line 968, in run
    run_metadata_ptr)
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/client/session.py", line 1191, in _run
    feed_dict_tensor, options, run_metadata)
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/client/session.py", line 1369, in _do_run
    run_metadata)
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/client/session.py", line 1394, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.
  (0) Resource exhausted: Failed to allocate request for 299.16MiB (313696384B) on device ordinal 0
	 [[node ResizeBilinear (defined at b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/compiler/tests/image_ops_test.py:563) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

  (1) Resource exhausted: Failed to allocate request for 299.16MiB (313696384B) on device ordinal 0
	 [[node ResizeBilinear (defined at b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/compiler/tests/image_ops_test.py:563) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

	 [[ResizeBilinear/_5]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

0 successful operations.
0 derived errors ignored.

Errors may have originated from an input operation.
Input Source operations connected to node ResizeBilinear:
 Placeholder (defined at b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/compiler/tests/image_ops_test.py:561)

Input Source operations connected to node ResizeBilinear:
 Placeholder (defined at b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/compiler/tests/image_ops_test.py:561)

Original stack trace for 'ResizeBilinear':
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/compiler/tests/image_ops_test.py", line 1294, in <module>
    test.main()
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/platform/test.py", line 58, in main
    return _googletest.main(argv)
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py", line 66, in main
    benchmark.benchmarks_main(true_main=main_wrapper)
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/platform/benchmark.py", line 518, in benchmarks_main
    true_main()
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py", line 65, in main_wrapper
    return app.run(main=g_main, argv=args)
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/platform/app.py", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/absl_py/absl/app.py", line 299, in run
    _run_main(main, args)
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/absl_py/absl/app.py", line 250, in _run_main
    sys.exit(main(argv))
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py", line 56, in g_main
    absltest_main(argv=argv)
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/absl_py/absl/testing/absltest.py", line 1948, in main
    _run_in_app(run_tests, args, kwargs)
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/absl_py/absl/testing/absltest.py", line 2055, in _run_in_app
    function(argv, args, kwargs)
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/absl_py/absl/testing/absltest.py", line 2334, in run_tests
    argv, args, kwargs, xml_reporter.TextAndXMLTestRunner)
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/absl_py/absl/testing/absltest.py", line 2304, in _run_and_get_tests_result
    test_program = unittest.TestProgram(*args, **kwargs)
  File "usr/lib/python3.6/unittest/main.py", line 95, in __init__
    self.runTests()
  File "usr/lib/python3.6/unittest/main.py", line 256, in runTests
    self.result = testRunner.run(self.test)
  File "usr/lib/python3.6/unittest/runner.py", line 176, in run
    test(result)
  File "usr/lib/python3.6/unittest/suite.py", line 84, in __call__
    return self.run(*args, **kwds)
  File "usr/lib/python3.6/unittest/suite.py", line 122, in run
    test(result)
  File "usr/lib/python3.6/unittest/suite.py", line 84, in __call__
    return self.run(*args, **kwds)
  File "usr/lib/python3.6/unittest/suite.py", line 122, in run
    test(result)
  File "usr/lib/python3.6/unittest/case.py", line 653, in __call__
    return self.run(*args, **kwds)
  File "usr/lib/python3.6/unittest/case.py", line 605, in run
    testMethod()
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/absl_py/absl/testing/parameterized.py", line 265, in bound_param_test
    test_method(self, *testcase_params)
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/compiler/tests/image_ops_test.py", line 626, in test
    large_tolerance=True)
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/compiler/tests/image_ops_test.py", line 563, in _assertForwardOpMatchesExpected
    image, target_shape, align_corners=align_corners)
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/ops/gen_image_ops.py", line 3686, in resize_bilinear
    half_pixel_centers=half_pixel_centers, name=name)
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/op_def_library.py", line 750, in _apply_op_helper
    attrs=attr_protos, op_def=op_def)
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/ops.py", line 3543, in _create_op_internal
    op_def=op_def)
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/ops.py", line 2023, in __init__
    self._traceback = tf_stack.extract_stack_for_node(self._c_op)


----------------------------------------------------------------------
Ran 12 tests in 12.832s

FAILED (errors=1, skipped=1)
================================================================================
==================== Test output for //tensorflow/compiler/tests:image_ops_test_gpu (shard 5 of 10):
2021-01-19 18:14:07.047141: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
Running tests under Python 3.6.10: /usr/bin/python3
WARNING:tensorflow:From /b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/compiler/tests/xla_test.py:87: Context.enable_xla_devices (from tensorflow.python.eager.context) is deprecated and will be removed in a future version.
Instructions for updating:
XLA:CPU and XLA:GPU devices are deprecated
W0119 18:14:09.390006 140078381700864 deprecation.py:336] From /b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/compiler/tests/xla_test.py:87: Context.enable_xla_devices (from tensorflow.python.eager.context) is deprecated and will be removed in a future version.
Instructions for updating:
XLA:CPU and XLA:GPU devices are deprecated
[ RUN      ] AdjustContrastTest.test_session
[  SKIPPED ] AdjustContrastTest.test_session
[ RUN      ] AdjustSaturationTest.testTwiceSaturation
INFO:tensorflow:Start test case: AdjustSaturationTest.testTwiceSaturation
I0119 18:14:09.398774 140078381700864 xla_test.py:199] Start test case: AdjustSaturationTest.testTwiceSaturation
2021-01-19 18:14:09.420005: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-01-19 18:14:09.439105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:09.440045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:14:09.440088: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
2021-01-19 18:14:09.442500: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10
2021-01-19 18:14:09.442562: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10
2021-01-19 18:14:09.444757: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-01-19 18:14:09.445307: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-01-19 18:14:09.447898: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-01-19 18:14:09.449251: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10
2021-01-19 18:14:09.454814: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7
2021-01-19 18:14:09.454953: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:09.456035: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:09.456951: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:14:09.457604: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-01-19 18:14:09.471711: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2300000000 Hz
2021-01-19 18:14:09.472065: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x3cd9330 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-01-19 18:14:09.472138: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): Host, Default Version
2021-01-19 18:14:09.799020: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:09.800160: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x3cdbdc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2021-01-19 18:14:09.800201: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0
2021-01-19 18:14:09.800744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:09.801897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:14:09.802209: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:09.803581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:09.804609: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:14:09.804690: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1
2021-01-19 18:14:10.416767: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-19 18:14:10.416816: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2021-01-19 18:14:10.416828: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2021-01-19 18:14:10.417280: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:10.418446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:10.419454: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15405 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
2021-01-19 18:14:10.428940: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:260] None of the MLIR optimization passes are enabled (registered 0 passes)
2021-01-19 18:14:10.814566: I tensorflow/compiler/jit/xla_device.cc:399] XLA_GPU and XLA_CPU devices are deprecated and will be removed in subsequent releases. Instead, use either @tf.function(jit_compile=True) for must-compile semantics, or run with TF_XLA_FLAGS=--tf_xla_auto_jit=2 for auto-clustering best-effort compilation.
2021-01-19 18:14:11.314246: I tensorflow/compiler/jit/xla_compilation_cache.cc:321] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
INFO:tensorflow:time(__main__.AdjustSaturationTest.testTwiceSaturation): 1.94s
I0119 18:14:11.342433 140078381700864 test_util.py:2071] time(__main__.AdjustSaturationTest.testTwiceSaturation): 1.94s
INFO:tensorflow:End test case: testTwiceSaturation
I0119 18:14:11.342989 140078381700864 xla_test.py:206] End test case: testTwiceSaturation
[       OK ] AdjustSaturationTest.testTwiceSaturation
[ RUN      ] BatchedNonMaxSuppressionCorrectnessTest.testBatchedNMSSingleFrom6Max3
INFO:tensorflow:Start test case: BatchedNonMaxSuppressionCorrectnessTest.testBatchedNMSSingleFrom6Max3
I0119 18:14:11.344156 140078381700864 xla_test.py:199] Start test case: BatchedNonMaxSuppressionCorrectnessTest.testBatchedNMSSingleFrom6Max3
2021-01-19 18:14:11.345795: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:11.346296: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:14:11.346444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:11.347116: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:11.347569: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:14:11.347626: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-19 18:14:11.347639: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2021-01-19 18:14:11.347646: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2021-01-19 18:14:11.347824: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:11.348471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:11.348938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15405 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
INFO:tensorflow:time(__main__.BatchedNonMaxSuppressionCorrectnessTest.testBatchedNMSSingleFrom6Max3): 4.11s
I0119 18:14:15.451184 140078381700864 test_util.py:2071] time(__main__.BatchedNonMaxSuppressionCorrectnessTest.testBatchedNMSSingleFrom6Max3): 4.11s
INFO:tensorflow:End test case: testBatchedNMSSingleFrom6Max3
I0119 18:14:15.452269 140078381700864 xla_test.py:206] End test case: testBatchedNMSSingleFrom6Max3
[       OK ] BatchedNonMaxSuppressionCorrectnessTest.testBatchedNMSSingleFrom6Max3
[ RUN      ] NonMaxSuppressionTest.test_scope
INFO:tensorflow:Start test case: NonMaxSuppressionTest.test_scope
I0119 18:14:15.454183 140078381700864 xla_test.py:199] Start test case: NonMaxSuppressionTest.test_scope
INFO:tensorflow:time(__main__.NonMaxSuppressionTest.test_scope): 0.0s
I0119 18:14:15.454797 140078381700864 test_util.py:2071] time(__main__.NonMaxSuppressionTest.test_scope): 0.0s
INFO:tensorflow:End test case: test_scope
I0119 18:14:15.455069 140078381700864 xla_test.py:206] End test case: test_scope
[       OK ] NonMaxSuppressionTest.test_scope
[ RUN      ] ResizeBilinearGradTest.test120x120To545x545
INFO:tensorflow:Start test case: ResizeBilinearGradTest.test120x120To545x545
I0119 18:14:15.456243 140078381700864 xla_test.py:199] Start test case: ResizeBilinearGradTest.test120x120To545x545
2021-01-19 18:14:15.503313: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:15.503959: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:14:15.504255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:15.504865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:15.505538: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:14:15.505603: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-19 18:14:15.505616: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2021-01-19 18:14:15.505623: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2021-01-19 18:14:15.506065: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:15.506804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:15.507415: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15405 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
2021-01-19 18:14:15.522323: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10
2021-01-19 18:14:15.757953: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7
2021-01-19 18:14:19.917976: I tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Loaded cuDNN version 7605
INFO:tensorflow:time(__main__.ResizeBilinearGradTest.test120x120To545x545): 4.88s
I0119 18:14:20.331516 140078381700864 test_util.py:2071] time(__main__.ResizeBilinearGradTest.test120x120To545x545): 4.88s
INFO:tensorflow:End test case: test120x120To545x545
I0119 18:14:20.332129 140078381700864 xla_test.py:206] End test case: test120x120To545x545
[       OK ] ResizeBilinearGradTest.test120x120To545x545
[ RUN      ] ResizeBilinearGradTest.test3x3To9x9
INFO:tensorflow:Start test case: ResizeBilinearGradTest.test3x3To9x9
I0119 18:14:20.334195 140078381700864 xla_test.py:199] Start test case: ResizeBilinearGradTest.test3x3To9x9
2021-01-19 18:14:20.335834: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:20.336523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:14:20.336700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:20.337272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:20.337764: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:14:20.337837: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-19 18:14:20.337850: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2021-01-19 18:14:20.337857: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2021-01-19 18:14:20.338028: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:20.338618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:20.339400: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15405 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
INFO:tensorflow:time(__main__.ResizeBilinearGradTest.test3x3To9x9): 0.18s
I0119 18:14:20.512105 140078381700864 test_util.py:2071] time(__main__.ResizeBilinearGradTest.test3x3To9x9): 0.18s
INFO:tensorflow:End test case: test3x3To9x9
I0119 18:14:20.512623 140078381700864 xla_test.py:206] End test case: test3x3To9x9
[       OK ] ResizeBilinearGradTest.test3x3To9x9
[ RUN      ] ResizeBilinearGradTest.test8x8To16x16
INFO:tensorflow:Start test case: ResizeBilinearGradTest.test8x8To16x16
I0119 18:14:20.513937 140078381700864 xla_test.py:199] Start test case: ResizeBilinearGradTest.test8x8To16x16
2021-01-19 18:14:20.515569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:20.516152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:14:20.516330: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:20.516983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:20.517544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:14:20.517611: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-19 18:14:20.517623: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2021-01-19 18:14:20.517630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2021-01-19 18:14:20.517806: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:20.518401: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:20.519044: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15405 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
INFO:tensorflow:time(__main__.ResizeBilinearGradTest.test8x8To16x16): 0.16s
I0119 18:14:20.678463 140078381700864 test_util.py:2071] time(__main__.ResizeBilinearGradTest.test8x8To16x16): 0.16s
INFO:tensorflow:End test case: test8x8To16x16
I0119 18:14:20.679026 140078381700864 xla_test.py:206] End test case: test8x8To16x16
[       OK ] ResizeBilinearGradTest.test8x8To16x16
[ RUN      ] ResizeBilinearTest.test103x103To545x545
INFO:tensorflow:Start test case: ResizeBilinearTest.test103x103To545x545
I0119 18:14:20.680416 140078381700864 xla_test.py:199] Start test case: ResizeBilinearTest.test103x103To545x545
2021-01-19 18:14:20.760272: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:20.760805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:14:20.760942: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:20.761578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:20.762224: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:14:20.762290: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-19 18:14:20.762303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2021-01-19 18:14:20.762310: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2021-01-19 18:14:20.762493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:20.763114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:20.763646: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15405 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
INFO:tensorflow:time(__main__.ResizeBilinearTest.test103x103To545x545): 0.24s
I0119 18:14:20.923661 140078381700864 test_util.py:2071] time(__main__.ResizeBilinearTest.test103x103To545x545): 0.24s
INFO:tensorflow:End test case: test103x103To545x545
I0119 18:14:20.924841 140078381700864 xla_test.py:206] End test case: test103x103To545x545
[       OK ] ResizeBilinearTest.test103x103To545x545
[ RUN      ] ResizeBilinearTest.test256x48To2048x384
INFO:tensorflow:Start test case: ResizeBilinearTest.test256x48To2048x384
I0119 18:14:20.926580 140078381700864 xla_test.py:199] Start test case: ResizeBilinearTest.test256x48To2048x384
2021-01-19 18:14:21.140352: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:21.140955: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:14:21.141194: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:21.141862: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:21.142393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:14:21.142463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-19 18:14:21.142476: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2021-01-19 18:14:21.142484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2021-01-19 18:14:21.142886: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:21.143493: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:21.144034: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15405 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
2021-01-19 18:14:21.552833: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 302.88M (317587456 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-01-19 18:14:21.587596: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 547.00M (573571072 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
INFO:tensorflow:time(__main__.ResizeBilinearTest.test256x48To2048x384): 0.82s
I0119 18:14:21.751057 140078381700864 test_util.py:2071] time(__main__.ResizeBilinearTest.test256x48To2048x384): 0.82s
INFO:tensorflow:End test case: test256x48To2048x384
I0119 18:14:21.751821 140078381700864 xla_test.py:206] End test case: test256x48To2048x384
[       OK ] ResizeBilinearTest.test256x48To2048x384
[ RUN      ] ResizeBilinearTest.test512x512To299x299
INFO:tensorflow:Start test case: ResizeBilinearTest.test512x512To299x299
I0119 18:14:21.753551 140078381700864 xla_test.py:199] Start test case: ResizeBilinearTest.test512x512To299x299
2021-01-19 18:14:21.851029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:21.851656: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:14:21.851810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:21.852377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:21.852852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:14:21.852913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-19 18:14:21.852925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2021-01-19 18:14:21.852932: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2021-01-19 18:14:21.853104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:21.853638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:21.854206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15405 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
2021-01-19 18:14:21.864648: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 314.58M (329861120 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-01-19 18:14:21.864699: W tensorflow/compiler/xla/service/gpu/gpu_conv_algorithm_picker.cc:773] Failed to determine best cudnn convolution algorithm: Resource exhausted: Failed to allocate request for 314.58MiB (329861120B) on device ordinal 0

Convolution performance may be suboptimal.
2021-01-19 18:14:21.904915: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 218.92M (229551596 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-01-19 18:14:22.044717: I tensorflow/stream_executor/cuda/cuda_driver.cc:789] failed to allocate 299.16M (313696384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2021-01-19 18:14:22.044860: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at xla_compile_on_demand_op.cc:179 : Resource exhausted: Failed to allocate request for 299.16MiB (313696384B) on device ordinal 0
INFO:tensorflow:time(__main__.ResizeBilinearTest.test512x512To299x299): 0.39s
I0119 18:14:22.147017 140078381700864 test_util.py:2071] time(__main__.ResizeBilinearTest.test512x512To299x299): 0.39s
INFO:tensorflow:End test case: test512x512To299x299
I0119 18:14:22.147792 140078381700864 xla_test.py:206] End test case: test512x512To299x299
[  FAILED  ] ResizeBilinearTest.test512x512To299x299
[ RUN      ] ResizeBilinearTest.test8x8To16x16
INFO:tensorflow:Start test case: ResizeBilinearTest.test8x8To16x16
I0119 18:14:22.153005 140078381700864 xla_test.py:199] Start test case: ResizeBilinearTest.test8x8To16x16
2021-01-19 18:14:22.154805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:22.155372: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:14:22.155518: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:22.156094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:22.156634: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:14:22.156730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-19 18:14:22.156743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2021-01-19 18:14:22.156750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2021-01-19 18:14:22.156965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:22.157580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:22.158165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15405 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
INFO:tensorflow:time(__main__.ResizeBilinearTest.test8x8To16x16): 0.17s
I0119 18:14:22.318335 140078381700864 test_util.py:2071] time(__main__.ResizeBilinearTest.test8x8To16x16): 0.17s
INFO:tensorflow:End test case: test8x8To16x16
I0119 18:14:22.318934 140078381700864 xla_test.py:206] End test case: test8x8To16x16
[       OK ] ResizeBilinearTest.test8x8To16x16
[ RUN      ] ResizeNearestNeighborTest.testAlignCorners3x3To12x12_uint8
INFO:tensorflow:Start test case: ResizeNearestNeighborTest.testAlignCorners3x3To12x12_uint8
I0119 18:14:22.320241 140078381700864 xla_test.py:199] Start test case: ResizeNearestNeighborTest.testAlignCorners3x3To12x12_uint8
2021-01-19 18:14:22.321714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:22.322306: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1760] Found device 0 with properties: 
pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0
coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s
2021-01-19 18:14:22.322437: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:22.323071: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:22.323695: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1898] Adding visible gpu devices: 0
2021-01-19 18:14:22.323790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-01-19 18:14:22.323802: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1306]      0 
2021-01-19 18:14:22.323810: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1319] 0:   N 
2021-01-19 18:14:22.324042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:22.324848: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-01-19 18:14:22.325515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1446] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15405 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)
INFO:tensorflow:time(__main__.ResizeNearestNeighborTest.testAlignCorners3x3To12x12_uint8): 0.16s
I0119 18:14:22.480396 140078381700864 test_util.py:2071] time(__main__.ResizeNearestNeighborTest.testAlignCorners3x3To12x12_uint8): 0.16s
INFO:tensorflow:End test case: testAlignCorners3x3To12x12_uint8
I0119 18:14:22.480905 140078381700864 xla_test.py:206] End test case: testAlignCorners3x3To12x12_uint8
[       OK ] ResizeNearestNeighborTest.testAlignCorners3x3To12x12_uint8
======================================================================
ERROR: test512x512To299x299 (__main__.ResizeBilinearTest)
test512x512To299x299 (__main__.ResizeBilinearTest)
test512x512To299x299(512, 512, 299, 299)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/client/session.py", line 1375, in _do_call
    return fn(*args)
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/client/session.py", line 1360, in _run_fn
    target_list, run_metadata)
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/client/session.py", line 1453, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.
  (0) Resource exhausted: Failed to allocate request for 299.16MiB (313696384B) on device ordinal 0
	 [[{{node ResizeBilinear}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

  (1) Resource exhausted: Failed to allocate request for 299.16MiB (313696384B) on device ordinal 0
	 [[{{node ResizeBilinear}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

	 [[ResizeBilinear/_5]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

0 successful operations.
0 derived errors ignored.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/absl_py/absl/testing/parameterized.py", line 265, in bound_param_test
    test_method(self, *testcase_params)
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/compiler/tests/image_ops_test.py", line 626, in test
    large_tolerance=True)
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/compiler/tests/image_ops_test.py", line 564, in _assertForwardOpMatchesExpected
    out = sess.run(resized, {image: image_np[np.newaxis, :, :, np.newaxis]})
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/client/session.py", line 968, in run
    run_metadata_ptr)
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/client/session.py", line 1191, in _run
    feed_dict_tensor, options, run_metadata)
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/client/session.py", line 1369, in _do_run
    run_metadata)
  File "/b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/client/session.py", line 1394, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.
  (0) Resource exhausted: Failed to allocate request for 299.16MiB (313696384B) on device ordinal 0
	 [[node ResizeBilinear (defined at b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/compiler/tests/image_ops_test.py:563) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

  (1) Resource exhausted: Failed to allocate request for 299.16MiB (313696384B) on device ordinal 0
	 [[node ResizeBilinear (defined at b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/compiler/tests/image_ops_test.py:563) ]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

	 [[ResizeBilinear/_5]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

0 successful operations.
0 derived errors ignored.

Errors may have originated from an input operation.
Input Source operations connected to node ResizeBilinear:
 Placeholder (defined at b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/compiler/tests/image_ops_test.py:561)

Input Source operations connected to node ResizeBilinear:
 Placeholder (defined at b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/compiler/tests/image_ops_test.py:561)

Original stack trace for 'ResizeBilinear':
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/compiler/tests/image_ops_test.py", line 1294, in <module>
    test.main()
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/platform/test.py", line 58, in main
    return _googletest.main(argv)
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py", line 66, in main
    benchmark.benchmarks_main(true_main=main_wrapper)
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/platform/benchmark.py", line 518, in benchmarks_main
    true_main()
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py", line 65, in main_wrapper
    return app.run(main=g_main, argv=args)
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/platform/app.py", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/absl_py/absl/app.py", line 299, in run
    _run_main(main, args)
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/absl_py/absl/app.py", line 250, in _run_main
    sys.exit(main(argv))
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py", line 56, in g_main
    absltest_main(argv=argv)
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/absl_py/absl/testing/absltest.py", line 1948, in main
    _run_in_app(run_tests, args, kwargs)
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/absl_py/absl/testing/absltest.py", line 2055, in _run_in_app
    function(argv, args, kwargs)
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/absl_py/absl/testing/absltest.py", line 2334, in run_tests
    argv, args, kwargs, xml_reporter.TextAndXMLTestRunner)
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/absl_py/absl/testing/absltest.py", line 2304, in _run_and_get_tests_result
    test_program = unittest.TestProgram(*args, **kwargs)
  File "usr/lib/python3.6/unittest/main.py", line 95, in __init__
    self.runTests()
  File "usr/lib/python3.6/unittest/main.py", line 256, in runTests
    self.result = testRunner.run(self.test)
  File "usr/lib/python3.6/unittest/runner.py", line 176, in run
    test(result)
  File "usr/lib/python3.6/unittest/suite.py", line 84, in __call__
    return self.run(*args, **kwds)
  File "usr/lib/python3.6/unittest/suite.py", line 122, in run
    test(result)
  File "usr/lib/python3.6/unittest/suite.py", line 84, in __call__
    return self.run(*args, **kwds)
  File "usr/lib/python3.6/unittest/suite.py", line 122, in run
    test(result)
  File "usr/lib/python3.6/unittest/case.py", line 653, in __call__
    return self.run(*args, **kwds)
  File "usr/lib/python3.6/unittest/case.py", line 605, in run
    testMethod()
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/absl_py/absl/testing/parameterized.py", line 265, in bound_param_test
    test_method(self, *testcase_params)
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/compiler/tests/image_ops_test.py", line 626, in test
    large_tolerance=True)
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/compiler/tests/image_ops_test.py", line 563, in _assertForwardOpMatchesExpected
    image, target_shape, align_corners=align_corners)
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/ops/gen_image_ops.py", line 3686, in resize_bilinear
    half_pixel_centers=half_pixel_centers, name=name)
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/op_def_library.py", line 750, in _apply_op_helper
    attrs=attr_protos, op_def=op_def)
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/ops.py", line 3543, in _create_op_internal
    op_def=op_def)
  File "b/f/w/bazel-out/k8-opt/bin/tensorflow/compiler/tests/image_ops_test_gpu.runfiles/org_tensorflow/tensorflow/python/framework/ops.py", line 2023, in __init__
    self._traceback = tf_stack.extract_stack_for_node(self._c_op)


----------------------------------------------------------------------
Ran 12 tests in 13.084s

FAILED (errors=1, skipped=1)
================================================================================
[35,548 / 35,597] 861 / 877 tests, 2 failed; Testing //tensorflow/compiler/tests:matrix_diag_ops_test_gpu; 153s remote ... (49 actions running)
[35,594 / 35,597] 874 / 877 tests, 2 failed; Testing //tensorflow/compiler/tests:matrix_diag_ops_test_gpu_mlir_bridge_test; 159s remote ... (3 actions running)
INFO: Elapsed time: 763.131s, Critical Path: 417.04s
INFO: 35597 processes: 20219 remote cache hit, 11808 internal, 1 local, 3569 remote.
INFO: Build completed, 2 tests FAILED, 35597 total actions
//tensorflow/c:c_api_test_gpu                                            PASSED in 17.1s
//tensorflow/c:env_test_gpu                                              PASSED in 10.8s
//tensorflow/c:kernels_test_gpu                                          PASSED in 11.8s
//tensorflow/c/eager:c_api_cluster_test_gpu                              PASSED in 13.0s
//tensorflow/c/eager:c_api_distributed_test_gpu                          PASSED in 10.9s
//tensorflow/c/eager:c_api_experimental_test                             PASSED in 29.9s
//tensorflow/c/eager:c_api_experimental_test_gpu                         PASSED in 11.6s
//tensorflow/c/eager:c_api_remote_function_test_gpu                      PASSED in 12.5s
//tensorflow/c/eager:c_api_remote_test_gpu                               PASSED in 12.0s
//tensorflow/c/eager:c_api_test_gpu                                      PASSED in 11.7s
//tensorflow/c/eager:c_api_unified_experimental_test                     PASSED in 45.6s
//tensorflow/c/eager:c_api_unified_experimental_test_gpu                 PASSED in 10.2s
//tensorflow/c/eager:gradient_checker_test                               PASSED in 26.9s
//tensorflow/c/eager:gradient_checker_test_gpu                           PASSED in 10.5s
//tensorflow/c/eager:gradients_test                                      PASSED in 32.3s
//tensorflow/c/eager:gradients_test_gpu                                  PASSED in 16.3s
//tensorflow/c/eager:mnist_gradients_test                                PASSED in 31.7s
//tensorflow/c/eager:mnist_gradients_test_gpu                            PASSED in 13.7s
//tensorflow/c/eager:unified_api_test                                    PASSED in 36.3s
//tensorflow/c/eager:unified_api_test_gpu                                PASSED in 18.7s
//tensorflow/c/experimental/gradients:custom_gradient_test               PASSED in 52.1s
//tensorflow/c/experimental/gradients:custom_gradient_test_gpu           PASSED in 12.1s
//tensorflow/c/experimental/gradients:nn_grad_test                       PASSED in 44.8s
//tensorflow/c/experimental/gradients:nn_grad_test_gpu                   PASSED in 11.9s
//tensorflow/compiler/jit:compilation_passes_test                        PASSED in 52.0s
//tensorflow/compiler/mlir/tools/kernel_gen/tests/tf_to_kernel:add_v2.mlir.test PASSED in 9.9s
//tensorflow/compiler/mlir/tools/kernel_gen/tests/tf_to_kernel:tanh.mlir.test PASSED in 9.8s
//tensorflow/compiler/tests:adadelta_test_gpu                            PASSED in 21.0s
//tensorflow/compiler/tests:adagrad_da_test_gpu                          PASSED in 12.7s
//tensorflow/compiler/tests:adagrad_test_gpu                             PASSED in 9.6s
//tensorflow/compiler/tests:adam_test_gpu                                PASSED in 22.0s
//tensorflow/compiler/tests:add_n_test_gpu                               PASSED in 7.8s
//tensorflow/compiler/tests:argminmax_test_gpu                           PASSED in 15.9s
//tensorflow/compiler/tests:argminmax_test_gpu_mlir_bridge_test          PASSED in 18.0s
//tensorflow/compiler/tests:bucketize_op_test_gpu                        PASSED in 11.2s
//tensorflow/compiler/tests:bucketize_op_test_gpu_mlir_bridge_test       PASSED in 8.4s
//tensorflow/compiler/tests:case_test_gpu                                PASSED in 7.9s
//tensorflow/compiler/tests:categorical_op_test_gpu                      PASSED in 15.1s
//tensorflow/compiler/tests:categorical_op_test_gpu_mlir_bridge_test     PASSED in 16.6s
//tensorflow/compiler/tests:cholesky_op_test_gpu                         PASSED in 10.1s
//tensorflow/compiler/tests:cholesky_op_test_gpu_mlir_bridge_test        PASSED in 18.7s
//tensorflow/compiler/tests:clustering_test_gpu                          PASSED in 20.2s
//tensorflow/compiler/tests:clustering_test_gpu_mlir_bridge_test         PASSED in 7.0s
//tensorflow/compiler/tests:concat_ops_test_gpu                          PASSED in 22.7s
//tensorflow/compiler/tests:concat_ops_test_gpu_mlir_bridge_test         PASSED in 10.6s
//tensorflow/compiler/tests:cond_test_gpu                                PASSED in 14.7s
//tensorflow/compiler/tests:data_format_ops_test_gpu                     PASSED in 10.6s
//tensorflow/compiler/tests:data_format_ops_test_gpu_mlir_bridge_test    PASSED in 11.0s
//tensorflow/compiler/tests:dense_layer_test_gpu                         PASSED in 12.1s
//tensorflow/compiler/tests:dynamic_slice_ops_test_gpu                   PASSED in 16.0s
//tensorflow/compiler/tests:dynamic_slice_ops_test_gpu_mlir_bridge_test  PASSED in 17.3s
//tensorflow/compiler/tests:dynamic_stitch_test_gpu                      PASSED in 8.9s
//tensorflow/compiler/tests:eager_test_gpu                               PASSED in 14.6s
//tensorflow/compiler/tests:einsum_op_test_gpu                           PASSED in 10.4s
//tensorflow/compiler/tests:einsum_op_test_gpu_mlir_bridge_test          PASSED in 18.9s
//tensorflow/compiler/tests:ensure_shape_op_test_gpu                     PASSED in 8.4s
//tensorflow/compiler/tests:extract_image_patches_op_test_gpu            PASSED in 9.0s
//tensorflow/compiler/tests:fake_quant_ops_test_gpu                      PASSED in 10.2s
//tensorflow/compiler/tests:fake_quant_ops_test_gpu_mlir_bridge_test     PASSED in 11.6s
//tensorflow/compiler/tests:fifo_queue_test_gpu                          PASSED in 8.1s
//tensorflow/compiler/tests:fifo_queue_test_gpu_mlir_bridge_test         PASSED in 9.2s
//tensorflow/compiler/tests:ftrl_test_gpu                                PASSED in 28.7s
//tensorflow/compiler/tests:function_test_gpu                            PASSED in 20.4s
//tensorflow/compiler/tests:function_test_gpu_mlir_bridge_test           PASSED in 8.3s
//tensorflow/compiler/tests:gather_nd_op_test_gpu                        PASSED in 23.1s
//tensorflow/compiler/tests:gather_nd_op_test_gpu_mlir_bridge_test       PASSED in 8.8s
//tensorflow/compiler/tests:gather_test_gpu                              PASSED in 35.8s
//tensorflow/compiler/tests:listdiff_op_test_gpu                         PASSED in 12.7s
//tensorflow/compiler/tests:listdiff_op_test_gpu_mlir_bridge_test        PASSED in 22.0s
//tensorflow/compiler/tests:lrn_ops_test_gpu                             PASSED in 9.2s
//tensorflow/compiler/tests:lrn_ops_test_gpu_mlir_bridge_test            PASSED in 8.8s
//tensorflow/compiler/tests:lstm_test_gpu                                PASSED in 16.0s
//tensorflow/compiler/tests:manip_ops_test_gpu                           PASSED in 11.9s
//tensorflow/compiler/tests:manip_ops_test_gpu_mlir_bridge_test          PASSED in 11.0s
//tensorflow/compiler/tests:matrix_band_part_test_gpu                    PASSED in 30.0s
//tensorflow/compiler/tests:matrix_band_part_test_gpu_mlir_bridge_test   PASSED in 29.8s
//tensorflow/compiler/tests:matrix_diag_ops_test_gpu                     PASSED in 162.0s
//tensorflow/compiler/tests:matrix_diag_ops_test_gpu_mlir_bridge_test    PASSED in 181.8s
//tensorflow/compiler/tests:matrix_inverse_op_test_gpu                   PASSED in 13.4s
//tensorflow/compiler/tests:matrix_solve_op_test_gpu                     PASSED in 9.3s
//tensorflow/compiler/tests:matrix_triangular_solve_op_test_gpu          PASSED in 21.1s
//tensorflow/compiler/tests:matrix_triangular_solve_op_test_gpu_mlir_bridge_test PASSED in 13.9s
//tensorflow/compiler/tests:momentum_test_gpu                            PASSED in 9.3s
//tensorflow/compiler/tests:nary_ops_test_gpu                            PASSED in 8.3s
//tensorflow/compiler/tests:nary_ops_test_gpu_mlir_bridge_test           PASSED in 8.8s
//tensorflow/compiler/tests:nullary_ops_test_gpu                         PASSED in 9.5s
//tensorflow/compiler/tests:nullary_ops_test_gpu_mlir_bridge_test        PASSED in 16.9s
//tensorflow/compiler/tests:placeholder_test_gpu                         PASSED in 7.1s
//tensorflow/compiler/tests:placeholder_test_gpu_mlir_bridge_test        PASSED in 7.2s
//tensorflow/compiler/tests:proximal_adagrad_test_gpu                    PASSED in 17.0s
//tensorflow/compiler/tests:proximal_gradient_descent_test_gpu           PASSED in 7.2s
//tensorflow/compiler/tests:quantized_ops_test_gpu                       PASSED in 8.8s
//tensorflow/compiler/tests:reduce_window_test_gpu                       PASSED in 9.1s
//tensorflow/compiler/tests:reshape_op_test_gpu                          PASSED in 9.6s
//tensorflow/compiler/tests:reshape_op_test_gpu_mlir_bridge_test         PASSED in 7.4s
//tensorflow/compiler/tests:reverse_ops_test_gpu                         PASSED in 9.7s
//tensorflow/compiler/tests:reverse_ops_test_gpu_mlir_bridge_test        PASSED in 11.1s
//tensorflow/compiler/tests:reverse_sequence_op_test_gpu                 PASSED in 8.4s
//tensorflow/compiler/tests:reverse_sequence_op_test_gpu_mlir_bridge_test PASSED in 9.2s
//tensorflow/compiler/tests:rmsprop_test_gpu                             PASSED in 11.8s
//tensorflow/compiler/tests:scan_ops_test_gpu                            PASSED in 20.6s
//tensorflow/compiler/tests:scan_ops_test_gpu_mlir_bridge_test           PASSED in 24.7s
//tensorflow/compiler/tests:scatter_nd_op_test_gpu                       PASSED in 16.6s
//tensorflow/compiler/tests:scatter_nd_op_test_gpu_mlir_bridge_test      PASSED in 17.8s
//tensorflow/compiler/tests:searchsorted_op_test_gpu                     PASSED in 8.5s
//tensorflow/compiler/tests:searchsorted_op_test_gpu_mlir_bridge_test    PASSED in 8.4s
//tensorflow/compiler/tests:segment_reduction_ops_test_gpu               PASSED in 17.6s
//tensorflow/compiler/tests:segment_reduction_ops_test_gpu_mlir_bridge_test PASSED in 18.8s
//tensorflow/compiler/tests:self_adjoint_eig_op_test_gpu                 PASSED in 13.6s
//tensorflow/compiler/tests:slice_ops_test_gpu                           PASSED in 15.7s
//tensorflow/compiler/tests:slice_ops_test_gpu_mlir_bridge_test          PASSED in 15.7s
//tensorflow/compiler/tests:sparse_to_dense_op_test_gpu                  PASSED in 8.3s
//tensorflow/compiler/tests:sparse_to_dense_op_test_gpu_mlir_bridge_test PASSED in 19.2s
//tensorflow/compiler/tests:stack_ops_test_gpu                           PASSED in 7.9s
//tensorflow/compiler/tests:stateless_random_ops_test_gpu                PASSED in 53.9s
//tensorflow/compiler/tests:stateless_random_ops_test_gpu_mlir_bridge_test PASSED in 44.6s
//tensorflow/compiler/tests:svd_op_test_gpu                              PASSED in 107.9s
//tensorflow/compiler/tests:tensor_list_ops_test_gpu                     PASSED in 9.7s
//tensorflow/compiler/tests:ternary_ops_test_gpu                         PASSED in 26.8s
//tensorflow/compiler/tests:ternary_ops_test_gpu_mlir_bridge_test        PASSED in 28.8s
//tensorflow/compiler/tests:unary_ops_composition_test                   PASSED in 24.3s
//tensorflow/compiler/tests:unary_ops_composition_test_gpu               PASSED in 6.8s
//tensorflow/compiler/tests:unary_ops_test_gpu                           PASSED in 87.7s
//tensorflow/compiler/tests:unary_ops_test_gpu_mlir_bridge_test          PASSED in 75.9s
//tensorflow/compiler/tests:variable_ops_test_gpu                        PASSED in 18.4s
//tensorflow/compiler/tests:while_test_gpu                               PASSED in 17.7s
//tensorflow/compiler/tests:xla_device_gpu_test_gpu                      PASSED in 15.2s
//tensorflow/compiler/tests:xla_device_test_gpu                          PASSED in 11.7s
//tensorflow/compiler/tests:xla_device_test_gpu_mlir_bridge_test         PASSED in 13.6s
//tensorflow/compiler/tests:xla_ops_test_gpu                             PASSED in 37.9s
//tensorflow/compiler/tests:xla_ops_test_gpu_mlir_bridge_test            PASSED in 20.6s
//tensorflow/compiler/tf2xla:fused_batchnorm_reserve_space_test_gpu      PASSED in 17.3s
//tensorflow/compiler/tf2xla:resource_util_test_gpu                      PASSED in 8.2s
//tensorflow/compiler/xla/client/lib:arithmetic_test_gpu                 PASSED in 13.2s
//tensorflow/compiler/xla/client/lib:comparators_test_gpu                PASSED in 20.2s
//tensorflow/compiler/xla/client/lib:constants_test_gpu                  PASSED in 23.6s
//tensorflow/compiler/xla/client/lib:logdet_test_gpu                     PASSED in 17.4s
//tensorflow/compiler/xla/client/lib:math_test_gpu                       PASSED in 22.5s
//tensorflow/compiler/xla/client/lib:matrix_test_gpu                     PASSED in 20.2s
//tensorflow/compiler/xla/client/lib:pooling_test_gpu                    PASSED in 14.6s
//tensorflow/compiler/xla/client/lib:qr_test_gpu                         PASSED in 16.0s
//tensorflow/compiler/xla/client/lib:quantize_test_gpu                   PASSED in 17.2s
//tensorflow/compiler/xla/client/lib:slicing_test_gpu                    PASSED in 22.6s
//tensorflow/compiler/xla/client/lib:sorting_test_gpu                    PASSED in 9.3s
//tensorflow/compiler/xla/service:dynamic_padder_test_gpu                PASSED in 32.9s
//tensorflow/compiler/xla/service:dynamic_update_slice_test_gpu          PASSED in 16.8s
//tensorflow/compiler/xla/service:elemental_ir_emitter_test_gpu          PASSED in 16.3s
//tensorflow/compiler/xla/service/gpu:buffer_comparator_test             PASSED in 9.9s
//tensorflow/compiler/xla/service/gpu:cudnn_pad_for_convolutions_test    PASSED in 11.5s
//tensorflow/compiler/xla/service/gpu:custom_call_test                   PASSED in 7.4s
//tensorflow/compiler/xla/service/gpu:gpu_conv_padding_legalization_test PASSED in 6.9s
//tensorflow/compiler/xla/service/gpu:gpu_conv_rewriter_test             PASSED in 24.6s
//tensorflow/compiler/xla/service/gpu:gpu_debug_info_manager_test        PASSED in 4.8s
//tensorflow/compiler/xla/service/gpu:gpu_layout_assignment_test         PASSED in 4.8s
//tensorflow/compiler/xla/service/gpu:gpu_sanitize_constant_names_test   PASSED in 12.8s
//tensorflow/compiler/xla/service/gpu:horizontal_input_fusion_test       PASSED in 89.1s
//tensorflow/compiler/xla/service/gpu/tests:constant.hlo.test            PASSED in 8.4s
//tensorflow/compiler/xla/service/gpu/tests:copy.hlo.test                PASSED in 6.5s
//tensorflow/compiler/xla/service/gpu/tests:copy_nested.hlo.test         PASSED in 9.2s
//tensorflow/compiler/xla/service/gpu/tests:elementwise.hlo.test         PASSED in 7.4s
//tensorflow/compiler/xla/service/gpu/tests:execute_memzero_thunk.mlir.test PASSED in 7.6s
//tensorflow/compiler/xla/service/gpu/tests:fusion.hlo.test              PASSED in 8.4s
//tensorflow/compiler/xla/service/gpu/tests:gemm_rewrite_test            PASSED in 13.7s
//tensorflow/compiler/xla/service/gpu/tests:gpu_alignment_test           PASSED in 17.6s
//tensorflow/compiler/xla/service/gpu/tests:gpu_atomic_test              PASSED in 11.3s
//tensorflow/compiler/xla/service/gpu/tests:gpu_copy_alone_test          PASSED in 13.1s
//tensorflow/compiler/xla/service/gpu/tests:gpu_copy_test                PASSED in 13.1s
//tensorflow/compiler/xla/service/gpu/tests:gpu_dyn_shape_test           PASSED in 12.9s
//tensorflow/compiler/xla/service/gpu/tests:gpu_ftz_test                 PASSED in 14.9s
//tensorflow/compiler/xla/service/gpu/tests:gpu_fusion_test              PASSED in 6.7s
//tensorflow/compiler/xla/service/gpu/tests:gpu_index_test               PASSED in 14.9s
//tensorflow/compiler/xla/service/gpu/tests:gpu_infeed_test              PASSED in 8.8s
//tensorflow/compiler/xla/service/gpu/tests:gpu_input_fusible_slice_test PASSED in 12.3s
//tensorflow/compiler/xla/service/gpu/tests:gpu_kernel_tiling_test       PASSED in 27.7s
//tensorflow/compiler/xla/service/gpu/tests:gpu_ldg_test                 PASSED in 13.3s
//tensorflow/compiler/xla/service/gpu/tests:gpu_noalias_test             PASSED in 20.5s
//tensorflow/compiler/xla/service/gpu/tests:gpu_unrolling_test           PASSED in 34.0s
//tensorflow/compiler/xla/service/gpu/tests:launch_dimensions.hlo.test   PASSED in 8.7s
//tensorflow/compiler/xla/service/gpu/tests:launch_dimensions_big.hlo.test PASSED in 6.7s
//tensorflow/compiler/xla/service/gpu/tests:mlir_sorting_test            PASSED in 9.1s
//tensorflow/compiler/xla/service/gpu/tests:pad_to_static.hlo.test       PASSED in 9.4s
//tensorflow/compiler/xla/service/gpu/tests:parallel_reduction_test      PASSED in 14.4s
//tensorflow/compiler/xla/service/gpu/tests:reduce_nested.hlo.test       PASSED in 7.4s
//tensorflow/compiler/xla/service/gpu/tests:reduce_unnested.hlo.test     PASSED in 8.1s
//tensorflow/compiler/xla/service/gpu/tests:reduction_degenerate_dim_remover_test PASSED in 11.8s
//tensorflow/compiler/xla/service/gpu/tests:reduction_dimension_grouper_test PASSED in 13.5s
//tensorflow/compiler/xla/service/gpu/tests:reduction_layout_normalizer_test PASSED in 18.6s
//tensorflow/compiler/xla/service/gpu/tests:reduction_vectorization_test PASSED in 14.7s
//tensorflow/compiler/xla/service/gpu/tests:scatter.hlo.test             PASSED in 5.9s
//tensorflow/compiler/xla/service/gpu/tests:select_and_scatter.hlo.test  PASSED in 8.2s
//tensorflow/compiler/xla/service/gpu/tests:slice_to_dynamic.hlo.test    PASSED in 8.7s
//tensorflow/compiler/xla/service/gpu/tests:sorting.hlo.test             PASSED in 7.8s
//tensorflow/compiler/xla/service/gpu/tests:sorting_test                 PASSED in 16.4s
//tensorflow/compiler/xla/service/gpu/tests:tree_reduction_rewriter_test PASSED in 26.0s
//tensorflow/compiler/xla/tests:all_reduce_test_gpu                      PASSED in 15.2s
//tensorflow/compiler/xla/tests:axpy_simple_test_gpu                     PASSED in 13.0s
//tensorflow/compiler/xla/tests:bad_rng_shape_validation_test_gpu        PASSED in 9.3s
//tensorflow/compiler/xla/tests:binop_scaling_test_gpu                   PASSED in 13.0s
//tensorflow/compiler/xla/tests:bitcast_convert_test_gpu                 PASSED in 14.1s
//tensorflow/compiler/xla/tests:broadcast_simple_test_gpu                PASSED in 19.0s
//tensorflow/compiler/xla/tests:broadcast_test_gpu                       PASSED in 7.1s
//tensorflow/compiler/xla/tests:buffer_donation_test_gpu                 PASSED in 16.8s
//tensorflow/compiler/xla/tests:call_test_gpu                            PASSED in 23.4s
//tensorflow/compiler/xla/tests:check_execution_arity_test_gpu           PASSED in 11.8s
//tensorflow/compiler/xla/tests:cholesky_test_gpu                        PASSED in 14.2s
//tensorflow/compiler/xla/tests:client_test_gpu                          PASSED in 8.7s
//tensorflow/compiler/xla/tests:compilation_cache_test_gpu               PASSED in 13.2s
//tensorflow/compiler/xla/tests:compute_constant_test_gpu                PASSED in 17.1s
//tensorflow/compiler/xla/tests:concat_test_gpu                          PASSED in 21.4s
//tensorflow/compiler/xla/tests:constants_test_gpu                       PASSED in 14.0s
//tensorflow/compiler/xla/tests:convert_test_gpu                         PASSED in 10.4s
//tensorflow/compiler/xla/tests:copy_test_gpu                            PASSED in 17.3s
//tensorflow/compiler/xla/tests:cpu_gpu_fusion_test_gpu                  PASSED in 14.9s
//tensorflow/compiler/xla/tests:deallocation_test_gpu                    PASSED in 12.7s
//tensorflow/compiler/xla/tests:deconstruct_tuple_test_gpu               PASSED in 7.6s
//tensorflow/compiler/xla/tests:deep_graph_test_gpu                      PASSED in 15.1s
//tensorflow/compiler/xla/tests:dynamic_ops_test_gpu                     PASSED in 12.4s
//tensorflow/compiler/xla/tests:dynamism_inference_test_gpu              PASSED in 11.7s
//tensorflow/compiler/xla/tests:execution_profile_test_gpu               PASSED in 16.2s
//tensorflow/compiler/xla/tests:execution_profile_test_with_xla_hlo_profile_gpu PASSED in 11.7s
//tensorflow/compiler/xla/tests:floor_ceil_test_gpu                      PASSED in 19.6s
//tensorflow/compiler/xla/tests:fmax_fmin_test_gpu                       PASSED in 12.8s
//tensorflow/compiler/xla/tests:gather_operation_test_gpu                PASSED in 20.8s
//tensorflow/compiler/xla/tests:get_dimension_size_test_gpu              PASSED in 20.5s
//tensorflow/compiler/xla/tests:half_test_gpu                            PASSED in 18.1s
//tensorflow/compiler/xla/tests:llvm_compiler_test                       PASSED in 17.5s
//tensorflow/compiler/xla/tests:local_client_allocation_test_gpu         PASSED in 13.1s
//tensorflow/compiler/xla/tests:log_test_gpu                             PASSED in 15.6s
//tensorflow/compiler/xla/tests:map_test_gpu                             PASSED in 13.5s
//tensorflow/compiler/xla/tests:matrix_ops_simple_test_gpu               PASSED in 22.2s
//tensorflow/compiler/xla/tests:multidimensional_slice_test_gpu          PASSED in 14.5s
//tensorflow/compiler/xla/tests:multioutput_fusion_test_gpu              PASSED in 25.0s
//tensorflow/compiler/xla/tests:outfeed_in_nested_computation_test_gpu   PASSED in 7.7s
//tensorflow/compiler/xla/tests:pad_test_gpu                             PASSED in 13.3s
//tensorflow/compiler/xla/tests:pred_test_gpu                            PASSED in 12.9s
//tensorflow/compiler/xla/tests:query_inferred_shape_test_gpu            PASSED in 7.1s
//tensorflow/compiler/xla/tests:reduce_hlo_test_gpu                      PASSED in 13.5s
//tensorflow/compiler/xla/tests:reduce_precision_test_gpu                PASSED in 17.3s
//tensorflow/compiler/xla/tests:replay_test_gpu                          PASSED in 13.0s
//tensorflow/compiler/xla/tests:reshape_motion_test_gpu                  PASSED in 17.4s
//tensorflow/compiler/xla/tests:reverse_test_gpu                         PASSED in 9.1s
//tensorflow/compiler/xla/tests:round_trip_packed_literal_test_gpu       PASSED in 6.3s
//tensorflow/compiler/xla/tests:round_trip_transfer_test_gpu             PASSED in 6.6s
//tensorflow/compiler/xla/tests:sample_file_test                         PASSED in 30.8s
//tensorflow/compiler/xla/tests:sample_text_test_gpu                     PASSED in 12.0s
//tensorflow/compiler/xla/tests:scatter_test_gpu                         PASSED in 19.5s
//tensorflow/compiler/xla/tests:select_and_scatter_test_gpu              PASSED in 55.0s
//tensorflow/compiler/xla/tests:select_test_gpu                          PASSED in 20.9s
//tensorflow/compiler/xla/tests:token_hlo_test_gpu                       PASSED in 16.0s
//tensorflow/compiler/xla/tests:transfer_manager_test_gpu                PASSED in 19.8s
//tensorflow/compiler/xla/tests:transpose_test_gpu                       PASSED in 19.9s
//tensorflow/compiler/xla/tests:triangular_solve_test_gpu                PASSED in 25.9s
//tensorflow/compiler/xla/tests:tuple_test_gpu                           PASSED in 13.6s
//tensorflow/compiler/xla/tests:unary_op_test_gpu                        PASSED in 13.7s
//tensorflow/compiler/xla/tests:vector_ops_reduce_test_gpu               PASSED in 7.9s
//tensorflow/compiler/xla/tests:vector_ops_simple_test_gpu               PASSED in 20.6s
//tensorflow/compiler/xla/tests:while_test_gpu                           PASSED in 31.6s
//tensorflow/compiler/xla/tests:xla_hlo_profile_test_gpu                 PASSED in 15.2s
//tensorflow/core/common_runtime:constant_folding_test                   PASSED in 5.6s
//tensorflow/core/common_runtime:direct_session_test_gpu                 PASSED in 13.6s
//tensorflow/core/common_runtime:hierarchical_tree_broadcaster_test_gpu  PASSED in 16.7s
//tensorflow/core/common_runtime:memory_types_test                       PASSED in 10.6s
//tensorflow/core/common_runtime:memory_types_test_gpu                   PASSED in 9.1s
//tensorflow/core/common_runtime:permuter_test_gpu                       PASSED in 12.8s
//tensorflow/core/common_runtime:process_function_library_runtime_test_gpu PASSED in 13.2s
//tensorflow/core/common_runtime:ring_gatherer_test_gpu                  PASSED in 18.2s
//tensorflow/core/common_runtime:ring_reducer_test_gpu                   PASSED in 20.2s
//tensorflow/core/common_runtime/device:device_event_mgr_test            PASSED in 10.5s
//tensorflow/core/common_runtime/device:device_event_mgr_test_gpu        PASSED in 12.5s
//tensorflow/core/common_runtime/device:device_id_manager_test           PASSED in 17.1s
//tensorflow/core/common_runtime/device:device_id_manager_test_gpu       PASSED in 9.6s
//tensorflow/core/common_runtime/gpu:gpu_allocator_retry_test            PASSED in 10.7s
//tensorflow/core/common_runtime/gpu:gpu_allocator_retry_test_gpu        PASSED in 8.5s
//tensorflow/core/common_runtime/gpu:gpu_bfc_allocator_test              PASSED in 26.6s
//tensorflow/core/common_runtime/gpu:gpu_bfc_allocator_test_gpu          PASSED in 37.3s
//tensorflow/core/common_runtime/gpu:gpu_debug_allocator_test            PASSED in 24.2s
//tensorflow/core/common_runtime/gpu:gpu_debug_allocator_test_gpu        PASSED in 16.6s
//tensorflow/core/common_runtime/gpu:gpu_device_test                     PASSED in 7.0s
//tensorflow/core/common_runtime/gpu:gpu_device_test_gpu                 PASSED in 10.8s
//tensorflow/core/common_runtime/gpu:gpu_device_unified_memory_test      PASSED in 16.1s
//tensorflow/core/common_runtime/gpu:gpu_device_unified_memory_test_gpu  PASSED in 11.0s
//tensorflow/core/common_runtime/gpu:gpu_virtual_mem_allocator_test      PASSED in 19.0s
//tensorflow/core/common_runtime/gpu:pool_allocator_test                 PASSED in 11.2s
//tensorflow/core/common_runtime/gpu:pool_allocator_test_gpu             PASSED in 11.5s
//tensorflow/core/distributed_runtime:rpcbench_test                      PASSED in 7.6s
//tensorflow/core/distributed_runtime:rpcbench_test_gpu                  PASSED in 9.0s
//tensorflow/core/distributed_runtime/rpc:grpc_channel_test              PASSED in 42.3s
//tensorflow/core/distributed_runtime/rpc:grpc_channel_test_gpu          PASSED in 12.2s
//tensorflow/core/distributed_runtime/rpc:rpc_rendezvous_mgr_test        PASSED in 25.8s
//tensorflow/core/distributed_runtime/rpc:rpc_rendezvous_mgr_test_gpu    PASSED in 14.4s
//tensorflow/core/framework:variant_op_copy_test                         PASSED in 12.5s
//tensorflow/core/framework:variant_op_copy_test_gpu                     PASSED in 6.1s
//tensorflow/core/grappler/clusters:utils_test                           PASSED in 9.4s
//tensorflow/core/grappler/optimizers:arithmetic_optimizer_test_gpu      PASSED in 18.6s
//tensorflow/core/grappler/optimizers:auto_mixed_precision_test_gpu      PASSED in 11.2s
//tensorflow/core/grappler/optimizers:auto_parallel_test_gpu             PASSED in 7.8s
//tensorflow/core/grappler/optimizers:common_subgraph_elimination_test_gpu PASSED in 10.0s
//tensorflow/core/grappler/optimizers:custom_graph_optimizer_registry_test_gpu PASSED in 9.7s
//tensorflow/core/grappler/optimizers:debug_stripper_test_gpu            PASSED in 13.3s
//tensorflow/core/grappler/optimizers:dependency_optimizer_test_gpu      PASSED in 7.6s
//tensorflow/core/grappler/optimizers:function_optimizer_test_gpu        PASSED in 9.9s
//tensorflow/core/grappler/optimizers:generic_layout_optimizer_test_gpu  PASSED in 16.0s
//tensorflow/core/grappler/optimizers:generic_layout_optimizer_transposer_test_gpu PASSED in 15.9s
//tensorflow/core/grappler/optimizers:graph_optimizer_stage_test_gpu     PASSED in 13.4s
//tensorflow/core/grappler/optimizers:layout_optimizer_test_gpu          PASSED in 17.2s
//tensorflow/core/grappler/optimizers:loop_optimizer_test_gpu            PASSED in 9.2s
//tensorflow/core/grappler/optimizers:memory_optimizer_test_gpu          PASSED in 9.5s
//tensorflow/core/grappler/optimizers:model_pruner_test_gpu              PASSED in 14.2s
//tensorflow/core/grappler/optimizers:pin_to_host_optimizer_test_gpu     PASSED in 15.1s
//tensorflow/core/grappler/optimizers:remapper_test_gpu                  PASSED in 15.9s
//tensorflow/core/grappler/optimizers:shape_optimizer_test_gpu           PASSED in 44.0s
//tensorflow/core/grappler/optimizers:static_schedule_test_gpu           PASSED in 8.8s
//tensorflow/core/kernels:bincount_op_test_gpu                           PASSED in 2.9s
//tensorflow/core/kernels:broadcast_to_op_test_gpu                       PASSED in 8.9s
//tensorflow/core/kernels:cast_op_test_gpu                               PASSED in 3.1s
//tensorflow/core/kernels:constant_op_test_gpu                           PASSED in 4.9s
//tensorflow/core/kernels:conv_grad_filter_ops_benchmark_test_gpu        PASSED in 3.8s
//tensorflow/core/kernels:conv_grad_input_ops_benchmark_test_gpu         PASSED in 12.0s
//tensorflow/core/kernels:conv_ops_benchmark_test_gpu                    PASSED in 9.4s
//tensorflow/core/kernels:conv_ops_test_gpu                              PASSED in 19.8s
//tensorflow/core/kernels:cwise_ops_test_gpu                             PASSED in 5.9s
//tensorflow/core/kernels:depthwise_conv_ops_test                        PASSED in 25.0s
//tensorflow/core/kernels:depthwise_conv_ops_test_gpu                    PASSED in 15.5s
//tensorflow/core/kernels:diag_op_test_gpu                               PASSED in 4.6s
//tensorflow/core/kernels:dynamic_partition_op_test_gpu                  PASSED in 8.8s
//tensorflow/core/kernels:dynamic_stitch_op_test_gpu                     PASSED in 3.1s
//tensorflow/core/kernels:fused_batch_norm_ex_op_test_gpu                PASSED in 15.9s
//tensorflow/core/kernels:fused_batch_norm_op_test_gpu                   PASSED in 8.9s
//tensorflow/core/kernels:gather_nd_op_test_gpu                          PASSED in 9.3s
//tensorflow/core/kernels:gather_op_test_gpu                             PASSED in 4.2s
//tensorflow/core/kernels:lrn_op_test_gpu                                PASSED in 4.9s
//tensorflow/core/kernels:matmul_op_test_gpu                             PASSED in 11.0s
//tensorflow/core/kernels:mfcc_op_test_gpu                               PASSED in 12.8s
//tensorflow/core/kernels:multinomial_op_test_gpu                        PASSED in 9.8s
//tensorflow/core/kernels:nn_ops_test_gpu                                PASSED in 8.9s
//tensorflow/core/kernels:parameterized_truncated_normal_op_test_gpu     PASSED in 9.0s
//tensorflow/core/kernels:quantize_and_dequantize_op_test_gpu            PASSED in 4.3s
//tensorflow/core/kernels:random_binomial_op_test_gpu                    PASSED in 5.4s
//tensorflow/core/kernels:random_op_test_gpu                             PASSED in 3.9s
//tensorflow/core/kernels:random_poisson_op_test_gpu                     PASSED in 2.7s
//tensorflow/core/kernels:reduction_ops_test_gpu                         PASSED in 9.4s
//tensorflow/core/kernels:scan_ops_test_gpu                              PASSED in 9.0s
//tensorflow/core/kernels:scatter_nd_op_test_gpu                         PASSED in 10.8s
//tensorflow/core/kernels:scoped_allocator_ops_test_gpu                  PASSED in 13.4s
//tensorflow/core/kernels:spacetobatch_benchmark_test_gpu                PASSED in 8.8s
//tensorflow/core/kernels:sparse_matmul_op_test_gpu                      PASSED in 11.9s
//tensorflow/core/kernels:sparse_tensor_dense_matmul_op_test_gpu         PASSED in 4.9s
//tensorflow/core/kernels:sparse_to_dense_op_test_gpu                    PASSED in 5.7s
//tensorflow/core/kernels:sparse_xent_op_test_gpu                        PASSED in 4.2s
//tensorflow/core/kernels:spectrogram_op_test_gpu                        PASSED in 13.2s
//tensorflow/core/kernels:split_op_test_gpu                              PASSED in 8.5s
//tensorflow/core/kernels:split_v_op_test_gpu                            PASSED in 8.6s
//tensorflow/core/kernels:unary_ops_composition_test_gpu                 PASSED in 13.2s
//tensorflow/core/kernels:xent_op_test_gpu                               PASSED in 10.9s
//tensorflow/core/kernels/image:adjust_contrast_op_benchmark_test_gpu    PASSED in 3.7s
//tensorflow/core/kernels/image:crop_and_resize_op_benchmark_test_gpu    PASSED in 5.7s
//tensorflow/core/kernels/image:mirror_pad_op_benchmark_test_gpu         PASSED in 12.9s
//tensorflow/core/kernels/image:non_max_suppression_op_gpu_test          PASSED in 18.7s
//tensorflow/core/kernels/image:non_max_suppression_op_gpu_test_gpu      PASSED in 12.8s
//tensorflow/core/kernels/image:resize_benchmark_test_gpu                PASSED in 3.0s
//tensorflow/core/kernels/image:resize_ops_test_gpu                      PASSED in 19.2s
//tensorflow/core/kernels/linalg:banded_triangular_solve_op_test_gpu     PASSED in 4.6s
//tensorflow/core/kernels/linalg:matrix_triangular_solve_op_test_gpu     PASSED in 14.2s
//tensorflow/core/kernels/mlir_generated:gpu_binary_ops_test             PASSED in 11.2s
//tensorflow/core/kernels/mlir_generated:gpu_binary_ops_test_gpu         PASSED in 8.4s
//tensorflow/core/kernels/mlir_generated:gpu_unary_ops_test              PASSED in 13.9s
//tensorflow/core/kernels/mlir_generated:gpu_unary_ops_test_gpu          PASSED in 7.8s
//tensorflow/core/platform:rocm_rocdl_path_test                          PASSED in 8.2s
//tensorflow/core/platform:rocm_rocdl_path_test_gpu                      PASSED in 3.3s
//tensorflow/core/util:gpu_kernel_helper_test_gpu                        PASSED in 4.2s
//tensorflow/examples/adding_an_op:cuda_op_test                          PASSED in 20.5s
//tensorflow/python:arithmetic_optimizer_test_gpu                        PASSED in 7.8s
//tensorflow/python:auto_mixed_precision_test_gpu                        PASSED in 18.6s
//tensorflow/python:batch_norm_benchmark_gpu                             PASSED in 6.0s
//tensorflow/python:bitwise_ops_test_gpu                                 PASSED in 7.8s
//tensorflow/python:collective_ops_benchmark_gpu                         PASSED in 5.9s
//tensorflow/python:collective_ops_gpu_test_gpu                          PASSED in 15.9s
//tensorflow/python:concat_benchmark_gpu                                 PASSED in 5.9s
//tensorflow/python:config_test_gpu                                      PASSED in 8.0s
//tensorflow/python:constant_folding_test_gpu                            PASSED in 8.2s
//tensorflow/python:control_flow_ops_benchmark_gpu                       PASSED in 5.4s
//tensorflow/python:device_lib_test                                      PASSED in 7.0s
//tensorflow/python:gradient_checker_v2_test_gpu                         PASSED in 19.0s
//tensorflow/python:graph_building_benchmark_gpu                         PASSED in 16.5s
//tensorflow/python:histogram_ops_test_gpu                               PASSED in 9.2s
//tensorflow/python:init_ops_test_gpu                                    PASSED in 8.1s
//tensorflow/python:init_ops_v2_test_gpu                                 PASSED in 10.9s
//tensorflow/python:math_grad_test_gpu                                   PASSED in 11.7s
//tensorflow/python:math_ops_linspace_test_gpu                           PASSED in 7.9s
//tensorflow/python:math_ops_test_gpu                                    PASSED in 10.6s
//tensorflow/python:matmul_benchmark_gpu                                 PASSED in 6.2s
//tensorflow/python:nn_grad_test_gpu                                     PASSED in 10.9s
//tensorflow/python:nn_test_gpu                                          PASSED in 45.2s
//tensorflow/python:nn_xent_test_gpu                                     PASSED in 20.6s
//tensorflow/python:op_callbacks_test_gpu                                PASSED in 11.4s
//tensorflow/python:raw_ops_test_gpu                                     PASSED in 6.4s
//tensorflow/python:rnn_grad_test_gpu                                    PASSED in 8.8s
//tensorflow/python:session_benchmark_gpu                                PASSED in 5.7s
//tensorflow/python:split_benchmark_gpu                                  PASSED in 6.5s
//tensorflow/python:stateful_random_ops_test_gpu                         PASSED in 10.7s
//tensorflow/python:transpose_benchmark_gpu                              PASSED in 5.4s
//tensorflow/python/compiler/tensorrt:trt_convert_test_gpu               PASSED in 28.7s
//tensorflow/python/compiler/xla:jit_compile_test_gpu                    PASSED in 10.4s
//tensorflow/python/compiler/xla:jit_test_gpu                            PASSED in 10.1s
//tensorflow/python/compiler/xla:xla_test_gpu                            PASSED in 17.8s
//tensorflow/python/data/experimental/kernel_tests:copy_to_device_test_gpu PASSED in 10.7s
//tensorflow/python/data/experimental/kernel_tests:prefetch_to_device_test_gpu PASSED in 23.7s
//tensorflow/python/data/experimental/kernel_tests:scan_test_gpu         PASSED in 11.6s
//tensorflow/python/data/experimental/kernel_tests:wrap_unwrap_test_gpu  PASSED in 9.3s
//tensorflow/python/data/experimental/kernel_tests/optimization:grappler_test_gpu PASSED in 8.2s
//tensorflow/python/data/kernel_tests:iterator_test_gpu                  PASSED in 19.1s
//tensorflow/python/data/kernel_tests:memory_cleanup_test_gpu            PASSED in 12.9s
//tensorflow/python/data/kernel_tests:optional_test_gpu                  PASSED in 23.2s
//tensorflow/python/data/kernel_tests:placement_test_gpu                 PASSED in 8.5s
//tensorflow/python/data/kernel_tests:reduce_test_gpu                    PASSED in 12.4s
//tensorflow/python/debug:analyzer_cli_test_gpu                          PASSED in 14.3s
//tensorflow/python/debug:check_numerics_callback_test_gpu               PASSED in 9.0s
//tensorflow/python/debug:debug_gradients_test_gpu                       PASSED in 8.6s
//tensorflow/python/debug:debug_graph_reconstruction_test_gpu            PASSED in 8.1s
//tensorflow/python/debug:debug_grappler_test_gpu                        PASSED in 8.1s
//tensorflow/python/debug:debug_v2_ops_test_gpu                          PASSED in 11.0s
//tensorflow/python/debug:session_debug_file_test_gpu                    PASSED in 11.2s
//tensorflow/python/debug:session_debug_multi_gpu_test_gpu               PASSED in 7.0s
//tensorflow/python/distribute:checkpoint_utils_test_gpu                 PASSED in 10.8s
//tensorflow/python/distribute:checkpointing_test_gpu                    PASSED in 8.2s
//tensorflow/python/distribute:collective_all_reduce_strategy_test_gpu   PASSED in 27.1s
//tensorflow/python/distribute:cross_device_utils_test_gpu               PASSED in 15.4s
//tensorflow/python/distribute:custom_training_loop_gradient_test_gpu    PASSED in 20.0s
//tensorflow/python/distribute:device_util_test_gpu                      PASSED in 8.0s
//tensorflow/python/distribute:distribute_utils_test_gpu                 PASSED in 8.1s
//tensorflow/python/distribute:input_ops_test_gpu                        PASSED in 9.9s
//tensorflow/python/distribute:metrics_v1_test_gpu                       PASSED in 22.1s
//tensorflow/python/distribute:mirrored_variable_test_gpu                PASSED in 19.8s
//tensorflow/python/distribute:moving_averages_test_gpu                  PASSED in 39.0s
//tensorflow/python/distribute:multi_worker_continuous_run_test_gpu      PASSED in 26.6s
//tensorflow/python/distribute:one_device_strategy_test_gpu              PASSED in 13.9s
//tensorflow/python/distribute:parameter_server_strategy_test_gpu        PASSED in 31.8s
//tensorflow/python/distribute:ps_values_test_gpu                        PASSED in 12.2s
//tensorflow/python/distribute:remote_mirrored_strategy_eager_test_gpu   PASSED in 11.9s
//tensorflow/python/distribute:strategy_combinations_test_gpu            PASSED in 31.7s
//tensorflow/python/distribute:test_util_test_gpu                        PASSED in 30.5s
//tensorflow/python/distribute:tf_function_test_gpu                      PASSED in 12.2s
//tensorflow/python/distribute:warm_starting_util_test_gpu               PASSED in 21.0s
//tensorflow/python/distribute:zero_batch_test_gpu                       PASSED in 14.3s
//tensorflow/python/distribute/integration_test:saved_model_test_gpu     PASSED in 29.6s
//tensorflow/python/distribute/parallel_device:parallel_device_test_gpu  PASSED in 16.7s
//tensorflow/python/distribute/v1:cross_device_ops_test_gpu              PASSED in 38.8s
//tensorflow/python/dlpack:dlpack_test_gpu                               PASSED in 7.8s
//tensorflow/python/eager:backprop_test_gpu                              PASSED in 65.1s
//tensorflow/python/eager:benchmarks_test_gpu                            PASSED in 13.0s
//tensorflow/python/eager:cancellation_test_gpu                          PASSED in 5.6s
//tensorflow/python/eager:context_test_gpu                               PASSED in 7.8s
//tensorflow/python/eager:core_test_gpu                                  PASSED in 18.2s
//tensorflow/python/eager:def_function_test_gpu                          PASSED in 15.2s
//tensorflow/python/eager:def_function_xla_jit_test_gpu                  PASSED in 20.6s
//tensorflow/python/eager:def_function_xla_jit_test_gpu_mlir_bridge_test PASSED in 10.5s
//tensorflow/python/eager:def_function_xla_test_gpu                      PASSED in 18.6s
//tensorflow/python/eager:function_argument_naming_test_gpu              PASSED in 8.9s
//tensorflow/python/eager:function_defun_collection_test_gpu             PASSED in 17.0s
//tensorflow/python/eager:graph_only_ops_test_gpu                        PASSED in 7.5s
//tensorflow/python/eager:monitoring_test_gpu                            PASSED in 5.8s
//tensorflow/python/eager:ops_test_gpu                                   PASSED in 9.5s
//tensorflow/python/eager:profiler_test_gpu                              PASSED in 11.8s
//tensorflow/python/eager:remote_benchmarks_test_gpu                     PASSED in 6.1s
//tensorflow/python/eager:tensor_test_gpu                                PASSED in 9.4s
//tensorflow/python/eager:wrap_function_device_test_gpu                  PASSED in 9.0s
//tensorflow/python/eager/benchmarks:kpi_benchmark_test_gpu              PASSED in 21.4s
//tensorflow/python/framework/experimental:unified_api_test_gpu          PASSED in 8.5s
//tensorflow/python/keras/benchmarks:antirectifier_benchmark_test_gpu    PASSED in 25.6s
//tensorflow/python/keras/benchmarks:bidirectional_lstm_benchmark_test_gpu PASSED in 28.1s
//tensorflow/python/keras/benchmarks:cifar10_cnn_benchmark_test_gpu      PASSED in 22.5s
//tensorflow/python/keras/benchmarks:eager_microbenchmarks_test_gpu      PASSED in 15.6s
//tensorflow/python/keras/benchmarks:mnist_conv_benchmark_test_gpu       PASSED in 13.4s
//tensorflow/python/keras/benchmarks:mnist_conv_custom_training_benchmark_test_gpu PASSED in 22.7s
//tensorflow/python/keras/benchmarks:mnist_hierarchical_rnn_benchmark_test_gpu PASSED in 22.3s
//tensorflow/python/keras/benchmarks:mnist_irnn_benchmark_test_gpu       PASSED in 15.6s
//tensorflow/python/keras/benchmarks:model_components_benchmarks_test_gpu PASSED in 21.6s
//tensorflow/python/keras/benchmarks:reuters_mlp_benchmark_test_gpu      PASSED in 24.6s
//tensorflow/python/keras/benchmarks:text_classification_transformer_benchmark_test_gpu PASSED in 20.6s
//tensorflow/python/keras/benchmarks/saved_model_benchmarks:densenet_benchmark_test_gpu PASSED in 12.1s
//tensorflow/python/keras/benchmarks/saved_model_benchmarks:efficientnet_benchmark_test_gpu PASSED in 12.7s
//tensorflow/python/keras/benchmarks/saved_model_benchmarks:inception_resnet_v2_benchmark_test_gpu PASSED in 14.2s
//tensorflow/python/keras/benchmarks/saved_model_benchmarks:mobilenet_benchmark_test_gpu PASSED in 21.0s
//tensorflow/python/keras/benchmarks/saved_model_benchmarks:nasnet_large_benchmark_test_gpu PASSED in 24.7s
//tensorflow/python/keras/benchmarks/saved_model_benchmarks:resnet152_v2_benchmark_test_gpu PASSED in 15.6s
//tensorflow/python/keras/benchmarks/saved_model_benchmarks:vgg_benchmark_test_gpu PASSED in 15.5s
//tensorflow/python/keras/benchmarks/saved_model_benchmarks:xception_benchmark_test_gpu PASSED in 10.3s
//tensorflow/python/keras/distribute:checkpointing_test_gpu              PASSED in 21.1s
//tensorflow/python/keras/distribute:collective_all_reduce_strategy_test_gpu PASSED in 27.2s
//tensorflow/python/keras/distribute:custom_training_loop_metrics_test_gpu PASSED in 33.7s
//tensorflow/python/keras/distribute:custom_training_loop_models_test_gpu PASSED in 151.2s
//tensorflow/python/keras/distribute:custom_training_loop_optimizer_test_gpu PASSED in 13.8s
//tensorflow/python/keras/distribute:distributed_training_utils_test_gpu PASSED in 12.4s
//tensorflow/python/keras/distribute:keras_metrics_test_gpu              PASSED in 9.5s
//tensorflow/python/keras/distribute:keras_models_test_gpu               PASSED in 21.4s
//tensorflow/python/keras/distribute:mirrored_strategy_test_gpu          PASSED in 17.0s
//tensorflow/python/keras/distribute:mirrored_variable_test_gpu          PASSED in 11.1s
//tensorflow/python/keras/engine:training_gpu_test_gpu                   PASSED in 30.0s
//tensorflow/python/keras/integration_test:gradient_checkpoint_test_gpu  PASSED in 31.9s
//tensorflow/python/keras/integration_test:saved_model_test_gpu          PASSED in 56.7s
//tensorflow/python/keras/layers:convolutional_transpose_test_gpu        PASSED in 43.7s
//tensorflow/python/keras/layers:embeddings_test_gpu                     PASSED in 11.5s
//tensorflow/python/keras/layers:separable_convolutional_test_gpu        PASSED in 7.3s
//tensorflow/python/keras/layers/preprocessing:category_crossing_distribution_test_gpu PASSED in 14.3s
//tensorflow/python/keras/layers/preprocessing:category_encoding_distribution_test_gpu PASSED in 15.7s
//tensorflow/python/keras/layers/preprocessing:discretization_distribution_test_gpu PASSED in 23.6s
//tensorflow/python/keras/layers/preprocessing:image_preprocessing_distribution_test_gpu PASSED in 30.8s
//tensorflow/python/keras/layers/preprocessing:text_vectorization_distribution_test_gpu PASSED in 16.5s
//tensorflow/python/keras/layers/preprocessing/benchmarks:image_preproc_benchmark_gpu PASSED in 8.1s
//tensorflow/python/keras/mixed_precision:device_compatibility_check_test_gpu PASSED in 9.3s
//tensorflow/python/keras/mixed_precision:loss_scale_benchmark_gpu       PASSED in 20.5s
//tensorflow/python/keras/mixed_precision:loss_scale_optimizer_test_gpu  PASSED in 25.3s
//tensorflow/python/keras/mixed_precision:mixed_precision_graph_rewrite_test_gpu PASSED in 11.9s
//tensorflow/python/keras/optimizer_v2:legacy_learning_rate_decay_test_gpu PASSED in 11.3s
//tensorflow/python/keras/tests:saver_test_gpu                           PASSED in 14.6s
//tensorflow/python/keras/tests:tracking_util_xla_test_gpu               PASSED in 12.7s
//tensorflow/python/keras/utils:multi_gpu_utils_test_gpu                 PASSED in 14.2s
//tensorflow/python/kernel_tests:aggregate_ops_test_gpu                  PASSED in 8.8s
//tensorflow/python/kernel_tests:argmax_op_test_gpu                      PASSED in 8.0s
//tensorflow/python/kernel_tests:banded_triangular_solve_op_test_gpu     PASSED in 8.9s
//tensorflow/python/kernel_tests:batchtospace_op_test_gpu                PASSED in 17.8s
//tensorflow/python/kernel_tests:benchmark_test_gpu                      PASSED in 8.7s
//tensorflow/python/kernel_tests:betainc_op_test_gpu                     PASSED in 9.4s
//tensorflow/python/kernel_tests:bias_op_deterministic_test_gpu          PASSED in 77.4s
//tensorflow/python/kernel_tests:bias_op_test_gpu                        PASSED in 55.4s
//tensorflow/python/kernel_tests:bincount_op_test_gpu                    PASSED in 9.5s
//tensorflow/python/kernel_tests:bitcast_op_test_gpu                     PASSED in 8.5s
//tensorflow/python/kernel_tests:broadcast_to_ops_test_gpu               PASSED in 16.2s
//tensorflow/python/kernel_tests:bucketize_op_test_gpu                   PASSED in 8.3s
//tensorflow/python/kernel_tests:cast_op_test_gpu                        PASSED in 17.9s
//tensorflow/python/kernel_tests:check_ops_test_gpu                      PASSED in 23.4s
//tensorflow/python/kernel_tests:collective_ops_test_gpu                 PASSED in 63.0s
//tensorflow/python/kernel_tests:compare_and_bitpack_op_test_gpu         PASSED in 8.2s
//tensorflow/python/kernel_tests:concat_op_test_gpu                      PASSED in 41.5s
//tensorflow/python/kernel_tests:cond_v2_test_gpu                        PASSED in 25.2s
//tensorflow/python/kernel_tests:constant_op_eager_test_gpu              PASSED in 7.2s
//tensorflow/python/kernel_tests:constant_op_test_gpu                    PASSED in 17.2s
//tensorflow/python/kernel_tests:conv1d_test_gpu                         PASSED in 25.2s
//tensorflow/python/kernel_tests:conv1d_transpose_test_gpu               PASSED in 21.9s
//tensorflow/python/kernel_tests:conv2d_transpose_test_gpu               PASSED in 14.3s
//tensorflow/python/kernel_tests:conv3d_backprop_filter_v2_grad_test_gpu PASSED in 18.6s
//tensorflow/python/kernel_tests:conv3d_transpose_test_gpu               PASSED in 11.2s
//tensorflow/python/kernel_tests:critical_section_test_gpu               PASSED in 9.9s
//tensorflow/python/kernel_tests:cross_grad_test_gpu                     PASSED in 14.6s
//tensorflow/python/kernel_tests:ctc_loss_op_test_gpu                    PASSED in 32.7s
//tensorflow/python/kernel_tests:cudnn_deterministic_ops_test_gpu        PASSED in 9.1s
//tensorflow/python/kernel_tests:cudnn_deterministic_test_gpu            PASSED in 32.3s
//tensorflow/python/kernel_tests:cumulative_logsumexp_test_gpu           PASSED in 16.3s
//tensorflow/python/kernel_tests:denormal_test_gpu                       PASSED in 7.1s
//tensorflow/python/kernel_tests:dense_update_ops_no_tsan_test_gpu       PASSED in 20.9s
//tensorflow/python/kernel_tests:dense_update_ops_test_gpu               PASSED in 17.3s
//tensorflow/python/kernel_tests:depthtospace_op_test_gpu                PASSED in 20.4s
//tensorflow/python/kernel_tests:determinant_op_test_gpu                 PASSED in 29.8s
//tensorflow/python/kernel_tests:dynamic_partition_op_test_gpu           PASSED in 21.1s
//tensorflow/python/kernel_tests:dynamic_stitch_op_test_gpu              PASSED in 8.8s
//tensorflow/python/kernel_tests:extract_image_patches_op_test_gpu       PASSED in 15.3s
//tensorflow/python/kernel_tests:extract_volume_patches_op_test_gpu      PASSED in 9.7s
//tensorflow/python/kernel_tests:gather_nd_op_test_gpu                   PASSED in 7.5s
//tensorflow/python/kernel_tests:gradient_correctness_test_gpu           PASSED in 8.4s
//tensorflow/python/kernel_tests:in_topk_op_test_gpu                     PASSED in 6.9s
//tensorflow/python/kernel_tests:large_concat_op_test_gpu                PASSED in 13.0s
//tensorflow/python/kernel_tests:linalg_ops_test_gpu                     PASSED in 23.0s
//tensorflow/python/kernel_tests:list_ops_test_gpu                       PASSED in 14.2s
//tensorflow/python/kernel_tests:logging_ops_test_gpu                    PASSED in 7.4s
//tensorflow/python/kernel_tests:lrn_op_test_gpu                         PASSED in 10.4s
//tensorflow/python/kernel_tests:lu_op_test_gpu                          PASSED in 13.9s
//tensorflow/python/kernel_tests:manip_ops_test_gpu                      PASSED in 8.8s
//tensorflow/python/kernel_tests:matrix_inverse_op_test_gpu              PASSED in 24.5s
//tensorflow/python/kernel_tests:matrix_solve_ls_op_test_gpu             PASSED in 14.5s
//tensorflow/python/kernel_tests:matrix_solve_op_test_gpu                PASSED in 30.9s
//tensorflow/python/kernel_tests:matrix_square_root_op_test_gpu          PASSED in 7.7s
//tensorflow/python/kernel_tests:morphological_ops_test_gpu              PASSED in 8.7s
//tensorflow/python/kernel_tests:nth_element_op_test_gpu                 PASSED in 8.6s
//tensorflow/python/kernel_tests:numerics_test_gpu                       PASSED in 8.6s
//tensorflow/python/kernel_tests:one_hot_op_test_gpu                     PASSED in 8.7s
//tensorflow/python/kernel_tests:pad_op_test_gpu                         PASSED in 9.0s
//tensorflow/python/kernel_tests:padding_fifo_queue_test_gpu             PASSED in 6.7s
//tensorflow/python/kernel_tests:pool_test_gpu                           PASSED in 21.2s
//tensorflow/python/kernel_tests:pooling_ops_3d_test_gpu                 PASSED in 20.1s
//tensorflow/python/kernel_tests:py_func_test_gpu                        PASSED in 17.6s
//tensorflow/python/kernel_tests:reduce_benchmark_test_gpu               PASSED in 6.5s
//tensorflow/python/kernel_tests:reduce_join_op_test_gpu                 PASSED in 8.6s
//tensorflow/python/kernel_tests:relu_op_test_gpu                        PASSED in 7.7s
//tensorflow/python/kernel_tests:reshape_op_test_gpu                     PASSED in 20.4s
//tensorflow/python/kernel_tests:resource_variable_ops_test_gpu          PASSED in 17.2s
//tensorflow/python/kernel_tests:reverse_sequence_op_test_gpu            PASSED in 7.5s
//tensorflow/python/kernel_tests:scalar_test_gpu                         PASSED in 7.5s
//tensorflow/python/kernel_tests:scan_ops_test_gpu                       PASSED in 52.6s
//tensorflow/python/kernel_tests:shape_ops_test_gpu                      PASSED in 23.1s
//tensorflow/python/kernel_tests:softmax_op_test_gpu                     PASSED in 7.1s
//tensorflow/python/kernel_tests:softplus_op_test_gpu                    PASSED in 7.9s
//tensorflow/python/kernel_tests:softsign_op_test_gpu                    PASSED in 16.7s
//tensorflow/python/kernel_tests:spacetobatch_op_test_gpu                PASSED in 11.2s
//tensorflow/python/kernel_tests:spacetodepth_op_test_gpu                PASSED in 8.3s
//tensorflow/python/kernel_tests:sparse_matmul_op_test_gpu               PASSED in 15.6s
//tensorflow/python/kernel_tests:sparse_tensor_dense_matmul_grad_test_gpu PASSED in 9.7s
//tensorflow/python/kernel_tests:sparse_tensor_dense_matmul_op_test_gpu  PASSED in 8.4s
//tensorflow/python/kernel_tests:sparse_xent_op_test_gpu                 PASSED in 8.3s
//tensorflow/python/kernel_tests:split_op_test_gpu                       PASSED in 17.0s
//tensorflow/python/kernel_tests:stack_ops_test_gpu                      PASSED in 7.4s
//tensorflow/python/kernel_tests:stage_op_test_gpu                       PASSED in 10.0s
//tensorflow/python/kernel_tests:string_to_hash_bucket_op_test_gpu       PASSED in 14.6s
//tensorflow/python/kernel_tests:string_to_number_op_test_gpu            PASSED in 17.9s
//tensorflow/python/kernel_tests:summary_ops_test_gpu                    PASSED in 12.0s
//tensorflow/python/kernel_tests:summary_v1_audio_op_test_gpu            PASSED in 7.5s
//tensorflow/python/kernel_tests:summary_v1_image_op_test_gpu            PASSED in 8.3s
//tensorflow/python/kernel_tests:template_mirrored_strategy_test_gpu     PASSED in 7.5s
//tensorflow/python/kernel_tests:topk_op_test_gpu                        PASSED in 17.5s
//tensorflow/python/kernel_tests:trace_op_test_gpu                       PASSED in 8.3s
//tensorflow/python/kernel_tests:unsorted_segment_join_op_test_gpu       PASSED in 7.7s
//tensorflow/python/kernel_tests:variable_ops_test_gpu                   PASSED in 7.8s
//tensorflow/python/kernel_tests:where_op_test_gpu                       PASSED in 12.8s
//tensorflow/python/kernel_tests:while_v2_test_gpu                       PASSED in 39.7s
//tensorflow/python/kernel_tests:xent_op_test_gpu                        PASSED in 8.0s
//tensorflow/python/kernel_tests:zero_division_test_gpu                  PASSED in 8.3s
//tensorflow/python/kernel_tests/array_ops:batch_gather_op_test_gpu      PASSED in 8.5s
//tensorflow/python/kernel_tests/array_ops:gather_op_test_gpu            PASSED in 31.0s
//tensorflow/python/kernel_tests/array_ops:scatter_nd_ops_test_gpu       PASSED in 9.0s
//tensorflow/python/kernel_tests/array_ops:slice_op_test_gpu             PASSED in 10.5s
//tensorflow/python/kernel_tests/array_ops:stack_op_test_gpu             PASSED in 13.7s
//tensorflow/python/kernel_tests/array_ops:unstack_op_test_gpu           PASSED in 8.6s
//tensorflow/python/kernel_tests/distributions:bernoulli_test_gpu        PASSED in 12.6s
//tensorflow/python/kernel_tests/distributions:beta_test_gpu             PASSED in 13.3s
//tensorflow/python/kernel_tests/distributions:bijector_test_gpu         PASSED in 8.0s
//tensorflow/python/kernel_tests/distributions:categorical_test_gpu      PASSED in 7.7s
//tensorflow/python/kernel_tests/distributions:dirichlet_multinomial_test_gpu PASSED in 9.8s
//tensorflow/python/kernel_tests/distributions:dirichlet_test_gpu        PASSED in 14.1s
//tensorflow/python/kernel_tests/distributions:exponential_test_gpu      PASSED in 8.8s
//tensorflow/python/kernel_tests/distributions:gamma_test_gpu            PASSED in 31.5s
//tensorflow/python/kernel_tests/distributions:identity_bijector_test_gpu PASSED in 14.3s
//tensorflow/python/kernel_tests/distributions:kullback_leibler_test_gpu PASSED in 9.0s
//tensorflow/python/kernel_tests/distributions:laplace_test_gpu          PASSED in 20.5s
//tensorflow/python/kernel_tests/distributions:multinomial_test_gpu      PASSED in 8.2s
//tensorflow/python/kernel_tests/distributions:normal_test_gpu           PASSED in 23.4s
//tensorflow/python/kernel_tests/distributions:special_math_test_gpu     PASSED in 14.0s
//tensorflow/python/kernel_tests/distributions:uniform_test_gpu          PASSED in 21.4s
//tensorflow/python/kernel_tests/linalg:linear_operator_addition_test_gpu PASSED in 8.6s
//tensorflow/python/kernel_tests/linalg:linear_operator_algebra_test_gpu PASSED in 16.5s
//tensorflow/python/kernel_tests/linalg:linear_operator_lower_triangular_test_gpu PASSED in 59.1s
//tensorflow/python/kernel_tests/linalg:linear_operator_test_gpu         PASSED in 16.2s
//tensorflow/python/kernel_tests/linalg/sparse:conjugate_gradient_test_gpu PASSED in 13.0s
//tensorflow/python/kernel_tests/linalg/sparse:csr_sparse_matrix_test_gpu PASSED in 12.5s
//tensorflow/python/kernel_tests/random:multinomial_op_test_gpu          PASSED in 8.1s
//tensorflow/python/kernel_tests/random:parameterized_truncated_normal_op_test_gpu PASSED in 14.4s
//tensorflow/python/kernel_tests/random:random_crop_test_gpu             PASSED in 16.0s
//tensorflow/python/kernel_tests/random:random_grad_test_gpu             PASSED in 8.9s
//tensorflow/python/kernel_tests/random:random_ops_test_gpu              PASSED in 21.8s
//tensorflow/python/kernel_tests/random:random_poisson_test_gpu          PASSED in 20.6s
//tensorflow/python/kernel_tests/signal:mel_ops_test                     PASSED in 15.6s
//tensorflow/python/kernel_tests/signal:mfcc_ops_test                    PASSED in 10.7s
//tensorflow/python/kernel_tests/signal:reconstruction_ops_test          PASSED in 9.1s
//tensorflow/python/kernel_tests/signal:shape_ops_test                   PASSED in 16.6s
//tensorflow/python/kernel_tests/v1_compat_tests:scatter_nd_ops_test_gpu PASSED in 5.4s
//tensorflow/python/kernel_tests/v1_compat_tests:session_ops_test_gpu    PASSED in 8.3s
//tensorflow/python/ops/numpy_ops:np_array_ops_test_gpu                  PASSED in 30.9s
//tensorflow/python/ops/numpy_ops:np_arrays_test_gpu                     PASSED in 8.1s
//tensorflow/python/ops/numpy_ops:np_interop_test_gpu                    PASSED in 37.4s
//tensorflow/python/ops/numpy_ops:np_logic_test_gpu                      PASSED in 9.1s
//tensorflow/python/ops/numpy_ops:np_math_ops_test_gpu                   PASSED in 33.0s
//tensorflow/python/ops/numpy_ops:np_random_test_gpu                     PASSED in 22.1s
//tensorflow/python/ops/numpy_ops:np_utils_test_gpu                      PASSED in 14.8s
//tensorflow/python/ops/parallel_for:array_test_gpu                      PASSED in 29.6s
//tensorflow/python/ops/parallel_for:gradients_test_gpu                  PASSED in 7.5s
//tensorflow/python/ops/v1_compat_tests:gradient_checker_test_gpu        PASSED in 22.0s
//tensorflow/python/profiler:profiler_client_test_gpu                    PASSED in 6.4s
//tensorflow/python/profiler:profiler_test_gpu                           PASSED in 30.4s
//tensorflow/python/profiler:profiler_v2_test_gpu                        PASSED in 7.1s
//tensorflow/python/profiler/integration_test:profiler_api_test_gpu      PASSED in 28.3s
//tensorflow/python/training:adadelta_test                               PASSED in 22.6s
//tensorflow/python/training:adagrad_da_test                             PASSED in 7.4s
//tensorflow/python/training:adagrad_test                                PASSED in 18.6s
//tensorflow/python/training:adam_test_gpu                               PASSED in 17.4s
//tensorflow/python/training:basic_loops_test                            PASSED in 9.2s
//tensorflow/python/training:checkpoint_management_test_gpu              PASSED in 22.5s
//tensorflow/python/training:coordinator_test                            PASSED in 12.6s
//tensorflow/python/training:device_setter_test                          PASSED in 5.9s
//tensorflow/python/training:ftrl_test                                   PASSED in 22.6s
//tensorflow/python/training:gradient_descent_test                       PASSED in 9.2s
//tensorflow/python/training:momentum_test                               PASSED in 9.3s
//tensorflow/python/training:moving_averages_test_gpu                    PASSED in 8.3s
//tensorflow/python/training:optimizer_test                              PASSED in 8.7s
//tensorflow/python/training:proximal_adagrad_test                       PASSED in 9.6s
//tensorflow/python/training:proximal_gradient_descent_test              PASSED in 8.4s
//tensorflow/python/training:quantize_training_test                      PASSED in 16.0s
//tensorflow/python/training:queue_runner_test                           PASSED in 6.8s
//tensorflow/python/training:rmsprop_test                                PASSED in 12.5s
//tensorflow/python/training:saver_test_gpu                              PASSED in 24.2s
//tensorflow/python/training:session_manager_test_gpu                    PASSED in 73.4s
//tensorflow/python/training:slot_creator_test                           PASSED in 8.1s
//tensorflow/python/training:tensorboard_logging_test                    PASSED in 6.9s
//tensorflow/python/training:training_ops_test                           PASSED in 5.7s
//tensorflow/python/training/experimental:mixed_precision_test_gpu       PASSED in 8.8s
//tensorflow/python/training/saving:functional_saver_test_gpu            PASSED in 8.5s
//tensorflow/stream_executor/cuda:cuda_driver_test                       PASSED in 3.9s
//tensorflow/stream_executor/cuda:cuda_driver_test_gpu                   PASSED in 13.0s
//tensorflow/stream_executor/cuda:memcpy_test                            PASSED in 9.1s
//tensorflow/stream_executor/cuda:memcpy_test_gpu                        PASSED in 11.6s
//tensorflow/stream_executor/cuda:redzone_allocator_test                 PASSED in 10.6s
//tensorflow/stream_executor/cuda:redzone_allocator_test_gpu             PASSED in 8.6s
//tensorflow/tools/docs:tf_doctest_gpu                                   PASSED in 55.8s
//tensorflow/compiler/tests:complex_div_test_gpu                         PASSED in 19.6s
  Stats over 2 runs: max = 19.6s, min = 14.1s, avg = 16.9s, dev = 2.8s
//tensorflow/compiler/tests:complex_div_test_gpu_mlir_bridge_test        PASSED in 10.1s
  Stats over 2 runs: max = 10.1s, min = 5.5s, avg = 7.8s, dev = 2.3s
//tensorflow/compiler/tests:sort_ops_test_gpu                            PASSED in 104.2s
  Stats over 2 runs: max = 104.2s, min = 84.0s, avg = 94.1s, dev = 10.1s
//tensorflow/compiler/tests:sort_ops_test_gpu_mlir_bridge_test           PASSED in 58.4s
  Stats over 2 runs: max = 58.4s, min = 37.7s, avg = 48.0s, dev = 10.3s
//tensorflow/compiler/xla/tests:conditional_test_gpu                     PASSED in 7.7s
  Stats over 2 runs: max = 7.7s, min = 7.3s, avg = 7.5s, dev = 0.2s
//tensorflow/python:control_flow_ops_test_gpu                            PASSED in 11.9s
  Stats over 2 runs: max = 11.9s, min = 10.2s, avg = 11.1s, dev = 0.8s
//tensorflow/python/distribute:cross_device_ops_test_gpu                 PASSED in 68.5s
  Stats over 2 runs: max = 68.5s, min = 63.4s, avg = 66.0s, dev = 2.5s
//tensorflow/python/distribute:strategy_common_test_gpu                  PASSED in 25.8s
  Stats over 2 runs: max = 25.8s, min = 20.3s, avg = 23.0s, dev = 2.7s
//tensorflow/python/distribute/integration_test:mwms_peer_failure_test_gpu PASSED in 59.5s
  Stats over 2 runs: max = 59.5s, min = 32.3s, avg = 45.9s, dev = 13.6s
//tensorflow/python/keras/layers:recurrent_v2_test_gpu                   PASSED in 23.6s
  Stats over 2 runs: max = 23.6s, min = 17.7s, avg = 20.7s, dev = 3.0s
//tensorflow/python/keras/optimizer_v2:rmsprop_test_gpu                  PASSED in 20.0s
  Stats over 2 runs: max = 20.0s, min = 11.0s, avg = 15.5s, dev = 4.5s
//tensorflow/python/kernel_tests:conv2d_backprop_filter_grad_test_gpu    PASSED in 41.5s
  Stats over 2 runs: max = 41.5s, min = 30.0s, avg = 35.8s, dev = 5.8s
//tensorflow/python/kernel_tests:functional_ops_test_gpu                 PASSED in 25.2s
  Stats over 2 runs: max = 25.2s, min = 10.9s, avg = 18.1s, dev = 7.2s
//tensorflow/python/kernel_tests:map_fn_test_gpu                         PASSED in 8.4s
  Stats over 2 runs: max = 8.4s, min = 7.9s, avg = 8.2s, dev = 0.3s
//tensorflow/python/kernel_tests:scatter_ops_test_gpu                    PASSED in 13.4s
  Stats over 2 runs: max = 13.4s, min = 10.6s, avg = 12.0s, dev = 1.4s
//tensorflow/python/training/experimental:loss_scaling_gradient_tape_test_gpu PASSED in 70.3s
  Stats over 2 runs: max = 70.3s, min = 44.9s, avg = 57.6s, dev = 12.7s
//tensorflow/compiler/tests:spacetobatch_op_test_gpu                     PASSED in 20.5s
  Stats over 3 runs: max = 20.5s, min = 9.8s, avg = 15.4s, dev = 4.4s
//tensorflow/compiler/tests:spacetobatch_op_test_gpu_mlir_bridge_test    PASSED in 9.6s
  Stats over 3 runs: max = 9.6s, min = 8.6s, avg = 9.1s, dev = 0.4s
//tensorflow/python/kernel_tests:matrix_triangular_solve_op_test_gpu     PASSED in 18.0s
  Stats over 3 runs: max = 18.0s, min = 8.2s, avg = 11.7s, dev = 4.5s
//tensorflow/python/kernel_tests/distributions:util_test_gpu             PASSED in 18.1s
  Stats over 3 runs: max = 18.1s, min = 10.4s, avg = 14.1s, dev = 3.2s
//tensorflow/python/kernel_tests/random:multinomial_op_big_test_gpu      PASSED in 11.9s
  Stats over 3 runs: max = 11.9s, min = 8.7s, avg = 10.2s, dev = 1.3s
//tensorflow/python:nn_batchnorm_test_gpu                                PASSED in 16.9s
  Stats over 4 runs: max = 16.9s, min = 8.6s, avg = 12.6s, dev = 3.2s
//tensorflow/python/debug:dumping_callback_test_gpu                      PASSED in 11.6s
  Stats over 4 runs: max = 11.6s, min = 10.9s, avg = 11.4s, dev = 0.3s
//tensorflow/python/distribute:strategy_gather_test_gpu                  PASSED in 57.5s
  Stats over 4 runs: max = 57.5s, min = 48.0s, avg = 51.7s, dev = 3.5s
//tensorflow/python/keras/distribute:keras_optimizer_v2_test_gpu         PASSED in 13.5s
  Stats over 4 runs: max = 13.5s, min = 9.3s, avg = 11.0s, dev = 1.5s
//tensorflow/python/keras/distribute:keras_premade_models_test_gpu       PASSED in 46.1s
  Stats over 4 runs: max = 46.1s, min = 31.1s, avg = 41.2s, dev = 6.0s
//tensorflow/python/keras/distribute:keras_stateful_lstm_model_correctness_test_gpu PASSED in 10.6s
  Stats over 4 runs: max = 10.6s, min = 6.7s, avg = 7.7s, dev = 1.6s
//tensorflow/python/keras/distribute:keras_utils_test_gpu                PASSED in 69.9s
  Stats over 4 runs: max = 69.9s, min = 62.7s, avg = 65.4s, dev = 2.7s
//tensorflow/python/keras/distribute:worker_training_state_test_gpu      PASSED in 28.8s
  Stats over 4 runs: max = 28.8s, min = 9.7s, avg = 15.3s, dev = 7.8s
//tensorflow/python/keras/layers:cudnn_recurrent_test_gpu                PASSED in 64.9s
  Stats over 4 runs: max = 64.9s, min = 26.9s, avg = 45.1s, dev = 13.7s
//tensorflow/python/keras/layers:normalization_test_gpu                  PASSED in 50.6s
  Stats over 4 runs: max = 50.6s, min = 24.2s, avg = 37.9s, dev = 10.3s
//tensorflow/python/keras/layers/preprocessing:category_crossing_test_gpu PASSED in 18.8s
  Stats over 4 runs: max = 18.8s, min = 9.2s, avg = 12.1s, dev = 3.9s
//tensorflow/python/keras/layers/preprocessing:hashing_test_gpu          PASSED in 12.0s
  Stats over 4 runs: max = 12.0s, min = 8.4s, avg = 9.7s, dev = 1.4s
//tensorflow/python/keras/layers/preprocessing:image_preprocessing_test_gpu PASSED in 38.9s
  Stats over 4 runs: max = 38.9s, min = 22.8s, avg = 27.4s, dev = 6.7s
//tensorflow/python/keras/optimizer_v2:adadelta_test_gpu                 PASSED in 18.3s
  Stats over 4 runs: max = 18.3s, min = 9.2s, avg = 12.0s, dev = 3.7s
//tensorflow/python/keras/optimizer_v2:adagrad_test_gpu                  PASSED in 25.8s
  Stats over 4 runs: max = 25.8s, min = 18.3s, avg = 20.7s, dev = 3.0s
//tensorflow/python/keras/optimizer_v2:adam_test_gpu                     PASSED in 25.1s
  Stats over 4 runs: max = 25.1s, min = 13.9s, avg = 18.6s, dev = 4.1s
//tensorflow/python/keras/optimizer_v2:adamax_test_gpu                   PASSED in 12.3s
  Stats over 4 runs: max = 12.3s, min = 9.0s, avg = 10.8s, dev = 1.2s
//tensorflow/python/keras/optimizer_v2:ftrl_test_gpu                     PASSED in 23.9s
  Stats over 4 runs: max = 23.9s, min = 17.2s, avg = 20.2s, dev = 2.8s
//tensorflow/python/keras/optimizer_v2:gradient_descent_test_gpu         PASSED in 24.5s
  Stats over 4 runs: max = 24.5s, min = 12.0s, avg = 20.1s, dev = 4.8s
//tensorflow/python/keras/optimizer_v2:learning_rate_schedule_test_gpu   PASSED in 20.0s
  Stats over 4 runs: max = 20.0s, min = 10.4s, avg = 16.1s, dev = 3.9s
//tensorflow/python/keras/optimizer_v2:nadam_test_gpu                    PASSED in 11.1s
  Stats over 4 runs: max = 11.1s, min = 8.1s, avg = 9.0s, dev = 1.2s
//tensorflow/python/kernel_tests:einsum_op_test_gpu                      PASSED in 75.5s
  Stats over 4 runs: max = 75.5s, min = 10.8s, avg = 35.6s, dev = 25.6s
//tensorflow/python/kernel_tests:init_ops_test_gpu                       PASSED in 32.2s
  Stats over 4 runs: max = 32.2s, min = 22.5s, avg = 28.1s, dev = 3.6s
//tensorflow/python/kernel_tests:pooling_ops_test_gpu                    PASSED in 29.6s
  Stats over 4 runs: max = 29.6s, min = 10.8s, avg = 21.1s, dev = 8.5s
//tensorflow/python/kernel_tests/random:random_gamma_test_gpu            PASSED in 40.0s
  Stats over 4 runs: max = 40.0s, min = 7.8s, avg = 23.1s, dev = 15.1s
//tensorflow/python/kernel_tests/signal:window_ops_test                  PASSED in 15.7s
  Stats over 4 runs: max = 15.7s, min = 12.2s, avg = 14.0s, dev = 1.6s
//tensorflow/compiler/tests:conv3d_test_gpu                              PASSED in 17.6s
  Stats over 5 runs: max = 17.6s, min = 6.2s, avg = 12.5s, dev = 3.7s
//tensorflow/compiler/tests:conv3d_test_gpu_mlir_bridge_test             PASSED in 15.8s
  Stats over 5 runs: max = 15.8s, min = 6.1s, avg = 12.1s, dev = 4.0s
//tensorflow/compiler/tests:depthwise_conv_op_test_gpu                   PASSED in 19.4s
  Stats over 5 runs: max = 19.4s, min = 13.8s, avg = 16.4s, dev = 1.9s
//tensorflow/compiler/tests:depthwise_conv_op_test_gpu_mlir_bridge_test  PASSED in 17.9s
  Stats over 5 runs: max = 17.9s, min = 12.3s, avg = 14.9s, dev = 1.9s
//tensorflow/compiler/tests:fused_batchnorm_test_gpu                     PASSED in 17.5s
  Stats over 5 runs: max = 17.5s, min = 6.6s, avg = 10.4s, dev = 3.9s
//tensorflow/compiler/tests:jit_test_gpu                                 PASSED in 52.5s
  Stats over 5 runs: max = 52.5s, min = 7.9s, avg = 19.0s, dev = 17.0s
//tensorflow/compiler/tests:qr_op_test_gpu                               PASSED in 87.7s
  Stats over 5 runs: max = 87.7s, min = 57.6s, avg = 73.4s, dev = 11.4s
//tensorflow/compiler/tests:reduce_ops_test_gpu                          PASSED in 11.9s
  Stats over 5 runs: max = 11.9s, min = 8.6s, avg = 10.0s, dev = 1.1s
//tensorflow/compiler/tests:reduce_ops_test_gpu_mlir_bridge_test         PASSED in 20.2s
  Stats over 5 runs: max = 20.2s, min = 9.5s, avg = 12.6s, dev = 3.9s
//tensorflow/compiler/tests:special_math_test_gpu                        PASSED in 161.5s
  Stats over 5 runs: max = 161.5s, min = 10.8s, avg = 66.1s, dev = 53.7s
//tensorflow/compiler/tests:special_math_test_gpu_mlir_bridge_test       PASSED in 150.6s
  Stats over 5 runs: max = 150.6s, min = 12.1s, avg = 66.1s, dev = 52.2s
//tensorflow/compiler/tests:unstack_test_gpu                             PASSED in 21.9s
  Stats over 5 runs: max = 21.9s, min = 5.2s, avg = 8.7s, dev = 6.6s
//tensorflow/compiler/tests:unstack_test_gpu_mlir_bridge_test            PASSED in 21.9s
  Stats over 5 runs: max = 21.9s, min = 5.4s, avg = 9.1s, dev = 6.4s
//tensorflow/python/distribute:custom_training_loop_input_test_gpu       PASSED in 30.6s
  Stats over 5 runs: max = 30.6s, min = 11.0s, avg = 23.3s, dev = 6.9s
//tensorflow/python/distribute:mirrored_strategy_test_gpu                PASSED in 31.5s
  Stats over 5 runs: max = 31.5s, min = 11.8s, avg = 20.4s, dev = 6.5s
//tensorflow/python/distribute:values_test_gpu                           PASSED in 106.2s
  Stats over 5 runs: max = 106.2s, min = 78.9s, avg = 88.1s, dev = 9.5s
//tensorflow/python/distribute:vars_test_gpu                             PASSED in 80.3s
  Stats over 5 runs: max = 80.3s, min = 52.5s, avg = 67.1s, dev = 10.7s
//tensorflow/python/eager:device_placement_test_gpu                      PASSED in 19.0s
  Stats over 5 runs: max = 19.0s, min = 7.7s, avg = 11.3s, dev = 4.0s
//tensorflow/python/eager:forwardprop_test_gpu                           PASSED in 33.2s
  Stats over 5 runs: max = 33.2s, min = 13.0s, avg = 21.0s, dev = 7.8s
//tensorflow/python/eager:function_gradients_test_gpu                    PASSED in 27.0s
  Stats over 5 runs: max = 27.0s, min = 18.1s, avg = 20.7s, dev = 3.3s
//tensorflow/python/kernel_tests:cholesky_op_test_gpu                    PASSED in 49.8s
  Stats over 5 runs: max = 49.8s, min = 26.4s, avg = 39.3s, dev = 8.0s
//tensorflow/python/kernel_tests:sparse_ops_test_gpu                     PASSED in 19.9s
  Stats over 5 runs: max = 19.9s, min = 7.3s, avg = 10.3s, dev = 4.8s
//tensorflow/python/kernel_tests/linalg:linear_operator_adjoint_test_gpu PASSED in 27.7s
  Stats over 5 runs: max = 27.7s, min = 16.7s, avg = 23.2s, dev = 4.3s
//tensorflow/python/kernel_tests/linalg:linear_operator_composition_test_gpu PASSED in 40.0s
  Stats over 5 runs: max = 40.0s, min = 23.0s, avg = 30.4s, dev = 5.8s
//tensorflow/python/kernel_tests/linalg:linear_operator_diag_test_gpu    PASSED in 38.6s
  Stats over 5 runs: max = 38.6s, min = 22.8s, avg = 28.7s, dev = 5.7s
//tensorflow/python/kernel_tests/linalg:linear_operator_full_matrix_test_gpu PASSED in 28.4s
  Stats over 5 runs: max = 28.4s, min = 18.8s, avg = 24.0s, dev = 4.2s
//tensorflow/python/kernel_tests/linalg:linear_operator_householder_test_gpu PASSED in 19.7s
  Stats over 5 runs: max = 19.7s, min = 15.9s, avg = 17.8s, dev = 1.5s
//tensorflow/python/kernel_tests/linalg:linear_operator_identity_test_gpu PASSED in 33.1s
  Stats over 5 runs: max = 33.1s, min = 20.6s, avg = 25.2s, dev = 4.2s
//tensorflow/python/kernel_tests/linalg:linear_operator_inversion_test_gpu PASSED in 22.1s
  Stats over 5 runs: max = 22.1s, min = 15.4s, avg = 18.6s, dev = 2.6s
//tensorflow/python/kernel_tests/linalg:linear_operator_permutation_test_gpu PASSED in 30.1s
  Stats over 5 runs: max = 30.1s, min = 24.4s, avg = 26.6s, dev = 2.1s
//tensorflow/python/kernel_tests/linalg:linear_operator_tridiag_test_gpu PASSED in 56.1s
  Stats over 5 runs: max = 56.1s, min = 44.1s, avg = 48.9s, dev = 4.3s
//tensorflow/python/kernel_tests/linalg:linear_operator_util_test_gpu    PASSED in 21.9s
  Stats over 5 runs: max = 21.9s, min = 15.3s, avg = 18.7s, dev = 2.6s
//tensorflow/python/kernel_tests/linalg:linear_operator_zeros_test_gpu   PASSED in 15.1s
  Stats over 5 runs: max = 15.1s, min = 11.3s, avg = 12.7s, dev = 1.3s
//tensorflow/python/ops/parallel_for:math_test_gpu                       PASSED in 37.6s
  Stats over 5 runs: max = 37.6s, min = 20.3s, avg = 31.4s, dev = 6.3s
//tensorflow/compiler/tests:fft_test_gpu                                 PASSED in 20.3s
  Stats over 6 runs: max = 20.3s, min = 14.2s, avg = 17.7s, dev = 2.1s
//tensorflow/compiler/xla/tests:prng_test_gpu                            PASSED in 23.4s
  Stats over 6 runs: max = 23.4s, min = 12.0s, avg = 14.5s, dev = 4.0s
//tensorflow/python:accumulate_n_benchmark_gpu                           PASSED in 15.5s
  Stats over 6 runs: max = 15.5s, min = 4.8s, avg = 7.1s, dev = 3.8s
//tensorflow/python/kernel_tests:diag_op_test_gpu                        PASSED in 59.0s
  Stats over 6 runs: max = 59.0s, min = 7.5s, avg = 17.7s, dev = 18.5s
//tensorflow/python/kernel_tests:reduction_ops_test_gpu                  PASSED in 32.4s
  Stats over 6 runs: max = 32.4s, min = 19.4s, avg = 26.5s, dev = 4.8s
//tensorflow/python/keras/distribute:keras_save_load_test_gpu            PASSED in 145.1s
  Stats over 7 runs: max = 145.1s, min = 116.4s, avg = 123.2s, dev = 9.2s
//tensorflow/python/keras/distribute:saved_model_mixed_api_test_gpu      PASSED in 132.3s
  Stats over 7 runs: max = 132.3s, min = 103.0s, avg = 117.1s, dev = 9.7s
//tensorflow/python/keras/distribute:saved_model_save_load_test_gpu      PASSED in 121.5s
  Stats over 7 runs: max = 121.5s, min = 106.9s, avg = 113.9s, dev = 4.5s
//tensorflow/python/keras/distribute:keras_embedding_model_correctness_test_gpu PASSED in 122.3s
  Stats over 8 runs: max = 122.3s, min = 94.1s, avg = 104.3s, dev = 9.7s
//tensorflow/python/keras/layers:convolutional_test_gpu                  PASSED in 42.1s
  Stats over 8 runs: max = 42.1s, min = 30.9s, avg = 37.7s, dev = 3.5s
//tensorflow/python/keras/optimizer_v2:optimizer_v2_test_gpu             PASSED in 51.4s
  Stats over 8 runs: max = 51.4s, min = 33.6s, avg = 40.9s, dev = 5.5s
//tensorflow/python/kernel_tests/linalg:linear_operator_block_diag_test_gpu PASSED in 33.9s
  Stats over 8 runs: max = 33.9s, min = 23.4s, avg = 26.7s, dev = 3.4s
//tensorflow/python/kernel_tests/linalg:linear_operator_block_lower_triangular_test_gpu PASSED in 40.7s
  Stats over 8 runs: max = 40.7s, min = 24.0s, avg = 28.8s, dev = 6.1s
//tensorflow/python/kernel_tests/signal:fft_ops_test                     PASSED in 28.7s
  Stats over 8 runs: max = 28.7s, min = 13.3s, avg = 21.9s, dev = 4.7s
//tensorflow/python/kernel_tests:extract_image_patches_grad_test_gpu     PASSED in 171.5s
  Stats over 9 runs: max = 171.5s, min = 5.7s, avg = 38.9s, dev = 49.1s
//tensorflow/compiler/tests:conv2d_test_gpu                              PASSED in 21.2s
  Stats over 10 runs: max = 21.2s, min = 9.7s, avg = 16.3s, dev = 3.7s
//tensorflow/compiler/tests:conv2d_test_gpu_mlir_bridge_test             PASSED in 20.2s
  Stats over 10 runs: max = 20.2s, min = 8.4s, avg = 11.5s, dev = 3.7s
//tensorflow/compiler/tests:random_ops_test_gpu                          PASSED in 21.9s
  Stats over 10 runs: max = 21.9s, min = 7.1s, avg = 12.5s, dev = 5.1s
//tensorflow/compiler/xla/client/lib:svd_test_gpu                        PASSED in 42.9s
  Stats over 10 runs: max = 42.9s, min = 17.1s, avg = 24.6s, dev = 6.8s
//tensorflow/compiler/xla/client/lib:tridiagonal_test_gpu                PASSED in 29.5s
  Stats over 10 runs: max = 29.5s, min = 16.9s, avg = 20.2s, dev = 3.5s
//tensorflow/python:cluster_test_gpu                                     PASSED in 19.1s
  Stats over 10 runs: max = 19.1s, min = 13.5s, avg = 16.0s, dev = 1.6s
//tensorflow/python:function_test_gpu                                    PASSED in 34.7s
  Stats over 10 runs: max = 34.7s, min = 15.9s, avg = 20.1s, dev = 5.7s
//tensorflow/python:special_math_ops_test_gpu                            PASSED in 34.3s
  Stats over 10 runs: max = 34.3s, min = 8.4s, avg = 13.4s, dev = 7.2s
//tensorflow/python/distribute:input_lib_test_gpu                        PASSED in 43.3s
  Stats over 10 runs: max = 43.3s, min = 20.3s, avg = 29.5s, dev = 7.5s
//tensorflow/python/distribute:input_lib_type_spec_test_gpu              PASSED in 26.1s
  Stats over 10 runs: max = 26.1s, min = 17.7s, avg = 22.3s, dev = 2.5s
//tensorflow/python/distribute:random_generator_test_gpu                 PASSED in 24.4s
  Stats over 10 runs: max = 24.4s, min = 9.6s, avg = 16.5s, dev = 4.4s
//tensorflow/python/keras/distribute:ctl_correctness_test_gpu            PASSED in 73.1s
  Stats over 10 runs: max = 73.1s, min = 49.3s, avg = 56.5s, dev = 8.2s
//tensorflow/python/keras/legacy_tf_layers:normalization_test_gpu        PASSED in 11.3s
  Stats over 10 runs: max = 11.3s, min = 6.7s, avg = 8.6s, dev = 1.5s
//tensorflow/python/keras/mixed_precision:keras_test_gpu                 PASSED in 64.1s
  Stats over 10 runs: max = 64.1s, min = 39.9s, avg = 53.3s, dev = 7.6s
//tensorflow/python/kernel_tests:array_ops_test_gpu                      PASSED in 28.6s
  Stats over 10 runs: max = 28.6s, min = 7.6s, avg = 14.9s, dev = 7.7s
//tensorflow/python/kernel_tests:inplace_ops_test_gpu                    PASSED in 16.0s
  Stats over 10 runs: max = 16.0s, min = 6.2s, avg = 9.1s, dev = 3.4s
//tensorflow/python/kernel_tests:rnn_test_gpu                            PASSED in 19.2s
  Stats over 10 runs: max = 19.2s, min = 8.5s, avg = 11.9s, dev = 3.4s
//tensorflow/python/kernel_tests:segment_reduction_ops_test_gpu          PASSED in 42.6s
  Stats over 10 runs: max = 42.6s, min = 7.0s, avg = 13.9s, dev = 10.7s
//tensorflow/python/kernel_tests:tensor_array_ops_test_gpu               PASSED in 17.3s
  Stats over 10 runs: max = 17.3s, min = 7.2s, avg = 10.3s, dev = 3.5s
//tensorflow/python/kernel_tests:tridiagonal_matmul_op_test_gpu          PASSED in 36.5s
  Stats over 10 runs: max = 36.5s, min = 4.9s, avg = 10.1s, dev = 8.9s
//tensorflow/python/kernel_tests/linalg:linear_operator_circulant_test_gpu PASSED in 59.5s
  Stats over 10 runs: max = 59.5s, min = 36.9s, avg = 42.8s, dev = 7.1s
//tensorflow/python/kernel_tests/linalg:linear_operator_kronecker_test_gpu PASSED in 48.2s
  Stats over 10 runs: max = 48.2s, min = 25.1s, avg = 35.1s, dev = 6.3s
//tensorflow/python/kernel_tests/linalg:linear_operator_low_rank_update_test_gpu PASSED in 65.7s
  Stats over 10 runs: max = 65.7s, min = 35.7s, avg = 46.4s, dev = 11.0s
//tensorflow/python/kernel_tests/linalg/sparse:csr_sparse_matrix_ops_test_gpu PASSED in 53.4s
  Stats over 10 runs: max = 53.4s, min = 9.8s, avg = 21.0s, dev = 15.3s
//tensorflow/python/kernel_tests/random:stateless_random_ops_test_gpu    PASSED in 72.8s
  Stats over 10 runs: max = 72.8s, min = 13.4s, avg = 39.1s, dev = 20.5s
//tensorflow/python/keras/layers:gru_v2_test_gpu                         PASSED in 37.1s
  Stats over 12 runs: max = 37.1s, min = 18.3s, avg = 28.8s, dev = 6.0s
//tensorflow/compiler/tests:image_ops_test_gpu                           FAILED in 3 out of 12 in 35.2s
  Stats over 12 runs: max = 35.2s, min = 14.3s, avg = 19.4s, dev = 5.2s
  /tmpfs/bazel_output/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/compiler/tests/image_ops_test_gpu/shard_5_of_10/test.log
  /tmpfs/bazel_output/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/compiler/tests/image_ops_test_gpu/shard_5_of_10/test_attempts/attempt_1.log
  /tmpfs/bazel_output/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/compiler/tests/image_ops_test_gpu/shard_5_of_10/test_attempts/attempt_2.log
//tensorflow/python/kernel_tests:rnn_cell_test_gpu                       PASSED in 30.4s
  Stats over 15 runs: max = 30.4s, min = 15.9s, avg = 20.5s, dev = 3.5s
//tensorflow/python:image_ops_test_gpu                                   PASSED in 24.0s
  Stats over 16 runs: max = 24.0s, min = 8.8s, avg = 15.9s, dev = 6.1s
//tensorflow/python/keras/distribute:keras_image_model_correctness_test_gpu PASSED in 141.3s
  Stats over 16 runs: max = 141.3s, min = 76.0s, avg = 104.1s, dev = 22.3s
//tensorflow/python/kernel_tests:control_flow_ops_py_test_gpu            PASSED in 17.9s
  Stats over 16 runs: max = 17.9s, min = 7.0s, avg = 9.3s, dev = 2.6s
//tensorflow/python/kernel_tests/signal:dct_ops_test                     PASSED in 12.6s
  Stats over 16 runs: max = 12.6s, min = 8.1s, avg = 10.0s, dev = 1.1s
//tensorflow/python/ops/parallel_for:control_flow_ops_test_gpu           PASSED in 33.0s
  Stats over 16 runs: max = 33.0s, min = 11.8s, avg = 17.6s, dev = 6.4s
//tensorflow/python/eager:function_test_gpu                              FAILED in 3 out of 17 in 28.5s
  Stats over 17 runs: max = 28.5s, min = 9.2s, avg = 12.8s, dev = 4.4s
  /tmpfs/bazel_output/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/eager/function_test_gpu/shard_9_of_15/test.log
  /tmpfs/bazel_output/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/eager/function_test_gpu/shard_9_of_15/test_attempts/attempt_1.log
  /tmpfs/bazel_output/_bazel_kbuilder/f2d52ca1f092ccbe254cc98c3dc90790/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/eager/function_test_gpu/shard_9_of_15/test_attempts/attempt_2.log
//tensorflow/compiler/tests:pooling_ops_3d_test_gpu                      PASSED in 7.9s
  Stats over 20 runs: max = 7.9s, min = 6.1s, avg = 6.7s, dev = 0.5s
//tensorflow/compiler/tests:pooling_ops_3d_test_gpu_mlir_bridge_test     PASSED in 18.3s
  Stats over 20 runs: max = 18.3s, min = 6.3s, avg = 10.5s, dev = 4.7s
//tensorflow/compiler/tests:pooling_ops_test_gpu                         PASSED in 9.3s
  Stats over 20 runs: max = 9.3s, min = 4.8s, avg = 6.5s, dev = 1.2s
//tensorflow/compiler/tests:pooling_ops_test_gpu_mlir_bridge_test        PASSED in 24.4s
  Stats over 20 runs: max = 24.4s, min = 5.0s, avg = 10.4s, dev = 6.0s
//tensorflow/compiler/xla/tests:convolution_dimension_numbers_test_gpu   PASSED in 22.3s
  Stats over 20 runs: max = 22.3s, min = 11.5s, avg = 14.6s, dev = 2.9s
//tensorflow/compiler/xla/tests:dot_operation_single_threaded_runtime_test_gpu PASSED in 24.7s
  Stats over 20 runs: max = 24.7s, min = 12.9s, avg = 15.7s, dev = 2.7s
//tensorflow/compiler/xla/tests:dot_operation_test_autotune_disabled_gpu PASSED in 19.8s
  Stats over 20 runs: max = 19.8s, min = 12.4s, avg = 14.6s, dev = 1.8s
//tensorflow/compiler/xla/tests:dot_operation_test_gpu                   PASSED in 25.2s
  Stats over 20 runs: max = 25.2s, min = 9.8s, avg = 14.2s, dev = 4.1s
//tensorflow/compiler/xla/tests:reduce_window_test_gpu                   PASSED in 50.3s
  Stats over 20 runs: max = 50.3s, min = 5.5s, avg = 14.0s, dev = 10.2s
//tensorflow/python/keras/distribute:distribute_strategy_test_gpu        PASSED in 107.9s
  Stats over 20 runs: max = 107.9s, min = 48.5s, avg = 83.2s, dev = 14.2s
//tensorflow/python/kernel_tests:batch_matmul_op_test_gpu                PASSED in 25.3s
  Stats over 20 runs: max = 25.3s, min = 16.5s, avg = 19.6s, dev = 2.1s
//tensorflow/python/kernel_tests:embedding_ops_test_gpu                  PASSED in 18.7s
  Stats over 20 runs: max = 18.7s, min = 4.6s, avg = 9.5s, dev = 4.0s
//tensorflow/python/kernel_tests:linalg_grad_test_gpu                    PASSED in 70.5s
  Stats over 20 runs: max = 70.5s, min = 24.1s, avg = 38.4s, dev = 13.5s
//tensorflow/python/kernel_tests:matmul_op_test_gpu                      PASSED in 30.8s
  Stats over 20 runs: max = 30.8s, min = 12.3s, avg = 18.6s, dev = 6.0s
//tensorflow/python/kernel_tests:matrix_band_part_op_test_gpu            PASSED in 18.9s
  Stats over 20 runs: max = 18.9s, min = 4.8s, avg = 10.0s, dev = 4.9s
//tensorflow/python/kernel_tests:norm_op_test_gpu                        PASSED in 20.2s
  Stats over 20 runs: max = 20.2s, min = 5.0s, avg = 9.5s, dev = 4.8s
//tensorflow/python/kernel_tests:normalize_op_test_gpu                   PASSED in 24.5s
  Stats over 20 runs: max = 24.5s, min = 7.0s, avg = 13.2s, dev = 5.6s
//tensorflow/python/kernel_tests:qr_op_test_gpu                          PASSED in 50.6s
  Stats over 20 runs: max = 50.6s, min = 18.7s, avg = 29.0s, dev = 8.8s
//tensorflow/python/kernel_tests:self_adjoint_eig_op_test_gpu            PASSED in 26.3s
  Stats over 20 runs: max = 26.3s, min = 7.9s, avg = 14.1s, dev = 5.6s
//tensorflow/python/kernel_tests:tensordot_op_test_gpu                   PASSED in 22.2s
  Stats over 20 runs: max = 22.2s, min = 7.8s, avg = 14.8s, dev = 4.9s
//tensorflow/compiler/xla/tests:array_elementwise_ops_test_gpu           PASSED in 23.3s
  Stats over 25 runs: max = 23.3s, min = 13.4s, avg = 16.7s, dev = 3.1s
//tensorflow/compiler/xla/tests:convolution_test_1d_gpu_alternative_layout_gpu PASSED in 21.0s
  Stats over 25 runs: max = 21.0s, min = 6.6s, avg = 9.3s, dev = 4.0s
//tensorflow/compiler/xla/tests:convolution_test_gpu_alternative_layout_gpu PASSED in 21.3s
  Stats over 25 runs: max = 21.3s, min = 8.9s, avg = 13.0s, dev = 3.2s
//tensorflow/compiler/xla/tests:convolution_variants_test_gpu            PASSED in 42.8s
  Stats over 30 runs: max = 42.8s, min = 11.9s, avg = 16.4s, dev = 6.1s
//tensorflow/compiler/xla/tests:iota_test_gpu                            PASSED in 21.9s
  Stats over 30 runs: max = 21.9s, min = 13.7s, avg = 16.0s, dev = 1.8s
//tensorflow/compiler/xla/tests:params_test_gpu                          PASSED in 22.9s
  Stats over 30 runs: max = 22.9s, min = 10.6s, avg = 13.3s, dev = 3.1s
//tensorflow/compiler/xla/tests:reshape_test_gpu                         PASSED in 31.9s
  Stats over 30 runs: max = 31.9s, min = 15.3s, avg = 20.2s, dev = 4.5s
//tensorflow/python/kernel_tests:conv_ops_3d_test_gpu                    PASSED in 23.1s
  Stats over 30 runs: max = 23.1s, min = 8.4s, avg = 13.2s, dev = 3.6s
//tensorflow/compiler/xla/tests:reduce_test_gpu                          PASSED in 28.7s
  Stats over 31 runs: max = 28.7s, min = 13.2s, avg = 18.6s, dev = 5.0s
//tensorflow/python/keras/distribute:keras_rnn_model_correctness_test_gpu PASSED in 91.7s
  Stats over 31 runs: max = 91.7s, min = 20.8s, avg = 53.0s, dev = 14.6s
//tensorflow/compiler/xla/tests:scalar_computations_test_gpu             PASSED in 19.7s
  Stats over 32 runs: max = 19.7s, min = 10.8s, avg = 13.0s, dev = 2.4s
//tensorflow/compiler/xla/tests:batch_normalization_test_gpu             PASSED in 30.1s
  Stats over 40 runs: max = 30.1s, min = 14.7s, avg = 20.5s, dev = 4.0s
//tensorflow/compiler/xla/tests:bfloat16_test_gpu                        PASSED in 26.9s
  Stats over 40 runs: max = 26.9s, min = 12.4s, avg = 15.4s, dev = 3.1s
//tensorflow/compiler/xla/tests:conv_depthwise_backprop_filter_test_gpu  PASSED in 35.8s
  Stats over 40 runs: max = 35.8s, min = 13.7s, avg = 20.2s, dev = 6.3s
//tensorflow/compiler/xla/tests:convolution_test_1d_autotune_disabled_gpu PASSED in 29.7s
  Stats over 40 runs: max = 29.7s, min = 14.5s, avg = 20.6s, dev = 4.3s
//tensorflow/compiler/xla/tests:convolution_test_autotune_disabled_gpu   PASSED in 33.5s
  Stats over 40 runs: max = 33.5s, min = 6.2s, avg = 10.0s, dev = 5.7s
//tensorflow/compiler/xla/tests:slice_test_gpu                           PASSED in 19.9s
  Stats over 40 runs: max = 19.9s, min = 8.0s, avg = 11.5s, dev = 2.6s
//tensorflow/compiler/xla/tests:conv_depthwise_test_gpu                  PASSED in 40.9s
  Stats over 50 runs: max = 40.9s, min = 11.5s, avg = 18.5s, dev = 5.7s
//tensorflow/compiler/xla/tests:convolution_test_1d_no_vmodule_gpu       PASSED in 33.8s
  Stats over 50 runs: max = 33.8s, min = 12.0s, avg = 16.0s, dev = 4.3s
//tensorflow/compiler/xla/tests:convolution_test_gpu                     PASSED in 30.1s
  Stats over 50 runs: max = 30.1s, min = 6.3s, avg = 13.8s, dev = 5.2s
//tensorflow/compiler/xla/tests:grouped_convolution_test_gpu             PASSED in 25.2s
  Stats over 50 runs: max = 25.2s, min = 9.3s, avg = 13.1s, dev = 3.4s
//tensorflow/python/kernel_tests:cwise_ops_binary_test_gpu               PASSED in 22.8s
  Stats over 50 runs: max = 22.8s, min = 7.9s, avg = 12.1s, dev = 3.2s
//tensorflow/python/kernel_tests:cwise_ops_test_gpu                      PASSED in 9.8s
  Stats over 50 runs: max = 9.8s, min = 4.5s, avg = 6.6s, dev = 1.2s
//tensorflow/python/kernel_tests:cwise_ops_unary_test_gpu                PASSED in 18.7s
  Stats over 50 runs: max = 18.7s, min = 4.5s, avg = 7.6s, dev = 3.6s
//tensorflow/python/kernel_tests/linalg/sparse:csr_sparse_matrix_dense_mat_mul_grad_test_gpu PASSED in 19.1s
  Stats over 50 runs: max = 19.1s, min = 4.7s, avg = 12.6s, dev = 3.5s
//tensorflow/python/kernel_tests/linalg/sparse:csr_sparse_matrix_grad_test_gpu PASSED in 15.6s
  Stats over 50 runs: max = 15.6s, min = 4.3s, avg = 5.9s, dev = 2.0s
//tensorflow/python/kernel_tests/linalg/sparse:csr_sparse_matrix_sparse_mat_mul_grad_test_gpu PASSED in 27.9s
  Stats over 50 runs: max = 27.9s, min = 4.3s, avg = 9.8s, dev = 7.5s

Executed 877 out of 877 tests: 875 tests pass and 2 fail remotely.
There were tests whose specified size is too big. Use the --test_verbose_timeout_warnings command line option to see which ones these are.
